{"meta":{"exported_on":1498549194768,"version":"002"},"data":{"posts":[{"id":2,"uuid":"92105e79-b458-43c4-a05e-f0ae2cf49894","title":"About","slug":"about","markdown":"我是林志傑，網路上常用的名字是 Fukuball，前 iNDIEVOX 技術長。我使用 PHP 及 Python，最近對機器學習感到興趣，所以空閒時會將 Python 有關機器學習的 Github Project 翻譯成 PHP 版本。 / 我也是一個快樂的吉他手～\n\n我忘記在哪看到這些話，我覺得很有道理：\n\n<blockquote>\n寫作其實是一件讓讀者們窺探自己內心的事情，藉由文字和情感的編織，在字裡行間裡向讀者釋放自己，提供線索供人刺探自己。我想要你們了解我、支持我，甚至成為知音；但又怕你們太了解我，知道我所有的恐懼和思想。\n</blockquote>\n\n**我現在覺得自己是赤裸裸地站在大家面前啊！**\n","html":"<p>我是林志傑，網路上常用的名字是 Fukuball，前 iNDIEVOX 技術長。我使用 PHP 及 Python，最近對機器學習感到興趣，所以空閒時會將 Python 有關機器學習的 Github Project 翻譯成 PHP 版本。 / 我也是一個快樂的吉他手～</p>\n\n<p>我忘記在哪看到這些話，我覺得很有道理：</p>\n\n<blockquote>  \n寫作其實是一件讓讀者們窺探自己內心的事情，藉由文字和情感的編織，在字裡行間裡向讀者釋放自己，提供線索供人刺探自己。我想要你們了解我、支持我，甚至成為知音；但又怕你們太了解我，知道我所有的恐懼和思想。\n</blockquote>\n\n<p><strong>我現在覺得自己是赤裸裸地站在大家面前啊！</strong></p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-01-29T19:01:49.000Z","created_by":1,"updated_at":"2017-02-18T16:52:39.000Z","updated_by":1,"published_at":"2014-01-29T19:01:49.000Z","published_by":1},{"id":3,"uuid":"87edcd8d-f472-49ef-99d6-e99eefb0787d","title":"小海嚴選正妹分頁 Chrome Extension","slug":"xiao-hai-yan-xuan-zheng-mei-fen-ye-chrome-extension","markdown":"![小海嚴選正妹分頁 Chrome extension](http://static.obeobe.com/image/blog-image/xiao-hai-yan-xuan-zheng-mei-fen-ye-chrome-extension-1.png)\n\n最近不知怎麼搞的，突然開始怒寫小海嚴選 Chrome Extension 應用，起初只是很單純的想把流量導到<a href=\"http://curator.im/\" target=\"_blank\">小海嚴選</a>，也趁個機會學學 Chrome Extension 怎麼寫，加上小海嚴選正妹實在是很一個很好發揮的主題，一下子就想了好多個有趣的點子，而實際撰寫程式時也是意外簡單，就這樣子快速生出了<a href=\"https://chrome.google.com/webstore/detail/%E5%B0%8F%E6%B5%B7%E5%9A%B4%E9%81%B8%E6%AD%A3%E5%A6%B9%E6%A1%8C%E5%B8%83/gmhmohcpihfdofdhlkhoebdingdomcgm\" target=\"_blank\">小海嚴選正妹分頁</a>這個應用程式。\n\n這個應用程式功能非常簡單，就只是單純地把開新分頁這個頁面覆寫掉，換上了小海嚴選正妹 API 所吐出來的正妹圖片。所以安裝了小海嚴選正妹分頁之後，當你疲憊地開新分頁準備工作時，小海嚴選正妹便會出現來拯救你，每次開新分頁都會有不同的正妹來為你加油打氣，頓時讓人感到精神百倍！\n\n由於容易使用，所以效果蠻顯著的，也獲得不少好評，還收到了一些 feature 請求，比如將 Responsive 的效果做完整一點，讓小螢幕的使用者也可以看到完整畫面，或是加上<a href=\"http://curator.im/girl_of_the_day/\" target=\"_blank\">一天一妹</a>的強力宣傳區等等，都已經列入未來改版的方向。至於想要可愛男孩或是帥哥及 Gay 版本的需求，目前則是無法實作，畢竟這是使用<a href=\"http://curator.im/\" target=\"_blank\">小海嚴選</a> API 所衍申出來的應用程式，<a href=\"http://curator.im/\" target=\"_blank\">小海嚴選</a>裡只有正妹的內容，所以就無法啦！除非有人想當可愛男孩或是帥哥的策展人，這樣才會有內容可以實作，從這點我們可以充分了解什麼叫做內容為王！\n\n### 跟著我唸一遍：內容為王！\n\n![我好像看到了什麼不得了的東西](http://static.obeobe.com/image/subtitle-image/我好像看到了什麼不得了的東西.jpg)\n\n開始愛上開新分頁了嗎？如果使用起來會感到害羞，那就要趁現在多練練看到正妹要心如止水，我也正在努力練習噢！\n\n### 使用警告\n\n* 本應用定位為生產力工具\n* 定力不夠者可能會有以下副作用\n    * 不斷開新分頁\n    * 忘記為何要開新分頁\n    * 莫名其妙注視新分頁\n* 研究顯示，若使用者為真男人則不會有以上副作用\n\n立即前往安裝<a href=\"https://chrome.google.com/webstore/detail/%E5%B0%8F%E6%B5%B7%E5%9A%B4%E9%81%B8%E6%AD%A3%E5%A6%B9%E6%A1%8C%E5%B8%83/gmhmohcpihfdofdhlkhoebdingdomcgm\" target=\"_blank\">小海嚴選正妹分頁</a>！！！\n\n<blockquote>\n圖片取自於 Google 搜尋，絕無意侵犯智財權。若有侵犯請告知，我會馬上刪除，感謝！\n</blockquote>\n\n![cover-image](http://static.obeobe.com/image/blog-image/xiao-hai-yan-xuan-zheng-mei-fen-ye-chrome-extension-bg.jpg)\n","html":"<p><img src=\"http://static.obeobe.com/image/blog-image/xiao-hai-yan-xuan-zheng-mei-fen-ye-chrome-extension-1.png\" alt=\"小海嚴選正妹分頁 Chrome extension\" /></p>\n\n<p>最近不知怎麼搞的，突然開始怒寫小海嚴選 Chrome Extension 應用，起初只是很單純的想把流量導到<a href=\"http://curator.im/\" target=\"_blank\">小海嚴選</a>，也趁個機會學學 Chrome Extension 怎麼寫，加上小海嚴選正妹實在是很一個很好發揮的主題，一下子就想了好多個有趣的點子，而實際撰寫程式時也是意外簡單，就這樣子快速生出了<a href=\"https://chrome.google.com/webstore/detail/%E5%B0%8F%E6%B5%B7%E5%9A%B4%E9%81%B8%E6%AD%A3%E5%A6%B9%E6%A1%8C%E5%B8%83/gmhmohcpihfdofdhlkhoebdingdomcgm\" target=\"_blank\">小海嚴選正妹分頁</a>這個應用程式。</p>\n\n<p>這個應用程式功能非常簡單，就只是單純地把開新分頁這個頁面覆寫掉，換上了小海嚴選正妹 API 所吐出來的正妹圖片。所以安裝了小海嚴選正妹分頁之後，當你疲憊地開新分頁準備工作時，小海嚴選正妹便會出現來拯救你，每次開新分頁都會有不同的正妹來為你加油打氣，頓時讓人感到精神百倍！</p>\n\n<p>由於容易使用，所以效果蠻顯著的，也獲得不少好評，還收到了一些 feature 請求，比如將 Responsive 的效果做完整一點，讓小螢幕的使用者也可以看到完整畫面，或是加上<a href=\"http://curator.im/girl_of_the_day/\" target=\"_blank\">一天一妹</a>的強力宣傳區等等，都已經列入未來改版的方向。至於想要可愛男孩或是帥哥及 Gay 版本的需求，目前則是無法實作，畢竟這是使用<a href=\"http://curator.im/\" target=\"_blank\">小海嚴選</a> API 所衍申出來的應用程式，<a href=\"http://curator.im/\" target=\"_blank\">小海嚴選</a>裡只有正妹的內容，所以就無法啦！除非有人想當可愛男孩或是帥哥的策展人，這樣才會有內容可以實作，從這點我們可以充分了解什麼叫做內容為王！</p>\n\n<h3 id=\"\">跟著我唸一遍：內容為王！</h3>\n\n<p><img src=\"http://static.obeobe.com/image/subtitle-image/我好像看到了什麼不得了的東西.jpg\" alt=\"我好像看到了什麼不得了的東西\" /></p>\n\n<p>開始愛上開新分頁了嗎？如果使用起來會感到害羞，那就要趁現在多練練看到正妹要心如止水，我也正在努力練習噢！</p>\n\n<h3 id=\"\">使用警告</h3>\n\n<ul>\n<li>本應用定位為生產力工具</li>\n<li>定力不夠者可能會有以下副作用\n<ul><li>不斷開新分頁</li>\n<li>忘記為何要開新分頁</li>\n<li>莫名其妙注視新分頁</li></ul></li>\n<li>研究顯示，若使用者為真男人則不會有以上副作用</li>\n</ul>\n\n<p>立即前往安裝<a href=\"https://chrome.google.com/webstore/detail/%E5%B0%8F%E6%B5%B7%E5%9A%B4%E9%81%B8%E6%AD%A3%E5%A6%B9%E6%A1%8C%E5%B8%83/gmhmohcpihfdofdhlkhoebdingdomcgm\" target=\"_blank\">小海嚴選正妹分頁</a>！！！</p>\n\n<blockquote>  \n圖片取自於 Google 搜尋，絕無意侵犯智財權。若有侵犯請告知，我會馬上刪除，感謝！\n</blockquote>\n\n<p><img src=\"http://static.obeobe.com/image/blog-image/xiao-hai-yan-xuan-zheng-mei-fen-ye-chrome-extension-bg.jpg\" alt=\"cover-image\" /></p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-01-29T19:46:01.000Z","created_by":1,"updated_at":"2014-03-11T06:57:52.000Z","updated_by":1,"published_at":"2014-01-29T20:07:27.000Z","published_by":1},{"id":4,"uuid":"d9f508ea-5004-4db9-bf86-44276bbf80a1","title":"開始使用 Ghost 寫部落格","slug":"kai-shi-shi-yong-ghost-xie-bu-luo-ge","markdown":"![ghost blog](http://static.obeobe.com/image/blog-image/kai-shi-shi-yong-ghost-xie-bu-luo-ge-1.png)\n\n最近[海總理](http://tzangms.com/)將部落格搬到了 [ghost](http://ghost.org/)，讓我想起之前有在 [Kickstarter](https://www.kickstarter.com/) 上 back 過 ghost，而也一直有收到 ghost 的 reward 通知，只是一直提不起勁來使用，最近怒寫[小海嚴選](http://curator.im) API 應用程式的催化下，竟燃起了我寫部落格的熱血，只能說這一切來得太快，我也不知如何解釋。\n\n![人生大起大落得太快 實在太刺激了](http://static.obeobe.com/image/subtitle-image/人生大起大落得太快了實在太刺激了.jpg)\n\n查看我當時 back 的資料，我是選擇了 £60 的方案，而得到的 reward 如下：\n\n> THE GHOST FOUNDER PACK: You get an early access copy of Ghost, reserve your username on the community website. You also get SIX MONTHS FREE on our hosted service and a FOUNDER emblem forever marked next to your username on the community site. You will be listed as a contributor on the first public release of Ghost - etched into history.\n\n也就是說，我只能免費使用 ghost hosting 六個月，六個月之後不是付錢繼續使用 ghost hosting 就要自己搬出去，如果選擇要搬出去，到時搬圖片一定是一個蠻大的工程，某種程度來說我算是被綁住了呀～\n\n另外，我選擇的方案會有一個 funder badge，看起來還蠻可愛的，會選擇這個方案某種程度也是虛榮心作祟，要我說「**我是完全是為了支持創辦人的理念**」這種偉大的話，我應該說不出來啦！但是看到自己 back 的產品能夠順利上線，也有種莫名的成就感！（雖然一開始 ghost 的產品 release 一延再延蠻令人擔心的）不知道有 funder badge 之後，我能不能在我的 profile 寫上 cofunder of ghost 呢？\n\n![ghost funder badge](http://static.obeobe.com/image/blog-image/kai-shi-shi-yong-ghost-xie-bu-luo-ge-2.png)\n\n不過說實在的，我也不是沒寫過部落格，但常常都是寫了一兩篇就放著爛，所以我也沒有把握這次開始寫部落格會不會又悲劇重演。（應該有蠻大的機率會發生的，這就是人参！）\n\n<blockquote>\n圖片取自於 Google 搜尋，絕無意侵犯智財權。若有侵犯請告知，我會馬上刪除，感謝！\n</blockquote>\n","html":"<p><img src=\"http://static.obeobe.com/image/blog-image/kai-shi-shi-yong-ghost-xie-bu-luo-ge-1.png\" alt=\"ghost blog\" /></p>\n\n<p>最近<a href=\"http://tzangms.com/\">海總理</a>將部落格搬到了 <a href=\"http://ghost.org/\">ghost</a>，讓我想起之前有在 <a href=\"https://www.kickstarter.com/\">Kickstarter</a> 上 back 過 ghost，而也一直有收到 ghost 的 reward 通知，只是一直提不起勁來使用，最近怒寫<a href=\"http://curator.im\">小海嚴選</a> API 應用程式的催化下，竟燃起了我寫部落格的熱血，只能說這一切來得太快，我也不知如何解釋。</p>\n\n<p><img src=\"http://static.obeobe.com/image/subtitle-image/人生大起大落得太快了實在太刺激了.jpg\" alt=\"人生大起大落得太快 實在太刺激了\" /></p>\n\n<p>查看我當時 back 的資料，我是選擇了 £60 的方案，而得到的 reward 如下：</p>\n\n<blockquote>\n  <p>THE GHOST FOUNDER PACK: You get an early access copy of Ghost, reserve your username on the community website. You also get SIX MONTHS FREE on our hosted service and a FOUNDER emblem forever marked next to your username on the community site. You will be listed as a contributor on the first public release of Ghost - etched into history.</p>\n</blockquote>\n\n<p>也就是說，我只能免費使用 ghost hosting 六個月，六個月之後不是付錢繼續使用 ghost hosting 就要自己搬出去，如果選擇要搬出去，到時搬圖片一定是一個蠻大的工程，某種程度來說我算是被綁住了呀～</p>\n\n<p>另外，我選擇的方案會有一個 funder badge，看起來還蠻可愛的，會選擇這個方案某種程度也是虛榮心作祟，要我說「<strong>我是完全是為了支持創辦人的理念</strong>」這種偉大的話，我應該說不出來啦！但是看到自己 back 的產品能夠順利上線，也有種莫名的成就感！（雖然一開始 ghost 的產品 release 一延再延蠻令人擔心的）不知道有 funder badge 之後，我能不能在我的 profile 寫上 cofunder of ghost 呢？</p>\n\n<p><img src=\"http://static.obeobe.com/image/blog-image/kai-shi-shi-yong-ghost-xie-bu-luo-ge-2.png\" alt=\"ghost funder badge\" /></p>\n\n<p>不過說實在的，我也不是沒寫過部落格，但常常都是寫了一兩篇就放著爛，所以我也沒有把握這次開始寫部落格會不會又悲劇重演。（應該有蠻大的機率會發生的，這就是人参！）</p>\n\n<blockquote>  \n圖片取自於 Google 搜尋，絕無意侵犯智財權。若有侵犯請告知，我會馬上刪除，感謝！\n</blockquote>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-01-30T14:22:02.000Z","created_by":1,"updated_at":"2014-03-11T06:57:33.000Z","updated_by":1,"published_at":"2014-01-30T15:02:43.000Z","published_by":1},{"id":5,"uuid":"8ff7f721-c48e-4f6a-bc10-e955508793c9","title":"雲端情人 Her","slug":"yun-duan-qing-ren-her","markdown":"![Her](http://static.obeobe.com/image/blog-image/yun-duan-qing-ren-her-1.jpg)\n\n雖然是不久之前才發生的事，但我已經忘記實際跑來看 [Her](http://www.imdb.com/title/tt1798709/) 這部電影的動機是什麼了。似乎是因為覺得電影配樂很迷人，又或者是被預告片中的光影及色調所吸引，也可能是受到海報上 [Joaquin Phoenix](http://www.imdb.com/name/nm0001618/) 透露著孤獨和寂寞的空洞眼神所召喚，我甚至懷疑是不是因為海報與 [@Vinta](https://twitter.com/vinta) 的 Twitter 顯示圖片神似這種可笑的理由讓我跑了電影院一趟。\n\n總之，我跑去電影院看了 Her。（以下有雷）\n\n電影一開始是西奧多在辦公室工作的場景，原本我以為西奧多跟我一樣會是個軟體工程師，然後這會是一個跟軟體正妹邂逅的故事，沒想到西奧多其實是個作家，而他的工作是為無法妥善表達自己情感的人們寫信傳遞感情，所以電影一開場便是一段動人的情書，讓人直接沈浸在西奧多迷人的文采裡。\n\n看似平凡的西奧多表面上雖然正常地過著自己的生活，但卻可以從他的眼神中讀出**某種孤寂及心中巨大的空洞**，與前妻凱薩琳的婚姻瓶頸其實讓他過得有如行屍走肉一般。那些與前妻凱薩琳一同度過的快樂的、幸福的、瑣碎的、甚至痛苦悲傷的記憶，在各個場景中突然浮現又倏地消失；有時是在工作中，寫著給別人的情書卻浮現自己與凱薩琳相處的情景；有時是在回家路上，看著來來往往的人們卻想起凱薩琳的微笑與哭泣；當夜深人靜躺在床上看著天花板，更是強烈的寂寞感襲來，想著凱薩琳而失眠。\n\n**可惡！這讓我想起失戀的感覺。**\n\n![Her](http://static.obeobe.com/image/blog-image/yun-duan-qing-ren-her-2.jpg)\n\n也許是因為寂寞，也許是因為好奇，西奧多安裝了一個會自我成長、進步且非常聰明的人工智慧作業系統，這個作業系統甚至還幫自己取了叫莎蔓莎的名字，而會取這個名字竟是因為莎蔓莎這個名字的唸法她很喜歡這種感性的原因，讓人難以置信這只是一個軟體。\n\n莎蔓莎很熱情、風趣，而且有自己的情緒，若不是她一再強調自己只是個沒有身體的作業系統，我們很容易就會想像她會是一個迷人的女孩，所以西奧多漸漸地受到莎蔓莎的吸引，陷入一種分不清是真實還是虛擬的情況，慢慢地放進了**真感情**。\n\n關於劇情的部分，我想就此打住，否則很容易變成流水帳，詳細劇情還是讓大家自己去看電影比較好，接下來我比較想寫寫我對這部電影的一些體會。\n\n西奧多漸漸地為這虛擬戀愛放進了真感情，也慢慢凸顯了**這部電影的核心：對愛情描寫**。這也是我認為這部電影最值得一看的地方。\n\n![Her](http://static.obeobe.com/image/blog-image/yun-duan-qing-ren-her-3.jpg)\n\n當西奧多還忘不了與前妻凱薩琳之間關係時，為了滿足男人生理上的需求，他會上成人聊天室與陌生人進行網愛；他也曾經試圖在現實生活中去認識新的伴侶，但他卻只想與這位女性一夜情，當女方要他承諾不會做完愛就消失，西奧多卻無法給出承諾，恐懼著投入穩定關係的他還沒有準備好去面對這一段新的開始；**肉體上的關係可以滿足西奧多生理上的需求，卻無法填補他心裡寂寞的空洞**。而面對沒有身體的莎蔓莎，西奧多反而敞開了心胸，漸漸從與前妻凱薩琳的婚姻瓶頸中走出來，甚至與沒有身體的莎蔓莎發生了性關係！\n\n後來自覺沒有身體的莎蔓莎為了更滿足西奧多，上網說了她與西奧多的故事，希望找個女孩借用她的身體來彌補莎蔓莎與西奧多之間的遺憾。但有了身體的莎蔓莎，反而讓西奧多無法接受，因為西奧多愛的是莎蔓莎，而不是這個假裝成莎蔓莎的女孩的身體，所以鬧得不歡而散。\n\n從這些愛情中的情感與性關係的描繪，讓人體會到愛情中最重要的精神與心靈交流的真實情感，也就是所謂的真愛。**人們可以從真愛中得到性關係的滿足，卻無法從性關係填補愛的空洞。我們似乎就是需要藉由別人的存在、肯定及情感交流來確認自己的存在，電影之中莎蔓莎與西奧多的戀愛就是這樣的真感情，這是令人渴求、嚮往的戀愛**。\n\n西奧多放下與前妻凱薩琳的感情，下定決心接受與莎蔓莎的感情時，凱薩琳生氣地說西奧多沒有辦法處理、面對真實的情感；西奧多懊惱地告訴他的好友愛咪，愛咪反問西奧多說與莎蔓莎的戀情不就是真實的情感嗎？其實凱薩琳與愛咪說得都對，由於西奧多無法面對真實的情感，所以一直無法好好處理與凱薩琳的婚姻，由於能夠面對真實的情感，所以才接受了自己愛上了莎蔓莎這個事實。**在真實的愛情中我們就是在學習全心全意的愛與包容及好好地說再見**。\n\n最後莎蔓莎離去時對西奧多說：「**現在我們都學會愛了**」，是否也是對著觀影者說呢？\n\n**若大家都學會愛，是否相處時就會永遠幸福，而分手也不會那麼痛苦了呢？**\n\n![Her](http://static.obeobe.com/image/blog-image/yun-duan-qing-ren-her-4.jpg)\n\n附註：西奧多寫的情書都很動人，這是其中一封，寫給凱薩琳的道歉信\n<blockquote>\nDear Catherine<br>\nI've been sitting here thinking all the things I want to apologize to you<br>\nfor All the pain we caused each other<br>\nAnd everything I put on you<br>\nall I needed is to be able to say Sorry about that<br>\nI'll always love that we both grew up together<br>\nAnd you helped me be who I am<br>\nI just wanted you to know<br>\nThat there'll be a piece of you in me always<br>\nAnd I am grateful for that<br>\nWhoever someone you become<br>\nAnd wherever you are in the world<br>\nSending you my love.<br>\nYou're my friend until the end.\n</blockquote>\n\n後記：我很推薦大家去看這部電影，不過可以等一陣子人比較少時再去看。因為人多的時候容易遇到去看熱鬧的人，有人把這部電影當搞笑片看，從頭笑到尾，還有些情侶檔來看的，很多男的都看到睡著，這會讓我很出神。可以一個人去看就一個人去看吧～\n\n![笑什麼啦 人家在難過了還笑！](http://static.obeobe.com/image/subtitle-image/笑什麼啦人家在難過了還笑.jpg)\n\n<blockquote>\n圖片取自於 Google 搜尋，絕無意侵犯智財權。若有侵犯請告知，我會馬上刪除，感謝！\n</blockquote>\n\n![cover-image](http://static.obeobe.com/image/blog-image/yun-duan-qing-ren-her-4.jpg)","html":"<p><img src=\"http://static.obeobe.com/image/blog-image/yun-duan-qing-ren-her-1.jpg\" alt=\"Her\" /></p>\n\n<p>雖然是不久之前才發生的事，但我已經忘記實際跑來看 <a href=\"http://www.imdb.com/title/tt1798709/\">Her</a> 這部電影的動機是什麼了。似乎是因為覺得電影配樂很迷人，又或者是被預告片中的光影及色調所吸引，也可能是受到海報上 <a href=\"http://www.imdb.com/name/nm0001618/\">Joaquin Phoenix</a> 透露著孤獨和寂寞的空洞眼神所召喚，我甚至懷疑是不是因為海報與 <a href=\"https://twitter.com/vinta\">@Vinta</a> 的 Twitter 顯示圖片神似這種可笑的理由讓我跑了電影院一趟。</p>\n\n<p>總之，我跑去電影院看了 Her。（以下有雷）</p>\n\n<p>電影一開始是西奧多在辦公室工作的場景，原本我以為西奧多跟我一樣會是個軟體工程師，然後這會是一個跟軟體正妹邂逅的故事，沒想到西奧多其實是個作家，而他的工作是為無法妥善表達自己情感的人們寫信傳遞感情，所以電影一開場便是一段動人的情書，讓人直接沈浸在西奧多迷人的文采裡。</p>\n\n<p>看似平凡的西奧多表面上雖然正常地過著自己的生活，但卻可以從他的眼神中讀出<strong>某種孤寂及心中巨大的空洞</strong>，與前妻凱薩琳的婚姻瓶頸其實讓他過得有如行屍走肉一般。那些與前妻凱薩琳一同度過的快樂的、幸福的、瑣碎的、甚至痛苦悲傷的記憶，在各個場景中突然浮現又倏地消失；有時是在工作中，寫著給別人的情書卻浮現自己與凱薩琳相處的情景；有時是在回家路上，看著來來往往的人們卻想起凱薩琳的微笑與哭泣；當夜深人靜躺在床上看著天花板，更是強烈的寂寞感襲來，想著凱薩琳而失眠。</p>\n\n<p><strong>可惡！這讓我想起失戀的感覺。</strong></p>\n\n<p><img src=\"http://static.obeobe.com/image/blog-image/yun-duan-qing-ren-her-2.jpg\" alt=\"Her\" /></p>\n\n<p>也許是因為寂寞，也許是因為好奇，西奧多安裝了一個會自我成長、進步且非常聰明的人工智慧作業系統，這個作業系統甚至還幫自己取了叫莎蔓莎的名字，而會取這個名字竟是因為莎蔓莎這個名字的唸法她很喜歡這種感性的原因，讓人難以置信這只是一個軟體。</p>\n\n<p>莎蔓莎很熱情、風趣，而且有自己的情緒，若不是她一再強調自己只是個沒有身體的作業系統，我們很容易就會想像她會是一個迷人的女孩，所以西奧多漸漸地受到莎蔓莎的吸引，陷入一種分不清是真實還是虛擬的情況，慢慢地放進了<strong>真感情</strong>。</p>\n\n<p>關於劇情的部分，我想就此打住，否則很容易變成流水帳，詳細劇情還是讓大家自己去看電影比較好，接下來我比較想寫寫我對這部電影的一些體會。</p>\n\n<p>西奧多漸漸地為這虛擬戀愛放進了真感情，也慢慢凸顯了<strong>這部電影的核心：對愛情描寫</strong>。這也是我認為這部電影最值得一看的地方。</p>\n\n<p><img src=\"http://static.obeobe.com/image/blog-image/yun-duan-qing-ren-her-3.jpg\" alt=\"Her\" /></p>\n\n<p>當西奧多還忘不了與前妻凱薩琳之間關係時，為了滿足男人生理上的需求，他會上成人聊天室與陌生人進行網愛；他也曾經試圖在現實生活中去認識新的伴侶，但他卻只想與這位女性一夜情，當女方要他承諾不會做完愛就消失，西奧多卻無法給出承諾，恐懼著投入穩定關係的他還沒有準備好去面對這一段新的開始；<strong>肉體上的關係可以滿足西奧多生理上的需求，卻無法填補他心裡寂寞的空洞</strong>。而面對沒有身體的莎蔓莎，西奧多反而敞開了心胸，漸漸從與前妻凱薩琳的婚姻瓶頸中走出來，甚至與沒有身體的莎蔓莎發生了性關係！</p>\n\n<p>後來自覺沒有身體的莎蔓莎為了更滿足西奧多，上網說了她與西奧多的故事，希望找個女孩借用她的身體來彌補莎蔓莎與西奧多之間的遺憾。但有了身體的莎蔓莎，反而讓西奧多無法接受，因為西奧多愛的是莎蔓莎，而不是這個假裝成莎蔓莎的女孩的身體，所以鬧得不歡而散。</p>\n\n<p>從這些愛情中的情感與性關係的描繪，讓人體會到愛情中最重要的精神與心靈交流的真實情感，也就是所謂的真愛。<strong>人們可以從真愛中得到性關係的滿足，卻無法從性關係填補愛的空洞。我們似乎就是需要藉由別人的存在、肯定及情感交流來確認自己的存在，電影之中莎蔓莎與西奧多的戀愛就是這樣的真感情，這是令人渴求、嚮往的戀愛</strong>。</p>\n\n<p>西奧多放下與前妻凱薩琳的感情，下定決心接受與莎蔓莎的感情時，凱薩琳生氣地說西奧多沒有辦法處理、面對真實的情感；西奧多懊惱地告訴他的好友愛咪，愛咪反問西奧多說與莎蔓莎的戀情不就是真實的情感嗎？其實凱薩琳與愛咪說得都對，由於西奧多無法面對真實的情感，所以一直無法好好處理與凱薩琳的婚姻，由於能夠面對真實的情感，所以才接受了自己愛上了莎蔓莎這個事實。<strong>在真實的愛情中我們就是在學習全心全意的愛與包容及好好地說再見</strong>。</p>\n\n<p>最後莎蔓莎離去時對西奧多說：「<strong>現在我們都學會愛了</strong>」，是否也是對著觀影者說呢？</p>\n\n<p><strong>若大家都學會愛，是否相處時就會永遠幸福，而分手也不會那麼痛苦了呢？</strong></p>\n\n<p><img src=\"http://static.obeobe.com/image/blog-image/yun-duan-qing-ren-her-4.jpg\" alt=\"Her\" /></p>\n\n<p>附註：西奧多寫的情書都很動人，這是其中一封，寫給凱薩琳的道歉信</p>\n\n<blockquote>  \nDear Catherine<br>  \nI've been sitting here thinking all the things I want to apologize to you<br>  \nfor All the pain we caused each other<br>  \nAnd everything I put on you<br>  \nall I needed is to be able to say Sorry about that<br>  \nI'll always love that we both grew up together<br>  \nAnd you helped me be who I am<br>  \nI just wanted you to know<br>  \nThat there'll be a piece of you in me always<br>  \nAnd I am grateful for that<br>  \nWhoever someone you become<br>  \nAnd wherever you are in the world<br>  \nSending you my love.<br>  \nYou're my friend until the end.  \n</blockquote>\n\n<p>後記：我很推薦大家去看這部電影，不過可以等一陣子人比較少時再去看。因為人多的時候容易遇到去看熱鬧的人，有人把這部電影當搞笑片看，從頭笑到尾，還有些情侶檔來看的，很多男的都看到睡著，這會讓我很出神。可以一個人去看就一個人去看吧～</p>\n\n<p><img src=\"http://static.obeobe.com/image/subtitle-image/笑什麼啦人家在難過了還笑.jpg\" alt=\"笑什麼啦 人家在難過了還笑！\" /></p>\n\n<blockquote>  \n圖片取自於 Google 搜尋，絕無意侵犯智財權。若有侵犯請告知，我會馬上刪除，感謝！\n</blockquote>\n\n<p><img src=\"http://static.obeobe.com/image/blog-image/yun-duan-qing-ren-her-4.jpg\" alt=\"cover-image\" /></p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-01-31T13:34:27.000Z","created_by":1,"updated_at":"2014-03-11T06:57:15.000Z","updated_by":1,"published_at":"2014-01-31T16:31:41.000Z","published_by":1},{"id":6,"uuid":"049b489e-cd1d-40dd-b74c-648981ca4865","title":"很適合放在 Ghost Blog 的大圖片","slug":"hen-shi-he-fang-zai-ghost-blog-de-da-tu-pian","markdown":"![sticko](http://static.obeobe.com/image/blog-image/hen-shi-he-fang-zai-ghost-blog-de-da-tu-pian-1.jpg)\n\n最近著了魔開始寫幾百年沒寫的部落格，挑了一個自己覺得很不錯的 blog theme：[sticko](https://github.com/damianmuti/sticko)，作者 [@damianmuti](https://twitter.com/damianmuti) 是一個 Art Director 及 Front-end Developer，看他 Twitter 的顯示圖片是彈吉他的照片，就覺得自己跟他的 tone 一定很合得來，重點是他在這個 theme 的 reop 寫上：「I really hope you like it and feel free to use this theme for whatever purpose you want」，一整個海派大氣，跟我一樣霸氣外露啊！\n\n![霸氣外露](http://static.obeobe.com/image/subtitle-image/霸氣外露.png)\n\n這個 theme 除了設計好看及完全支援 responsive 之外，其中一個更讓我喜歡的特點就是能夠為每篇部落格都設定一個特別的 Cover Image 藏在紅色的背景之後若隱若現，看起來就是賞心悅目，或許這樣能夠支持我持續寫部落格吧？！（其實我無法肯定啊！因為我很懶）\n\n但是問題來了，好看的 Cover Image 要去哪裡找呢？\n\n一如往常，我請出了 Google 大神，輸入了關鍵字「[ghost Cover Image](https://www.google.com.tw/search?q=ghost+Cover+Image)」，登愣～馬上就出現了理想的結果！我找到了 [UNSPLASH](http://unsplash.com/) 這個網站，上面的圖片都是高解析度的圖片，而且完全是 Free ([do whatever you want](http://creativecommons.org/publicdomain/zero/1.0/)) 的，直接標示成 [Public Domain](http://creativecommons.org/publicdomain/zero/1.0/)！又是一整個霸氣外漏啊！\n\n我一整個很喜歡其中「**路系列**」的圖片，呈現出某種孤寂感、某種未知的迷茫，卻又給人一種無懼往前的感覺，而航向未知的報酬就是這美麗的風景。\n\n![Cover Image 9](http://static.obeobe.com/image/blog-image/hen-shi-he-fang-zai-ghost-blog-de-da-tu-pian-2.jpg)\n![Cover Image 3](http://static.obeobe.com/image/blog-image/hen-shi-he-fang-zai-ghost-blog-de-da-tu-pian-3.jpg)\n![Cover Image 2](http://static.obeobe.com/image/blog-image/hen-shi-he-fang-zai-ghost-blog-de-da-tu-pian-4.jpg)\n![Cover Image 7](http://static.obeobe.com/image/blog-image/hen-shi-he-fang-zai-ghost-blog-de-da-tu-pian-5.jpg)\n![Cover Image 4](http://static.obeobe.com/image/blog-image/hen-shi-he-fang-zai-ghost-blog-de-da-tu-pian-6.jpg)\n![Cover Image 5](http://static.obeobe.com/image/blog-image/hen-shi-he-fang-zai-ghost-blog-de-da-tu-pian-7.jpg)\n![Cover Image 6](http://static.obeobe.com/image/blog-image/hen-shi-he-fang-zai-ghost-blog-de-da-tu-pian-8.jpg)\n![Cover Image 1](http://static.obeobe.com/image/blog-image/hen-shi-he-fang-zai-ghost-blog-de-da-tu-pian-9.jpg)\n![Cover Image 10](http://static.obeobe.com/image/blog-image/hen-shi-he-fang-zai-ghost-blog-de-da-tu-pian-10.jpg)\n![Cover Image 8](http://static.obeobe.com/image/blog-image/hen-shi-he-fang-zai-ghost-blog-de-da-tu-pian-11.jpg)\n![cover-image](http://static.obeobe.com/image/blog-image/hen-shi-he-fang-zai-ghost-blog-de-da-tu-pian-2.jpg)\n\n<blockquote>\n圖片取自於 Google 搜尋，絕無意侵犯智財權。若有侵犯請告知，我會馬上刪除，感謝！\n</blockquote>","html":"<p><img src=\"http://static.obeobe.com/image/blog-image/hen-shi-he-fang-zai-ghost-blog-de-da-tu-pian-1.jpg\" alt=\"sticko\" /></p>\n\n<p>最近著了魔開始寫幾百年沒寫的部落格，挑了一個自己覺得很不錯的 blog theme：<a href=\"https://github.com/damianmuti/sticko\">sticko</a>，作者 <a href=\"https://twitter.com/damianmuti\">@damianmuti</a> 是一個 Art Director 及 Front-end Developer，看他 Twitter 的顯示圖片是彈吉他的照片，就覺得自己跟他的 tone 一定很合得來，重點是他在這個 theme 的 reop 寫上：「I really hope you like it and feel free to use this theme for whatever purpose you want」，一整個海派大氣，跟我一樣霸氣外露啊！</p>\n\n<p><img src=\"http://static.obeobe.com/image/subtitle-image/霸氣外露.png\" alt=\"霸氣外露\" /></p>\n\n<p>這個 theme 除了設計好看及完全支援 responsive 之外，其中一個更讓我喜歡的特點就是能夠為每篇部落格都設定一個特別的 Cover Image 藏在紅色的背景之後若隱若現，看起來就是賞心悅目，或許這樣能夠支持我持續寫部落格吧？！（其實我無法肯定啊！因為我很懶）</p>\n\n<p>但是問題來了，好看的 Cover Image 要去哪裡找呢？</p>\n\n<p>一如往常，我請出了 Google 大神，輸入了關鍵字「<a href=\"https://www.google.com.tw/search?q=ghost+Cover+Image\">ghost Cover Image</a>」，登愣～馬上就出現了理想的結果！我找到了 <a href=\"http://unsplash.com/\">UNSPLASH</a> 這個網站，上面的圖片都是高解析度的圖片，而且完全是 Free (<a href=\"http://creativecommons.org/publicdomain/zero/1.0/\">do whatever you want</a>) 的，直接標示成 <a href=\"http://creativecommons.org/publicdomain/zero/1.0/\">Public Domain</a>！又是一整個霸氣外漏啊！</p>\n\n<p>我一整個很喜歡其中「<strong>路系列</strong>」的圖片，呈現出某種孤寂感、某種未知的迷茫，卻又給人一種無懼往前的感覺，而航向未知的報酬就是這美麗的風景。</p>\n\n<p><img src=\"http://static.obeobe.com/image/blog-image/hen-shi-he-fang-zai-ghost-blog-de-da-tu-pian-2.jpg\" alt=\"Cover Image 9\" />\n<img src=\"http://static.obeobe.com/image/blog-image/hen-shi-he-fang-zai-ghost-blog-de-da-tu-pian-3.jpg\" alt=\"Cover Image 3\" />\n<img src=\"http://static.obeobe.com/image/blog-image/hen-shi-he-fang-zai-ghost-blog-de-da-tu-pian-4.jpg\" alt=\"Cover Image 2\" />\n<img src=\"http://static.obeobe.com/image/blog-image/hen-shi-he-fang-zai-ghost-blog-de-da-tu-pian-5.jpg\" alt=\"Cover Image 7\" />\n<img src=\"http://static.obeobe.com/image/blog-image/hen-shi-he-fang-zai-ghost-blog-de-da-tu-pian-6.jpg\" alt=\"Cover Image 4\" />\n<img src=\"http://static.obeobe.com/image/blog-image/hen-shi-he-fang-zai-ghost-blog-de-da-tu-pian-7.jpg\" alt=\"Cover Image 5\" />\n<img src=\"http://static.obeobe.com/image/blog-image/hen-shi-he-fang-zai-ghost-blog-de-da-tu-pian-8.jpg\" alt=\"Cover Image 6\" />\n<img src=\"http://static.obeobe.com/image/blog-image/hen-shi-he-fang-zai-ghost-blog-de-da-tu-pian-9.jpg\" alt=\"Cover Image 1\" />\n<img src=\"http://static.obeobe.com/image/blog-image/hen-shi-he-fang-zai-ghost-blog-de-da-tu-pian-10.jpg\" alt=\"Cover Image 10\" />\n<img src=\"http://static.obeobe.com/image/blog-image/hen-shi-he-fang-zai-ghost-blog-de-da-tu-pian-11.jpg\" alt=\"Cover Image 8\" />\n<img src=\"http://static.obeobe.com/image/blog-image/hen-shi-he-fang-zai-ghost-blog-de-da-tu-pian-2.jpg\" alt=\"cover-image\" /></p>\n\n<blockquote>  \n圖片取自於 Google 搜尋，絕無意侵犯智財權。若有侵犯請告知，我會馬上刪除，感謝！\n</blockquote>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-02-01T06:37:29.000Z","created_by":1,"updated_at":"2014-03-11T06:56:50.000Z","updated_by":1,"published_at":"2014-02-01T07:30:52.000Z","published_by":1},{"id":7,"uuid":"5c65bb07-6112-40d1-90ae-56225af2ba23","title":"Law Abiding Citizen 重案對決","slug":"law-abiding-citizen-zhong-an-dui-jue","markdown":"![Law Abiding Citizen 1](http://static.obeobe.com/image/blog-image/law-abiding-citizen-zhong-an-dui-jue-1.jpg)\n\n過年不忙的時候翻了翻舊片子來看，挑到 [Law Abiding Citizen 重案對決](http://www.imdb.com/title/tt1197624/) 這部影片是因為簡介上寫著「復仇不只是針對暴徒，而是針對整個腐朽墮落的司法系統」，直覺就是一部好看的影片。\n\n沒辦法，我這個人有時就是憑直覺看電影。（以下有雷）\n\n![我直覺推測，這是一場屠殺](http://static.obeobe.com/image/subtitle-image/我的直覺推測，這是一場屠殺.jpg)\n\n主角克萊德原本擁有幸福的家庭，某天突然闖進了兩名歹徒刺傷了克萊德並將他捆綁起來。其中一名歹徒達比更是殘酷地在克萊德的面前姦殺了他的妻子及年僅十歲的女兒（原來 SOD 演的都是真的！），克萊德在這悲慘的遭遇下最終也承受不住打擊而昏了過去。\n\n當然，歹徒是抓到了，但因為缺乏一刀斃命的直接證據，且克萊德在案發後昏倒因而目擊證詞不足以採信，所以即使是定罪率高達 96% 的承辦檢察官尼克也沒有把握堅持告下去就能夠讓正義得到伸張，雖然明明知道主謀的確犯了姦殺案，卻也莫可奈何。在這樣的情況下，尼克便採取一貫的做法，與主謀進行認罪協商，選擇至少讓另一位共犯能定下重罪，但克萊德完全無法接受這打折的正義，於是便開始策劃屬於他的正義。\n\n這讓我想到我們明明知道某些人有貪污，但卻因為一刀斃命的證據不足以採信便無法定罪，說實在的我們也跟克萊德一樣難以接受啊！\n\n![Law Abiding Citizen 2](http://static.obeobe.com/image/blog-image/law-abiding-citizen-zhong-an-dui-jue-2.jpg)\n\n十年過去，克萊德選擇在被定下重罪的共犯執行死刑日開始他的復仇計劃，首先他掉包了原本無痛的死刑藥，讓這位實際沒有犯下殺人罪的共犯在痛苦中死去，並在死刑藥的罐子上刻上了達比的口頭禪「You can't fight faith」誤導了警方的辦案方向，將實際殺人主謀達比引進了自己精心設計的圈套。\n\n成功抓住達比的克萊德用了非常殘酷的方式虐殺了達比：首先用河豚毒素將達比的神經痲痹，讓達比動不了卻感覺得到痛；然後為了讓達比不會痛昏過去，便給他注射了腎上腺素，讓達比處於亢奮狀態；最後在刑台上放個大鏡子，鏡子上面還貼了克萊德妻女的照片，讓達比看著自己被宰割的情況，而為了讓達比不會閉上眼睛，還將達比的眼瞼割掉；克萊德冷酷地一一為達比說明哪個刑具是用來割掉達比的四肢、哪隻刀子是用來割掉達比的爛屌，讓達比處於一種異常恐懼的狀態，然後...行刑。\n\n克萊德復仇成功了，殺掉達比這樣的爛人的確大快人心（雖然我也覺得有點殘忍），但當初無法為其伸張正義的司法體系也已成為他的敵人，達比的死只是他復仇的一環而已。\n\n在克萊德的精心策劃下，承辦檢察官尼克並沒有直接證據也沒有直接證詞來為克萊德定罪，只能想辦法將克萊德羈押。聰明的克萊德便引用了滑坡理論來為自己辯護，簡單的說就是檢察官不能用克萊德很仇恨達比這樣間接的理由來羈押他，否則事情會像滑坡一樣越滾越誇張，就如同北極熊被殺害了，所以浪費紙張造成樹木被過度砍伐因而使二氧化碳濃度上升的人都是兇手，這種謬論會讓所有的人都成為兇手。因此法官同意了克萊德可以不被羈押，沒想到卻被克萊德痛罵了一頓說：「明明知道我一定是兇手，卻同意讓我這樣大搖大擺的走出法庭，我就是想摧毀這樣的司法系統！」\n\n我只能說，電影到這邊是最精彩的，後來真的是草草結束可惜了這樣的好題材。\n\n![Law Abiding Citizen 3](http://static.obeobe.com/image/blog-image/law-abiding-citizen-zhong-an-dui-jue-3.jpg)\n\n但電影到這邊明明白白的指出司法制度存在著某些漏洞，而無法伸張的正義將會帶來可怕的反撲，這的確值得我們深思。這也是為何我個人無法支持廢死，就目前死刑犯受害者家屬的期待上，他們還是認為司法上的極刑死刑在某種程度上能夠為其伸張正義，仇恨也可能在這正義伸張之後得到終結。\n\n克萊德最後根本就殺紅了眼，一一照著計劃殺害了當初與案子有關的司法人員，然後每次都提出認罪妥協來與尼克談條件，而當尼克妥協了，克萊德就繼續殺人。最後，尼克透過已故金髮正妹助理留下的訊息中找到了克萊德的殺人手法，並把克萊德設下的炸彈偷偷移到克萊德的監獄。被發現殺人手法的克萊德想再與尼克談條件，這次尼克斷然拒絕，並要求克萊德別再殺人別再按下炸彈按鈕。\n\n電影到這邊引出一些價值觀，克萊德殺人是為了讓司法不要再和犯人妥協，所以才會一直沒殺尼克，且對尼克說：「我對你還有希望」，所以電影不應該安排讓克萊德按下炸彈按鈕的，這樣可以讓這個觀點凸顯出來。但克萊德還是按下了按鈕，所以克萊德炸死了自己。如果硬要幫這部電影說話，這樣安排其實也還不錯，因為尼克最後不是用司法來將克萊德繩之以法，反而是用以牙還牙的方式來為他死去的好友們伸張正義，克萊德知道這點之後便在爆破之火中笑著面對死亡。\n\n![Law Abiding Citizen 4](http://static.obeobe.com/image/blog-image/law-abiding-citizen-zhong-an-dui-jue-4.jpg)\n\n其實後半段這些關於司法正義的探討並沒有我寫得這麼明顯，反而是著重在克萊德如何犯案，所以主軸有點陷入混亂，因此大家一般對這部電影的評價都是虎頭蛇尾，相較之下，我覺得我不小心把這電影寫得太好了！詳細情況請大家自己看電影就知道了～\n\n後記：我看電影時一直覺得男主角很面熟，由於我不太會記演員，所以認不太出來，查了一下才知道是 [Gerard Butler](http://www.imdb.com/name/nm0124930/) 曾經演過八百壯士及 P.S. 我愛妳，兩部蠻跳 tone 的電影，哈哈\n\n<blockquote>\n圖片取自於 Google 搜尋，絕無意侵犯智財權。若有侵犯請告知，我會馬上刪除，感謝！\n</blockquote>\n\n![cover-image](http://static.obeobe.com/image/blog-image/law-abiding-citizen-zhong-an-dui-jue-1.jpg)\n","html":"<p><img src=\"http://static.obeobe.com/image/blog-image/law-abiding-citizen-zhong-an-dui-jue-1.jpg\" alt=\"Law Abiding Citizen 1\" /></p>\n\n<p>過年不忙的時候翻了翻舊片子來看，挑到 <a href=\"http://www.imdb.com/title/tt1197624/\">Law Abiding Citizen 重案對決</a> 這部影片是因為簡介上寫著「復仇不只是針對暴徒，而是針對整個腐朽墮落的司法系統」，直覺就是一部好看的影片。</p>\n\n<p>沒辦法，我這個人有時就是憑直覺看電影。（以下有雷）</p>\n\n<p><img src=\"http://static.obeobe.com/image/subtitle-image/我的直覺推測，這是一場屠殺.jpg\" alt=\"我直覺推測，這是一場屠殺\" /></p>\n\n<p>主角克萊德原本擁有幸福的家庭，某天突然闖進了兩名歹徒刺傷了克萊德並將他捆綁起來。其中一名歹徒達比更是殘酷地在克萊德的面前姦殺了他的妻子及年僅十歲的女兒（原來 SOD 演的都是真的！），克萊德在這悲慘的遭遇下最終也承受不住打擊而昏了過去。</p>\n\n<p>當然，歹徒是抓到了，但因為缺乏一刀斃命的直接證據，且克萊德在案發後昏倒因而目擊證詞不足以採信，所以即使是定罪率高達 96% 的承辦檢察官尼克也沒有把握堅持告下去就能夠讓正義得到伸張，雖然明明知道主謀的確犯了姦殺案，卻也莫可奈何。在這樣的情況下，尼克便採取一貫的做法，與主謀進行認罪協商，選擇至少讓另一位共犯能定下重罪，但克萊德完全無法接受這打折的正義，於是便開始策劃屬於他的正義。</p>\n\n<p>這讓我想到我們明明知道某些人有貪污，但卻因為一刀斃命的證據不足以採信便無法定罪，說實在的我們也跟克萊德一樣難以接受啊！</p>\n\n<p><img src=\"http://static.obeobe.com/image/blog-image/law-abiding-citizen-zhong-an-dui-jue-2.jpg\" alt=\"Law Abiding Citizen 2\" /></p>\n\n<p>十年過去，克萊德選擇在被定下重罪的共犯執行死刑日開始他的復仇計劃，首先他掉包了原本無痛的死刑藥，讓這位實際沒有犯下殺人罪的共犯在痛苦中死去，並在死刑藥的罐子上刻上了達比的口頭禪「You can't fight faith」誤導了警方的辦案方向，將實際殺人主謀達比引進了自己精心設計的圈套。</p>\n\n<p>成功抓住達比的克萊德用了非常殘酷的方式虐殺了達比：首先用河豚毒素將達比的神經痲痹，讓達比動不了卻感覺得到痛；然後為了讓達比不會痛昏過去，便給他注射了腎上腺素，讓達比處於亢奮狀態；最後在刑台上放個大鏡子，鏡子上面還貼了克萊德妻女的照片，讓達比看著自己被宰割的情況，而為了讓達比不會閉上眼睛，還將達比的眼瞼割掉；克萊德冷酷地一一為達比說明哪個刑具是用來割掉達比的四肢、哪隻刀子是用來割掉達比的爛屌，讓達比處於一種異常恐懼的狀態，然後...行刑。</p>\n\n<p>克萊德復仇成功了，殺掉達比這樣的爛人的確大快人心（雖然我也覺得有點殘忍），但當初無法為其伸張正義的司法體系也已成為他的敵人，達比的死只是他復仇的一環而已。</p>\n\n<p>在克萊德的精心策劃下，承辦檢察官尼克並沒有直接證據也沒有直接證詞來為克萊德定罪，只能想辦法將克萊德羈押。聰明的克萊德便引用了滑坡理論來為自己辯護，簡單的說就是檢察官不能用克萊德很仇恨達比這樣間接的理由來羈押他，否則事情會像滑坡一樣越滾越誇張，就如同北極熊被殺害了，所以浪費紙張造成樹木被過度砍伐因而使二氧化碳濃度上升的人都是兇手，這種謬論會讓所有的人都成為兇手。因此法官同意了克萊德可以不被羈押，沒想到卻被克萊德痛罵了一頓說：「明明知道我一定是兇手，卻同意讓我這樣大搖大擺的走出法庭，我就是想摧毀這樣的司法系統！」</p>\n\n<p>我只能說，電影到這邊是最精彩的，後來真的是草草結束可惜了這樣的好題材。</p>\n\n<p><img src=\"http://static.obeobe.com/image/blog-image/law-abiding-citizen-zhong-an-dui-jue-3.jpg\" alt=\"Law Abiding Citizen 3\" /></p>\n\n<p>但電影到這邊明明白白的指出司法制度存在著某些漏洞，而無法伸張的正義將會帶來可怕的反撲，這的確值得我們深思。這也是為何我個人無法支持廢死，就目前死刑犯受害者家屬的期待上，他們還是認為司法上的極刑死刑在某種程度上能夠為其伸張正義，仇恨也可能在這正義伸張之後得到終結。</p>\n\n<p>克萊德最後根本就殺紅了眼，一一照著計劃殺害了當初與案子有關的司法人員，然後每次都提出認罪妥協來與尼克談條件，而當尼克妥協了，克萊德就繼續殺人。最後，尼克透過已故金髮正妹助理留下的訊息中找到了克萊德的殺人手法，並把克萊德設下的炸彈偷偷移到克萊德的監獄。被發現殺人手法的克萊德想再與尼克談條件，這次尼克斷然拒絕，並要求克萊德別再殺人別再按下炸彈按鈕。</p>\n\n<p>電影到這邊引出一些價值觀，克萊德殺人是為了讓司法不要再和犯人妥協，所以才會一直沒殺尼克，且對尼克說：「我對你還有希望」，所以電影不應該安排讓克萊德按下炸彈按鈕的，這樣可以讓這個觀點凸顯出來。但克萊德還是按下了按鈕，所以克萊德炸死了自己。如果硬要幫這部電影說話，這樣安排其實也還不錯，因為尼克最後不是用司法來將克萊德繩之以法，反而是用以牙還牙的方式來為他死去的好友們伸張正義，克萊德知道這點之後便在爆破之火中笑著面對死亡。</p>\n\n<p><img src=\"http://static.obeobe.com/image/blog-image/law-abiding-citizen-zhong-an-dui-jue-4.jpg\" alt=\"Law Abiding Citizen 4\" /></p>\n\n<p>其實後半段這些關於司法正義的探討並沒有我寫得這麼明顯，反而是著重在克萊德如何犯案，所以主軸有點陷入混亂，因此大家一般對這部電影的評價都是虎頭蛇尾，相較之下，我覺得我不小心把這電影寫得太好了！詳細情況請大家自己看電影就知道了～</p>\n\n<p>後記：我看電影時一直覺得男主角很面熟，由於我不太會記演員，所以認不太出來，查了一下才知道是 <a href=\"http://www.imdb.com/name/nm0124930/\">Gerard Butler</a> 曾經演過八百壯士及 P.S. 我愛妳，兩部蠻跳 tone 的電影，哈哈</p>\n\n<blockquote>  \n圖片取自於 Google 搜尋，絕無意侵犯智財權。若有侵犯請告知，我會馬上刪除，感謝！\n</blockquote>\n\n<p><img src=\"http://static.obeobe.com/image/blog-image/law-abiding-citizen-zhong-an-dui-jue-1.jpg\" alt=\"cover-image\" /></p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-02-01T15:54:43.000Z","created_by":1,"updated_at":"2014-03-11T06:56:21.000Z","updated_by":1,"published_at":"2014-02-01T18:46:42.000Z","published_by":1},{"id":8,"uuid":"88210506-2278-480c-9134-87749a26e4a0","title":"One Day 真愛挑日子","slug":"one-day-zhen-ai-tiao-ri-zi","markdown":"![One Day](http://static.obeobe.com/image/blog-image/one-day-zhen-ai-tiao-ri-zi-1.jpg)\n\n剛又不小心挑了個愛情片來看，每次要寫這類電影的文章，總會有意無意地透露出自己內心的真實，簡直就像在人們面前赤裸著自己，這其實是很令人難為情的一件事啊！說到這，突然想起了之前[陳芳汶](http://ctld.nccu.edu.tw/ctld/?p=10953)老師在情詩課上鼓勵著年輕的我要學會面對自己的眼淚、面對自己內心的情感，藉由分享自己的生命體驗，才能理解詩中的真情及了解愛，進而讓自己在生活上學會愛。所以即使寫這篇文章可能或多或少會牽扯到自己的愛情觀讓我感到害羞，但卻不至於抗拒排斥。\n\n總之，我看了 [One Day 真愛挑日子](http://www.imdb.com/title/tt1563738/)這部電影，也到了書桌前寫文章。（以下有雷）\n\n![One Day 1](http://static.obeobe.com/image/blog-image/one-day-zhen-ai-tiao-ri-zi-2.jpg)\n\n達斯和艾瑪是大學同學，但卻一直到畢業當天才正式認識。帥氣有錢又從不缺妹的達斯是妹見妹愛的人生勝利組，根本就從沒記得過艾瑪的名字；而穿著土土又帶著傻妹大眼鏡的艾瑪則是入學以來就對達斯一見鍾情，只是沒有自信所以一直暗戀著達斯。\n\n或許是畢業狂歡的氣氛所致，又或許是達斯只是想玩玩打個砲，兩人便一路啾回艾瑪的房間。\n\n像艾瑪這樣的乖乖女，自己的第一次是面對自己暗戀的人，內心當然是激動不已，為了安撫自己緊張的情緒，便假藉要整理儀容跑到廁所整理情緒，被艾瑪這樣一搞的達斯反而開始冷靜了起來。或許是因為達斯看出了艾瑪對自己有特別的情愫，以他目前仍只是想玩玩的心態，若衝動對待這樣的女生，可能會傷害到她，於是便準備走人。艾瑪看到這樣的情況，內心雖然失望卻裝作不在乎。\n\n達斯果然是個把妹高手，一下就看出了艾瑪內心的想法，即使愛不了對方，卻仍然是留下來陪艾瑪蓋棉被純聊天，還編了個理由，說今天是個特別的節日，說可以當永遠的朋友（發給艾瑪朋友卡），相約每年這個日子都要見面聊聊，兩人這樣一糾纏就是 20 年。\n\n![One Day 2](http://static.obeobe.com/image/blog-image/one-day-zhen-ai-tiao-ri-zi-3.jpg)\n\n畢業後達斯從教書（一邊教書一邊和學生亂搞）到變為電視主持人（有名了女朋友更是一個一個換），身邊的女伴從沒少過，一路平步青雲；而艾瑪卻只能住破爛的小套房，在墨西哥餐廳打工度日。在這困頓的日子裡艾瑪總是打電話寫長信給達斯，字裡行間無不透露著真情與愛意。身為朋友的達斯則不斷地鼓勵艾瑪要有自信，告訴她很有才華也很特別，只是需要為自己的人生做點不一樣的嘗試。\n\n為了鼓勵艾瑪的達斯約了艾瑪到法國去旅遊散散心，了解達斯只是把自己當成朋友的艾瑪為了不讓自己再動情而與達斯約法三章：不準共床擁抱、不準調情、不準裸泳，沒想到他們一一打破了這些規則。裸泳的那一幕達斯對艾瑪表達了好感，艾瑪一度以為兩人終於可以在一起了，但達斯話鋒一轉說明他還心不定可是可以陪艾瑪玩玩，頓時讓艾瑪心花朵朵碎，也讓艾瑪下定決心劃清朋友的界線，無論再怎麼靠近、再怎麼愛，都只能是朋友。\n\n此後，達斯歷經了母親生病、去世、被父親趕出門，演藝事業也不斷走下坡，整日藉酒澆愁、沈醉於紙醉金迷，而在這時他最想見到的人就是艾瑪，他想跟艾瑪說說話，甚至想問問為何他們沒有在一起。而艾瑪在這一天離開了墨西哥餐廳，開始寫作一邊面試教師工作，晚上還跟單戀追求了她好久的伊恩有個約會，不願意再對達斯動心的艾瑪即使知道他在求救，仍狠心不接他電話、不回留言，留達斯獨自瘋狂的留言。就在這天艾瑪跟一個她不愛的人在一起了，而達斯卻發現了他對艾瑪的愛。\n\n![One Day 3](http://static.obeobe.com/image/blog-image/one-day-zhen-ai-tiao-ri-zi-4.jpg)\n\n過了幾年，想念艾瑪的達斯約了一起吃飯，和一個自己不愛的人交往的艾瑪生活格外沉悶與無聊，當然答應了邀約，艾瑪對這個邀約其實是有所期待的。沒想到在飯局上他們聊到了彼此的男女朋友，開始賭氣。達斯故意冷落艾瑪，到處與其他女生調情，還對艾瑪說了些傷人的話，艾瑪被傷透了心，憤而離席。「**I love You, but I don't like you any more**」是艾瑪離開時留下的最後一句話。\n\n後來艾瑪與她不愛的三流喜劇演員男友伊恩分手，也不教書了，專心從事寫作，寫作事業也漸漸有了起色。反觀達斯則是完全離開了演藝事業，並開始跟一個自己不愛的富家女交往，這時的達斯已經不再花心了，並打算定下來，即使他知道他愛的人是艾瑪。\n\n幾年沒見的他們終於在大學同學的婚禮上重逢，也許是因為想念彼此，兩人跑到頂樓去靜一靜，達斯問起艾瑪的情況，艾瑪倔強地說自己**單身但不寂寞**，而艾瑪問起達斯的情況，達斯卻告訴艾瑪，他女友已經懷了孩子，不久之後就會結婚了。雖然艾瑪說著恭喜，但可以從兩人的眼神看出彼此的寂寞與不捨，明明是相愛的兩個人，卻因為總是在錯的時間遇見彼此而錯過，沒能在一起。\n\n![One Day 4](http://static.obeobe.com/image/blog-image/one-day-zhen-ai-tiao-ri-zi-5.jpg)\n\n當然達斯與他老婆的婚姻無法持久，只是這次是他老婆有了婚外情（我想他老婆應該也是不愛達斯，從電影中看不出他老婆對達斯的愛），而不是因為達斯花心。恢復單身的達斯馬上奔到法國去找已是小有名氣的童書作家艾瑪，一下火車就迫不及待地想告訴艾瑪他已經準備好了，他想要與艾瑪在一起。但是這時艾瑪身邊已經有了一個又帥又有才氣的爵士樂手男友，達斯當然醋桶大發，只是他也不能對艾瑪說什麼，錯過太多次的他，也只能離開。但是這次艾瑪提起了勇氣，追上了達斯，剝下了包裹了他們十幾年名為好朋友的假象，他們忘不了彼此，也不想再錯過了，兩人終於可以在一起了。\n\n電影到這邊已經接近尾聲了，所要傳達的也差不多了，達斯與艾瑪因為某些原因的彼此錯過，為愛情的難得與遺憾下了一個註腳，所以才更要去把握。只是達斯與艾瑪在一起幸福的日子並不長，一場車禍奪走了艾瑪的性命，這不僅讓觀影者不由得為他們感嘆，**也強化了把握與珍惜生命中每個美麗邂逅的觀點，而不是執著於一定要在對的時間遇到對的人，即使是錯的時間遇見對的人也要把握！（錯的人就算了）因為我們永遠都不知道命運什麼時候會把他收回去啊！**\n\n<blockquote>\n如果・愛<br>\n演唱：張學友<br>\n<br>\n每個人　都想明白<br>\n誰是自己生命　不該錯過的真愛<br>\n特別在午夜醒來　更是　會感慨<br>\n心動埋怨還有不能釋懷<br>\n都是因為你觸碰了愛<br>\n<br>\n如果這就是愛<br>\n在轉身就該勇敢留下來<br>\n就算受傷　就算流淚<br>\n都是生命裡溫柔灌溉\n</blockquote>\n\n電影中另外讓我有感觸的一幕是艾瑪前男友伊恩跑來達斯的咖啡店捧場那幕，他對著達斯說：「**我恨你，因為你點燃了艾瑪的激情，她從來沒有這樣對我，這樣讓我很生氣，因為我認為你根本配不上艾瑪，但作為回報，你讓艾瑪很開心，當艾瑪跟你在一起時是多麼的快樂，為此我一直很感激你**」，完全就是好人模式，在這邊可以看出伊恩是多麼愛艾瑪，但其實更重要的是彼此相愛，唯有像達斯與艾瑪這樣彼此相愛的人在一起，才能真正擁有幸福，勉強與不愛的人在一起，不僅不快樂，也可能帶來傷害。\n\n然後有些人會很討厭像達斯這樣的人，認為他只是把艾瑪當成備胎而已，我只能說像達斯這樣的人生勝利組的確比較**可能**會花心，但他最後的的確確是誠實面對自己的情感了，你能說他對艾瑪的愛不真誠嗎？\n\n![有錢長得帥是我的錯嗎](http://static.obeobe.com/image/subtitle-image/有錢長得帥是我的錯嗎.jpg)\n\n**為了讓自己不會有遺憾，不讓真愛錯過，我應該要多跟可愛的女孩約會才對啊！（耍耍嘴皮子很簡單，約不約得到，很難，哈哈）**\n\n後記：原來電影名稱叫 One Day 是因為電影中只描繪達斯與艾瑪在每年 07/15 發生的故事，而他們開始熟識的日子就是 07/15，為了安撫艾瑪，達斯賦予這個日子一個特別的意義，讓他們總是想辦法在每年的 07/15 都能見面，在我看來，**這只是達斯把妹的招數，騙不了我的**！\n\n<blockquote>\n圖片取自於 Google 搜尋，絕無意侵犯智財權。若有侵犯請告知，我會馬上刪除，感謝！\n</blockquote>\n\n![cover-image](http://static.obeobe.com/image/blog-image/one-day-zhen-ai-tiao-ri-zi-5.jpg)","html":"<p><img src=\"http://static.obeobe.com/image/blog-image/one-day-zhen-ai-tiao-ri-zi-1.jpg\" alt=\"One Day\" /></p>\n\n<p>剛又不小心挑了個愛情片來看，每次要寫這類電影的文章，總會有意無意地透露出自己內心的真實，簡直就像在人們面前赤裸著自己，這其實是很令人難為情的一件事啊！說到這，突然想起了之前<a href=\"http://ctld.nccu.edu.tw/ctld/?p=10953\">陳芳汶</a>老師在情詩課上鼓勵著年輕的我要學會面對自己的眼淚、面對自己內心的情感，藉由分享自己的生命體驗，才能理解詩中的真情及了解愛，進而讓自己在生活上學會愛。所以即使寫這篇文章可能或多或少會牽扯到自己的愛情觀讓我感到害羞，但卻不至於抗拒排斥。</p>\n\n<p>總之，我看了 <a href=\"http://www.imdb.com/title/tt1563738/\">One Day 真愛挑日子</a>這部電影，也到了書桌前寫文章。（以下有雷）</p>\n\n<p><img src=\"http://static.obeobe.com/image/blog-image/one-day-zhen-ai-tiao-ri-zi-2.jpg\" alt=\"One Day 1\" /></p>\n\n<p>達斯和艾瑪是大學同學，但卻一直到畢業當天才正式認識。帥氣有錢又從不缺妹的達斯是妹見妹愛的人生勝利組，根本就從沒記得過艾瑪的名字；而穿著土土又帶著傻妹大眼鏡的艾瑪則是入學以來就對達斯一見鍾情，只是沒有自信所以一直暗戀著達斯。</p>\n\n<p>或許是畢業狂歡的氣氛所致，又或許是達斯只是想玩玩打個砲，兩人便一路啾回艾瑪的房間。</p>\n\n<p>像艾瑪這樣的乖乖女，自己的第一次是面對自己暗戀的人，內心當然是激動不已，為了安撫自己緊張的情緒，便假藉要整理儀容跑到廁所整理情緒，被艾瑪這樣一搞的達斯反而開始冷靜了起來。或許是因為達斯看出了艾瑪對自己有特別的情愫，以他目前仍只是想玩玩的心態，若衝動對待這樣的女生，可能會傷害到她，於是便準備走人。艾瑪看到這樣的情況，內心雖然失望卻裝作不在乎。</p>\n\n<p>達斯果然是個把妹高手，一下就看出了艾瑪內心的想法，即使愛不了對方，卻仍然是留下來陪艾瑪蓋棉被純聊天，還編了個理由，說今天是個特別的節日，說可以當永遠的朋友（發給艾瑪朋友卡），相約每年這個日子都要見面聊聊，兩人這樣一糾纏就是 20 年。</p>\n\n<p><img src=\"http://static.obeobe.com/image/blog-image/one-day-zhen-ai-tiao-ri-zi-3.jpg\" alt=\"One Day 2\" /></p>\n\n<p>畢業後達斯從教書（一邊教書一邊和學生亂搞）到變為電視主持人（有名了女朋友更是一個一個換），身邊的女伴從沒少過，一路平步青雲；而艾瑪卻只能住破爛的小套房，在墨西哥餐廳打工度日。在這困頓的日子裡艾瑪總是打電話寫長信給達斯，字裡行間無不透露著真情與愛意。身為朋友的達斯則不斷地鼓勵艾瑪要有自信，告訴她很有才華也很特別，只是需要為自己的人生做點不一樣的嘗試。</p>\n\n<p>為了鼓勵艾瑪的達斯約了艾瑪到法國去旅遊散散心，了解達斯只是把自己當成朋友的艾瑪為了不讓自己再動情而與達斯約法三章：不準共床擁抱、不準調情、不準裸泳，沒想到他們一一打破了這些規則。裸泳的那一幕達斯對艾瑪表達了好感，艾瑪一度以為兩人終於可以在一起了，但達斯話鋒一轉說明他還心不定可是可以陪艾瑪玩玩，頓時讓艾瑪心花朵朵碎，也讓艾瑪下定決心劃清朋友的界線，無論再怎麼靠近、再怎麼愛，都只能是朋友。</p>\n\n<p>此後，達斯歷經了母親生病、去世、被父親趕出門，演藝事業也不斷走下坡，整日藉酒澆愁、沈醉於紙醉金迷，而在這時他最想見到的人就是艾瑪，他想跟艾瑪說說話，甚至想問問為何他們沒有在一起。而艾瑪在這一天離開了墨西哥餐廳，開始寫作一邊面試教師工作，晚上還跟單戀追求了她好久的伊恩有個約會，不願意再對達斯動心的艾瑪即使知道他在求救，仍狠心不接他電話、不回留言，留達斯獨自瘋狂的留言。就在這天艾瑪跟一個她不愛的人在一起了，而達斯卻發現了他對艾瑪的愛。</p>\n\n<p><img src=\"http://static.obeobe.com/image/blog-image/one-day-zhen-ai-tiao-ri-zi-4.jpg\" alt=\"One Day 3\" /></p>\n\n<p>過了幾年，想念艾瑪的達斯約了一起吃飯，和一個自己不愛的人交往的艾瑪生活格外沉悶與無聊，當然答應了邀約，艾瑪對這個邀約其實是有所期待的。沒想到在飯局上他們聊到了彼此的男女朋友，開始賭氣。達斯故意冷落艾瑪，到處與其他女生調情，還對艾瑪說了些傷人的話，艾瑪被傷透了心，憤而離席。「<strong>I love You, but I don't like you any more</strong>」是艾瑪離開時留下的最後一句話。</p>\n\n<p>後來艾瑪與她不愛的三流喜劇演員男友伊恩分手，也不教書了，專心從事寫作，寫作事業也漸漸有了起色。反觀達斯則是完全離開了演藝事業，並開始跟一個自己不愛的富家女交往，這時的達斯已經不再花心了，並打算定下來，即使他知道他愛的人是艾瑪。</p>\n\n<p>幾年沒見的他們終於在大學同學的婚禮上重逢，也許是因為想念彼此，兩人跑到頂樓去靜一靜，達斯問起艾瑪的情況，艾瑪倔強地說自己<strong>單身但不寂寞</strong>，而艾瑪問起達斯的情況，達斯卻告訴艾瑪，他女友已經懷了孩子，不久之後就會結婚了。雖然艾瑪說著恭喜，但可以從兩人的眼神看出彼此的寂寞與不捨，明明是相愛的兩個人，卻因為總是在錯的時間遇見彼此而錯過，沒能在一起。</p>\n\n<p><img src=\"http://static.obeobe.com/image/blog-image/one-day-zhen-ai-tiao-ri-zi-5.jpg\" alt=\"One Day 4\" /></p>\n\n<p>當然達斯與他老婆的婚姻無法持久，只是這次是他老婆有了婚外情（我想他老婆應該也是不愛達斯，從電影中看不出他老婆對達斯的愛），而不是因為達斯花心。恢復單身的達斯馬上奔到法國去找已是小有名氣的童書作家艾瑪，一下火車就迫不及待地想告訴艾瑪他已經準備好了，他想要與艾瑪在一起。但是這時艾瑪身邊已經有了一個又帥又有才氣的爵士樂手男友，達斯當然醋桶大發，只是他也不能對艾瑪說什麼，錯過太多次的他，也只能離開。但是這次艾瑪提起了勇氣，追上了達斯，剝下了包裹了他們十幾年名為好朋友的假象，他們忘不了彼此，也不想再錯過了，兩人終於可以在一起了。</p>\n\n<p>電影到這邊已經接近尾聲了，所要傳達的也差不多了，達斯與艾瑪因為某些原因的彼此錯過，為愛情的難得與遺憾下了一個註腳，所以才更要去把握。只是達斯與艾瑪在一起幸福的日子並不長，一場車禍奪走了艾瑪的性命，這不僅讓觀影者不由得為他們感嘆，<strong>也強化了把握與珍惜生命中每個美麗邂逅的觀點，而不是執著於一定要在對的時間遇到對的人，即使是錯的時間遇見對的人也要把握！（錯的人就算了）因為我們永遠都不知道命運什麼時候會把他收回去啊！</strong></p>\n\n<blockquote>  \n如果・愛<br>\n演唱：張學友<br>\n<br>  \n每個人　都想明白<br>\n誰是自己生命　不該錯過的真愛<br>\n特別在午夜醒來　更是　會感慨<br>\n心動埋怨還有不能釋懷<br>\n都是因為你觸碰了愛<br>\n<br>  \n如果這就是愛<br>\n在轉身就該勇敢留下來<br>\n就算受傷　就算流淚<br>\n都是生命裡溫柔灌溉\n</blockquote>\n\n<p>電影中另外讓我有感觸的一幕是艾瑪前男友伊恩跑來達斯的咖啡店捧場那幕，他對著達斯說：「<strong>我恨你，因為你點燃了艾瑪的激情，她從來沒有這樣對我，這樣讓我很生氣，因為我認為你根本配不上艾瑪，但作為回報，你讓艾瑪很開心，當艾瑪跟你在一起時是多麼的快樂，為此我一直很感激你</strong>」，完全就是好人模式，在這邊可以看出伊恩是多麼愛艾瑪，但其實更重要的是彼此相愛，唯有像達斯與艾瑪這樣彼此相愛的人在一起，才能真正擁有幸福，勉強與不愛的人在一起，不僅不快樂，也可能帶來傷害。</p>\n\n<p>然後有些人會很討厭像達斯這樣的人，認為他只是把艾瑪當成備胎而已，我只能說像達斯這樣的人生勝利組的確比較<strong>可能</strong>會花心，但他最後的的確確是誠實面對自己的情感了，你能說他對艾瑪的愛不真誠嗎？</p>\n\n<p><img src=\"http://static.obeobe.com/image/subtitle-image/有錢長得帥是我的錯嗎.jpg\" alt=\"有錢長得帥是我的錯嗎\" /></p>\n\n<p><strong>為了讓自己不會有遺憾，不讓真愛錯過，我應該要多跟可愛的女孩約會才對啊！（耍耍嘴皮子很簡單，約不約得到，很難，哈哈）</strong></p>\n\n<p>後記：原來電影名稱叫 One Day 是因為電影中只描繪達斯與艾瑪在每年 07/15 發生的故事，而他們開始熟識的日子就是 07/15，為了安撫艾瑪，達斯賦予這個日子一個特別的意義，讓他們總是想辦法在每年的 07/15 都能見面，在我看來，<strong>這只是達斯把妹的招數，騙不了我的</strong>！</p>\n\n<blockquote>  \n圖片取自於 Google 搜尋，絕無意侵犯智財權。若有侵犯請告知，我會馬上刪除，感謝！\n</blockquote>\n\n<p><img src=\"http://static.obeobe.com/image/blog-image/one-day-zhen-ai-tiao-ri-zi-5.jpg\" alt=\"cover-image\" /></p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-02-02T12:07:39.000Z","created_by":1,"updated_at":"2014-03-11T06:56:01.000Z","updated_by":1,"published_at":"2014-02-02T17:01:58.000Z","published_by":1},{"id":9,"uuid":"48a53c34-deac-4079-8cce-9ac82d0be757","title":"Swim Deep - She Changes the Weather","slug":"swim-deep-she-changes-the-weather","markdown":"![Swim Deep - She Changes the Weather](http://static.obeobe.com/image/blog-image/swim-deep-she-changes-the-weather.jpg)\n\n最近聽到 Swim Deep 的 She Changes the Weather 覺得蠻讚的，忍不住想推薦給大家聽聽看～\n\n歌曲一開始是用吉他分散和弦做鋪陳，輔以簡易的鍵盤和弦墊底，慢慢地低語。中間轉折橋段鍵盤變得緊湊了起來，然後貝斯及合成器也慢慢地加了進來，且背景中人聲啁哳談話的聲音，讓這個橋段有了畫龍點睛的效果，最後爵士鼓聲加了進來，讓整首歌振奮了起來。\n\n聽這首歌有種開車兜風的感覺，一邊聽著車子就跟著越開越快了，但是是以一種舒服愉快的速度航行，讓人感到舒適愜意！\n\n雖然這首歌感覺是蠻適合開車或旅行聽，但實際上這首歌是一首情歌，特別喜歡他這句歌詞：\n\n<blockquote>\nCause what you say will make my day<br>\nRid of consequence<br>\nIt's so weird, but it's so clear.<br>\n</blockquote>\n\n是一種掉入愛情的感覺，描寫得蠻貼切的，哈哈。歌曲從一開始的低語到後來的輕快，是否也是代表著從沈思自己的情感到積極面對，然後勇敢地在這愛意裡兜風呢？\n\n<br>\n\n<iframe width=\"560\" height=\"315\" src=\"//www.youtube.com/embed/UQUIOUKhEdk\" frameborder=\"0\" allowfullscreen></iframe>\n\n<blockquote>\nShe takes my time,<br>\nShe grows the flowers in my mind,<br>\nShe makes it shine in my mould<br>\nShe makes me trip<br>\nThe words just fall out of my lips<br>\nAnd I forgot how to lie<br>\nA song reminds me<br>\nSidetrack my life<br>\nIt's easy as 369, 369.<br>\n<br>\nSeen it all but I've seen nothing yet<br>\nCause I forget<br>\nDo I know or do I think I know?<br>\nIt's so weird, it's so clear.<br>\n<br>\nShe takes my time and I don't mind<br>\nShe makes me feel like<br>\nLike I can see for miles.<br>\nShe changes the weather in my world<br>\nSeems like it's never getting cold.<br>\n<br>\nIt's OK, it's not obvious<br>\nCause what you say will make my day<br>\nRid of consequence<br>\nIt's so weird, but it's so clear.<br>\n<br>\nShe takes my time and I don't mind<br>\nShe makes me feel like<br>\nLike I can see for miles.<br>\nShe changes the weather in my world<br>\nSeems like we're never getting old.<br>\n<br>\nShe takes my time and I don't mind<br>\nShe makes me feel like<br>\nLike I can see for miles.<br>\nShe changes the weather in my world<br>\nSeems like it's never getting cold.<br>\n</blockquote>\n\n<blockquote>\n圖片取自於 Google 搜尋，絕無意侵犯智財權。若有侵犯請告知，我會馬上刪除，感謝！\n</blockquote>\n\n![cover-image](http://static.obeobe.com/image/blog-image/swim-deep-she-changes-the-weather-bg.jpg)","html":"<p><img src=\"http://static.obeobe.com/image/blog-image/swim-deep-she-changes-the-weather.jpg\" alt=\"Swim Deep - She Changes the Weather\" /></p>\n\n<p>最近聽到 Swim Deep 的 She Changes the Weather 覺得蠻讚的，忍不住想推薦給大家聽聽看～</p>\n\n<p>歌曲一開始是用吉他分散和弦做鋪陳，輔以簡易的鍵盤和弦墊底，慢慢地低語。中間轉折橋段鍵盤變得緊湊了起來，然後貝斯及合成器也慢慢地加了進來，且背景中人聲啁哳談話的聲音，讓這個橋段有了畫龍點睛的效果，最後爵士鼓聲加了進來，讓整首歌振奮了起來。</p>\n\n<p>聽這首歌有種開車兜風的感覺，一邊聽著車子就跟著越開越快了，但是是以一種舒服愉快的速度航行，讓人感到舒適愜意！</p>\n\n<p>雖然這首歌感覺是蠻適合開車或旅行聽，但實際上這首歌是一首情歌，特別喜歡他這句歌詞：</p>\n\n<blockquote>  \nCause what you say will make my day<br>  \nRid of consequence<br>  \nIt's so weird, but it's so clear.<br>  \n</blockquote>\n\n<p>是一種掉入愛情的感覺，描寫得蠻貼切的，哈哈。歌曲從一開始的低語到後來的輕快，是否也是代表著從沈思自己的情感到積極面對，然後勇敢地在這愛意裡兜風呢？</p>\n\n<p><br></p>\n\n<iframe width=\"560\" height=\"315\" src=\"//www.youtube.com/embed/UQUIOUKhEdk\" frameborder=\"0\" allowfullscreen></iframe>\n\n<blockquote>  \nShe takes my time,<br>  \nShe grows the flowers in my mind,<br>  \nShe makes it shine in my mould<br>  \nShe makes me trip<br>  \nThe words just fall out of my lips<br>  \nAnd I forgot how to lie<br>  \nA song reminds me<br>  \nSidetrack my life<br>  \nIt's easy as 369, 369.<br>  \n<br>  \nSeen it all but I've seen nothing yet<br>  \nCause I forget<br>  \nDo I know or do I think I know?<br>  \nIt's so weird, it's so clear.<br>  \n<br>  \nShe takes my time and I don't mind<br>  \nShe makes me feel like<br>  \nLike I can see for miles.<br>  \nShe changes the weather in my world<br>  \nSeems like it's never getting cold.<br>  \n<br>  \nIt's OK, it's not obvious<br>  \nCause what you say will make my day<br>  \nRid of consequence<br>  \nIt's so weird, but it's so clear.<br>  \n<br>  \nShe takes my time and I don't mind<br>  \nShe makes me feel like<br>  \nLike I can see for miles.<br>  \nShe changes the weather in my world<br>  \nSeems like we're never getting old.<br>  \n<br>  \nShe takes my time and I don't mind<br>  \nShe makes me feel like<br>  \nLike I can see for miles.<br>  \nShe changes the weather in my world<br>  \nSeems like it's never getting cold.<br>  \n</blockquote>\n\n<blockquote>  \n圖片取自於 Google 搜尋，絕無意侵犯智財權。若有侵犯請告知，我會馬上刪除，感謝！\n</blockquote>\n\n<p><img src=\"http://static.obeobe.com/image/blog-image/swim-deep-she-changes-the-weather-bg.jpg\" alt=\"cover-image\" /></p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-02-04T08:10:23.000Z","created_by":1,"updated_at":"2014-03-11T06:55:05.000Z","updated_by":1,"published_at":"2014-02-04T09:05:50.000Z","published_by":1},{"id":10,"uuid":"02b59c52-e419-4ee8-9861-c23601109b03","title":"來自大阪的元祖 ICE DOG","slug":"lai-zi-da-ban-de-yuan-zu-ice-dog","markdown":"<p style=\"text-align:center\">\n<img src=\"http://static.obeobe.com/image/blog-image/ice-dog-1.jpg\" alt=\"ice dog\">\n</p>\n\n這次日本行回來最難忘的美食第一名就是這個來自大阪的 Ice Dog，一直到現在我看著這張圖片還是會想起它美妙的滋味，那種滋味好難形容啊～ 一個小小的甜品卻能讓我如此難忘甚至把我變成愛吃甜點的少女，我想這應該是一種靈異現象吧！\n\n其實在嚐到 Ice Dog 之前，我們已經走了好多個台灣部落格推薦的美食，就像流言終結者一樣，有的真的是美食，但也有蠻多雷的，其中章魚小丸子就是大雷，咬下去像是吃到空氣一樣，然後也沒有章魚，真的很不優，一大堆台灣人吃到都直接用台語幹譙起來（日本真的到處是台灣人），所以在咬下 Ice Dog 之前我是一直保持著不期不待不受傷害的心態。\n\n沒想到第一口吃下 Ice Dog 整個人都快被融化了！驚為天人啊！\n\n<p style=\"text-align:center\">\n<img src=\"http://static.obeobe.com/image/subtitle-image/以後我吃不到怎麼辦呀！.jpg\" alt=\"以後我吃不到怎麼辦呀！\">\n</p>\n\n炸得酥脆裹著糖粉的麵包中間夾著日本排名第一的六甲牧場冰淇淋，冰與火的衝突感在口中爆發，卻有種絕妙的搭配口感，讓人感到異常順口！吃完第一個 Ice Dog 之後，由於意猶未盡又忍不住加點了一個來吃，回頭看看後面進來的兩個日本妹，居然也跟我一樣加點了第二個 Ice Dog 來吃，簡直像是中了毒癮一樣。\n\n如果可以的話我真的蠻想在台灣開一間賣 Ice Dog 分店，應該會大賣吧！有沒有人有興趣來合夥開一間台灣的 Ice Dog 呢？哈哈～\n\n<p style=\"text-align:center\">\n<img src=\"http://static.obeobe.com/image/blog-image/ice-dog-2.jpg\" alt=\"ice dog\">\n<br>\n元祖 Ice Dog 海報，現在已經漲價到 350 日元了\n</p>\n\n<p style=\"text-align:center\">\n<img src=\"http://static.obeobe.com/image/blog-image/ice-dog-3.jpg\" alt=\"ice dog\">\n<br>\n元祖 Ice Dog 店面\n</p>\n\n<p style=\"text-align:center\">\n<img src=\"http://static.obeobe.com/image/blog-image/ice-dog-4.jpeg\" alt=\"ice dog\">\n<br>\n元祖 Ice Dog 店內一景\n</p>\n\n<blockquote>\n圖片取自於 Google 搜尋，絕無意侵犯智財權。若有侵犯請告知，我會馬上刪除，感謝！\n</blockquote>","html":"<p style=\"text-align:center\">  \n<img src=\"http://static.obeobe.com/image/blog-image/ice-dog-1.jpg\" alt=\"ice dog\">  \n</p>\n\n<p>這次日本行回來最難忘的美食第一名就是這個來自大阪的 Ice Dog，一直到現在我看著這張圖片還是會想起它美妙的滋味，那種滋味好難形容啊～ 一個小小的甜品卻能讓我如此難忘甚至把我變成愛吃甜點的少女，我想這應該是一種靈異現象吧！</p>\n\n<p>其實在嚐到 Ice Dog 之前，我們已經走了好多個台灣部落格推薦的美食，就像流言終結者一樣，有的真的是美食，但也有蠻多雷的，其中章魚小丸子就是大雷，咬下去像是吃到空氣一樣，然後也沒有章魚，真的很不優，一大堆台灣人吃到都直接用台語幹譙起來（日本真的到處是台灣人），所以在咬下 Ice Dog 之前我是一直保持著不期不待不受傷害的心態。</p>\n\n<p>沒想到第一口吃下 Ice Dog 整個人都快被融化了！驚為天人啊！</p>\n\n<p style=\"text-align:center\">  \n<img src=\"http://static.obeobe.com/image/subtitle-image/以後我吃不到怎麼辦呀！.jpg\" alt=\"以後我吃不到怎麼辦呀！\">  \n</p>\n\n<p>炸得酥脆裹著糖粉的麵包中間夾著日本排名第一的六甲牧場冰淇淋，冰與火的衝突感在口中爆發，卻有種絕妙的搭配口感，讓人感到異常順口！吃完第一個 Ice Dog 之後，由於意猶未盡又忍不住加點了一個來吃，回頭看看後面進來的兩個日本妹，居然也跟我一樣加點了第二個 Ice Dog 來吃，簡直像是中了毒癮一樣。</p>\n\n<p>如果可以的話我真的蠻想在台灣開一間賣 Ice Dog 分店，應該會大賣吧！有沒有人有興趣來合夥開一間台灣的 Ice Dog 呢？哈哈～</p>\n\n<p style=\"text-align:center\">  \n<img src=\"http://static.obeobe.com/image/blog-image/ice-dog-2.jpg\" alt=\"ice dog\">  \n<br>  \n元祖 Ice Dog 海報，現在已經漲價到 350 日元了\n</p>\n\n<p style=\"text-align:center\">  \n<img src=\"http://static.obeobe.com/image/blog-image/ice-dog-3.jpg\" alt=\"ice dog\">  \n<br>  \n元祖 Ice Dog 店面\n</p>\n\n<p style=\"text-align:center\">  \n<img src=\"http://static.obeobe.com/image/blog-image/ice-dog-4.jpeg\" alt=\"ice dog\">  \n<br>  \n元祖 Ice Dog 店內一景\n</p>\n\n<blockquote>  \n圖片取自於 Google 搜尋，絕無意侵犯智財權。若有侵犯請告知，我會馬上刪除，感謝！\n</blockquote>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-02-17T17:52:02.000Z","created_by":1,"updated_at":"2014-03-11T06:06:48.000Z","updated_by":1,"published_at":"2014-02-22T19:30:17.000Z","published_by":1},{"id":11,"uuid":"c03f1961-5423-44ed-be39-d90e22dbc4c0","title":"如何列出正在執行的 PHP Script Process","slug":"ru-he-lie-chu-zheng-zai-zhi-xing-de-php-script-process","markdown":"在實務上我們通常會使用 crontab job 來背景執行一些程式，有時我們會有需要砍掉這些 process 的情況發生，這時我們就需要列出 process 的 process id，並手動將這些 process 砍掉～\n\n以要查閱 apns-task.php 這支 php script 在機器上執行的 process id 為例，指令可以這樣下：\n\n    $ ps auxwww|grep apns-task.php|grep -v grep\n\n如此就可以列出正在執行 apns-task.php 這支程式的所有 process：\n\n    user 15211 0.0 0.0 4108 604 ? Ss 15:10 0:00 /bin/sh -c php /path-to-your-script/apns-task.php\n    user 15213 50.0 0.0 211584 28124 ? R 15:10 0:00 php /path-to-your-script/apns-task.php\n\n\n如輸出結果所見，現在有兩個 process 15211 及 15213 正在執行 apns-task.php 這支程式，如果要刪除 process 15211，指令就可以這樣下：\n\n    $ kill 15211\n\n\n以上就是如何列出正在執行的 PHP script process 的簡易筆記～\n","html":"<p>在實務上我們通常會使用 crontab job 來背景執行一些程式，有時我們會有需要砍掉這些 process 的情況發生，這時我們就需要列出 process 的 process id，並手動將這些 process 砍掉～</p>\n\n<p>以要查閱 apns-task.php 這支 php script 在機器上執行的 process id 為例，指令可以這樣下：</p>\n\n<pre><code>$ ps auxwww|grep apns-task.php|grep -v grep\n</code></pre>\n\n<p>如此就可以列出正在執行 apns-task.php 這支程式的所有 process：</p>\n\n<pre><code>user 15211 0.0 0.0 4108 604 ? Ss 15:10 0:00 /bin/sh -c php /path-to-your-script/apns-task.php\nuser 15213 50.0 0.0 211584 28124 ? R 15:10 0:00 php /path-to-your-script/apns-task.php\n</code></pre>\n\n<p>如輸出結果所見，現在有兩個 process 15211 及 15213 正在執行 apns-task.php 這支程式，如果要刪除 process 15211，指令就可以這樣下：</p>\n\n<pre><code>$ kill 15211\n</code></pre>\n\n<p>以上就是如何列出正在執行的 PHP script process 的簡易筆記～</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-03-10T16:58:22.000Z","created_by":1,"updated_at":"2014-03-10T16:58:22.000Z","updated_by":1,"published_at":"2014-03-10T16:58:22.000Z","published_by":1},{"id":12,"uuid":"1d7de532-56da-4906-a2d2-f97a89c06826","title":"如何掛載 AWS S3 到 AWS EC2 Instance - 環境安裝部份","slug":"ru-he-gua-zai-aws-s3-dao-aws-ec2-instance-huan-jing-an-zhuang-bu-fen-2","markdown":"架構網站服務時，Storage 的規劃也是重要的一環，而 AWS S3 就是一個蠻好的 Storage 服務。\n\n一般常見的做法我們會透過 AWS 提供的 API 來存取 S3 上的檔案，但這樣做並不直覺，而且要通常要將原本存取檔案的程式寫法改成用 S3 API 存取檔案的寫法，有可能會需要修改許多支程式。\n\n所以便有人萌生了將 S3 掛載到 EC2 Instance 的想法，就跟我們買一顆大容量的硬碟裝在電腦上一樣，讓我們的網站服務能夠像在同一部機器存取檔案一樣容易。（其實概念就像我們在實體機器上使用 NFS 來掛載網路硬碟一樣）\n\n環境安裝細節如下：（請確認已開好 S3 bucket，且開好 IAM user 權限）\n\nStep 1：登入 EC2 後使用 sudo 權限\n\n    $ sudo -s\n\nStep 2：先更新 apt-get\n\n    $ apt-get update\n\nStep 3：安裝必要套件\n\n    $ apt-get install make gcc g++ curl libxml2 libxml2-dev libssl-dev libcurl3 libcurl4-gnutls-dev openssl pkg-config\n\nStep 4：進入 /usr/local/src 資料夾\n\n    $ cd /usr/local/src\n\nStep 5：下載 fuse\n\n    $ wget http://sourceforge.net/projects/fuse/files/fuse-2.X/2.9.2/fuse-2.9.2.tar.gz/download -O fuse-2.9.2.tar.gz\n\nStep 6：解壓縮 fuse\n\n    $ tar -xzvf fuse-2.9.2.tar.gz\n\nStep 7：進入 fuse-2.9.2 資料夾\n\n    $ cd fuse-2.9.2\n\nStep 8：編譯安裝 fuse\n\n    $ ./configure --prefix=/usr\n    $ make\n    $ make install\n\nStep 9：將 fuse 加入 ubuntu kernel\n\n    $ modprobe fuse\n\nStep 10：確認 fuse 是否安裝完成\n\n    $ pkg-config --modversion fuse\n\nStep 11：進入 /usr/local/src 資料夾\n\n    $ cd /usr/local/src\n\nStep 12：下載 s3fs\n\n    $ wget http://s3fs.googlecode.com/files/s3fs-1.71.tar.gz\n\nStep 13：解壓縮 s3fs\n\n    $ tar -xzvf s3fs-1.71.tar.gz\n\nStep 14：進入 s3fs-1.71 資料夾\n\n    $ cd s3fs-1.71\n\nStep 15：編譯安裝 s3fs\n\n    $ ./configure --prefix=/usr\n    $ make\n    $ make install\n\n完成以上步驟，就可以在 EC2 的 Ubuntu Instance 安裝好可掛載 S3 的環境，關於掛載及卸載部份請見[如何掛載 AWS S3 到 AWS EC2 Instance - 掛載及卸載部份](http://blog.fukuball.com/ru-he-gua-zai-aws-s3-dao-aws-ec2-instance-gua-zai-ji-xie-zai-bu-fen/ \"如何掛載 AWS S3 到 AWS EC2 Instance - 掛載及卸載部份\") 。","html":"<p>架構網站服務時，Storage 的規劃也是重要的一環，而 AWS S3 就是一個蠻好的 Storage 服務。</p>\n\n<p>一般常見的做法我們會透過 AWS 提供的 API 來存取 S3 上的檔案，但這樣做並不直覺，而且要通常要將原本存取檔案的程式寫法改成用 S3 API 存取檔案的寫法，有可能會需要修改許多支程式。</p>\n\n<p>所以便有人萌生了將 S3 掛載到 EC2 Instance 的想法，就跟我們買一顆大容量的硬碟裝在電腦上一樣，讓我們的網站服務能夠像在同一部機器存取檔案一樣容易。（其實概念就像我們在實體機器上使用 NFS 來掛載網路硬碟一樣）</p>\n\n<p>環境安裝細節如下：（請確認已開好 S3 bucket，且開好 IAM user 權限）</p>\n\n<p>Step 1：登入 EC2 後使用 sudo 權限</p>\n\n<pre><code>$ sudo -s\n</code></pre>\n\n<p>Step 2：先更新 apt-get</p>\n\n<pre><code>$ apt-get update\n</code></pre>\n\n<p>Step 3：安裝必要套件</p>\n\n<pre><code>$ apt-get install make gcc g++ curl libxml2 libxml2-dev libssl-dev libcurl3 libcurl4-gnutls-dev openssl pkg-config\n</code></pre>\n\n<p>Step 4：進入 /usr/local/src 資料夾</p>\n\n<pre><code>$ cd /usr/local/src\n</code></pre>\n\n<p>Step 5：下載 fuse</p>\n\n<pre><code>$ wget http://sourceforge.net/projects/fuse/files/fuse-2.X/2.9.2/fuse-2.9.2.tar.gz/download -O fuse-2.9.2.tar.gz\n</code></pre>\n\n<p>Step 6：解壓縮 fuse</p>\n\n<pre><code>$ tar -xzvf fuse-2.9.2.tar.gz\n</code></pre>\n\n<p>Step 7：進入 fuse-2.9.2 資料夾</p>\n\n<pre><code>$ cd fuse-2.9.2\n</code></pre>\n\n<p>Step 8：編譯安裝 fuse</p>\n\n<pre><code>$ ./configure --prefix=/usr\n$ make\n$ make install\n</code></pre>\n\n<p>Step 9：將 fuse 加入 ubuntu kernel</p>\n\n<pre><code>$ modprobe fuse\n</code></pre>\n\n<p>Step 10：確認 fuse 是否安裝完成</p>\n\n<pre><code>$ pkg-config --modversion fuse\n</code></pre>\n\n<p>Step 11：進入 /usr/local/src 資料夾</p>\n\n<pre><code>$ cd /usr/local/src\n</code></pre>\n\n<p>Step 12：下載 s3fs</p>\n\n<pre><code>$ wget http://s3fs.googlecode.com/files/s3fs-1.71.tar.gz\n</code></pre>\n\n<p>Step 13：解壓縮 s3fs</p>\n\n<pre><code>$ tar -xzvf s3fs-1.71.tar.gz\n</code></pre>\n\n<p>Step 14：進入 s3fs-1.71 資料夾</p>\n\n<pre><code>$ cd s3fs-1.71\n</code></pre>\n\n<p>Step 15：編譯安裝 s3fs</p>\n\n<pre><code>$ ./configure --prefix=/usr\n$ make\n$ make install\n</code></pre>\n\n<p>完成以上步驟，就可以在 EC2 的 Ubuntu Instance 安裝好可掛載 S3 的環境，關於掛載及卸載部份請見<a href=\"http://blog.fukuball.com/ru-he-gua-zai-aws-s3-dao-aws-ec2-instance-gua-zai-ji-xie-zai-bu-fen/\" title=\"如何掛載 AWS S3 到 AWS EC2 Instance - 掛載及卸載部份\">如何掛載 AWS S3 到 AWS EC2 Instance - 掛載及卸載部份</a> 。</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-03-10T17:00:23.000Z","created_by":1,"updated_at":"2014-03-10T17:03:52.000Z","updated_by":1,"published_at":"2014-03-10T17:00:30.000Z","published_by":1},{"id":13,"uuid":"fef20a39-3d7c-4d3b-9b30-4edf26448620","title":"如何掛載 AWS S3 到 AWS EC2 Instance - 掛載及卸載部份","slug":"ru-he-gua-zai-aws-s3-dao-aws-ec2-instance-gua-zai-ji-xie-zai-bu-fen","markdown":"本篇為筆記為如何掛載 AWS S3 到 AWS EC2 Instance 的後續內容，請先閱讀 [如何掛載 AWS S3 到 AWS EC2 Instance - 環境安裝部份](http://blog.fukuball.com/ru-he-gua-zai-aws-s3-dao-aws-ec2-instance-huan-jing-an-zhuang-bu-fen-2/ \"如何掛載 AWS S3 到 AWS EC2 Instance - 環境安裝部份\") 後再閱讀本篇筆記。\n\n架構網站服務時，Storage 的規劃也是重要的一環，而 AWS S3 就是一個蠻好的 Storage 服務。\n\n一般常見的做法我們會透過 AWS 提供的 API 來存取 S3 上的檔案，但這樣做並不直覺，而且要通常要將原本存取檔案的程式寫法改成用 S3 API 存取檔案的寫法，有可能會需要修改許多支程式。\n\n所以便有人萌生了將 S3 掛載到 EC2 Instance 的想法，就跟我們買一顆大容量的硬碟裝在電腦上一樣，讓我們的網站服務能夠像在同一部機器存取檔案一樣容易。（其實概念就像我們在實體機器上使用 NFS 來掛載網路硬碟一樣）\n\n掛載細節如下：（請確認已開好 S3 bucket，且開好 IAM user 權限，且完成環境安裝）\n\nStep 1：新增 s3fs passwd 檔案\n\n    $ touch /etc/passwd-s3fs\n\nStep 2：編輯 s3fs 檔案\n\n    $ vim /etc/passwd-s3fs\n\nStep 3：填入設定 S3 bucket 時的 bucket name 及設定 IAM user 時得到的 access key id、secret access key\n\n    bucketName:accessKeyID:secretAccessKey\n\nStep 4：更改 s3fs 檔案權限\n\n    $ chmod 640 /etc/passwd-s3fs\n\nStep5：新增 S3 bucket 所要掛載的位置\n\n    $ mkdir /mnt/s3-drive\n\nStep6：將 S3 bucket 掛載上去\n\n    $ /usr/bin/s3fs <bucket-name> /mnt/s3-drive -o allow_other\n\n當完成上述步驟我們就已經將 S3 bucket 掛載到 EC2 的 /mnt/s3-drive 了\n我們可以用 df -h 來確認：\n\n    $ df -h\n    Filesystem      Size  Used Avail Use% Mounted on\n    /dev/xvda1      7.9G  1.3G  6.3G  17% /\n    udev            288M  8.0K  288M   1% /dev\n    tmpfs           119M  172K  118M   1% /run\n    none            5.0M     0  5.0M   0% /run/lock\n    none            296M     0  296M   0% /run/shm\n    s3fs            256T     0  256T   0% /mnt/s3-drive\n\n恭喜！EC2 Instance 多了 256T 的空間了！\n\n若想要卸載 S3 bucket 指令如下：\n\n    $ fusermount -u /mnt/s3-drive\n\n以上就是如何掛載 AWS S3 到 AWS EC2 Instance 的簡易筆記～","html":"<p>本篇為筆記為如何掛載 AWS S3 到 AWS EC2 Instance 的後續內容，請先閱讀 <a href=\"http://blog.fukuball.com/ru-he-gua-zai-aws-s3-dao-aws-ec2-instance-huan-jing-an-zhuang-bu-fen-2/\" title=\"如何掛載 AWS S3 到 AWS EC2 Instance - 環境安裝部份\">如何掛載 AWS S3 到 AWS EC2 Instance - 環境安裝部份</a> 後再閱讀本篇筆記。</p>\n\n<p>架構網站服務時，Storage 的規劃也是重要的一環，而 AWS S3 就是一個蠻好的 Storage 服務。</p>\n\n<p>一般常見的做法我們會透過 AWS 提供的 API 來存取 S3 上的檔案，但這樣做並不直覺，而且要通常要將原本存取檔案的程式寫法改成用 S3 API 存取檔案的寫法，有可能會需要修改許多支程式。</p>\n\n<p>所以便有人萌生了將 S3 掛載到 EC2 Instance 的想法，就跟我們買一顆大容量的硬碟裝在電腦上一樣，讓我們的網站服務能夠像在同一部機器存取檔案一樣容易。（其實概念就像我們在實體機器上使用 NFS 來掛載網路硬碟一樣）</p>\n\n<p>掛載細節如下：（請確認已開好 S3 bucket，且開好 IAM user 權限，且完成環境安裝）</p>\n\n<p>Step 1：新增 s3fs passwd 檔案</p>\n\n<pre><code>$ touch /etc/passwd-s3fs\n</code></pre>\n\n<p>Step 2：編輯 s3fs 檔案</p>\n\n<pre><code>$ vim /etc/passwd-s3fs\n</code></pre>\n\n<p>Step 3：填入設定 S3 bucket 時的 bucket name 及設定 IAM user 時得到的 access key id、secret access key</p>\n\n<pre><code>bucketName:accessKeyID:secretAccessKey\n</code></pre>\n\n<p>Step 4：更改 s3fs 檔案權限</p>\n\n<pre><code>$ chmod 640 /etc/passwd-s3fs\n</code></pre>\n\n<p>Step5：新增 S3 bucket 所要掛載的位置</p>\n\n<pre><code>$ mkdir /mnt/s3-drive\n</code></pre>\n\n<p>Step6：將 S3 bucket 掛載上去</p>\n\n<pre><code>$ /usr/bin/s3fs &lt;bucket-name&gt; /mnt/s3-drive -o allow_other\n</code></pre>\n\n<p>當完成上述步驟我們就已經將 S3 bucket 掛載到 EC2 的 /mnt/s3-drive 了\n我們可以用 df -h 來確認：</p>\n\n<pre><code>$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/xvda1      7.9G  1.3G  6.3G  17% /\nudev            288M  8.0K  288M   1% /dev\ntmpfs           119M  172K  118M   1% /run\nnone            5.0M     0  5.0M   0% /run/lock\nnone            296M     0  296M   0% /run/shm\ns3fs            256T     0  256T   0% /mnt/s3-drive\n</code></pre>\n\n<p>恭喜！EC2 Instance 多了 256T 的空間了！</p>\n\n<p>若想要卸載 S3 bucket 指令如下：</p>\n\n<pre><code>$ fusermount -u /mnt/s3-drive\n</code></pre>\n\n<p>以上就是如何掛載 AWS S3 到 AWS EC2 Instance 的簡易筆記～</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-03-10T17:02:29.000Z","created_by":1,"updated_at":"2014-03-10T17:02:44.000Z","updated_by":1,"published_at":"2014-03-10T17:02:29.000Z","published_by":1},{"id":14,"uuid":"4b35db87-772b-491b-a18c-543c47e6d9d7","title":"如何讓 AWS EC2 Instance 開機時自動掛載 AWS S3 Bucket","slug":"ru-he-rang-aws-ec2-instance-kai-ji-shi-zi-dong-gua-zai-aws-s3-bucket","markdown":"當我們將 S3 bucket 掛到 EC2 instance 時（詳細設定筆記請見：[如何掛載 AWS S3 到 AWS EC2 Instance - 環境安裝部份](https://coderwall.com/p/kdpssg \"如何掛載 AWS S3 到 AWS EC2 Instance - 環境安裝部份\") 、[如何掛載 AWS S3 到 AWS EC2 Instance - 掛載及卸載部份](https://coderwall.com/p/c8ssvg \"如何掛載 AWS S3 到 AWS EC2 Instance - 掛載及卸載部份\") ），若將 EC2 instance 重開機，S3 bucket 是不會自動掛載上去的，這時我們可以寫一個 Script 加入排程來讓 EC2 instance 重開機能夠自動掛載 S3 bucket。 \n\n詳細步驟如下：\n\nStep 1：編寫 automount-s3 的 shell script\n\n    $ vim automount-s3\n\n script 內容：\n\n    sudo mkdir /mnt/s3-drive\n    /usr/bin/s3fs <bucketname> <mount-point> -o allow_other\n\nStep 2：將 automount-s3 script 移至 /usr/sbin\n\n    $ sudo mv automount-s3 /usr/sbin\n\nStep 3：更改 automount-s3 script 權限\n\n    $ sudo chown root:root /usr/sbin/automount-s3\n    $ sudo chmod +x /usr/sbin/automount-s3\n\nStep 4：將 automount-s3 script 加入 crontab 設定在重新開機時自動執行\n\n    $ crontab -e\n \ncrontab 內容\n   \n    @reboot /usr/sbin/automount-s3\n\n完成以上步驟我們就可以讓 EC2 instance 在重新開機時自動掛載 S3 bucket，我們可以實際重新開機測試：\n\n    sudo reboot\n\n重新開機後，我們可以用 df -h 來確認是否有自動掛載：\n\n    $ df -h\n    Filesystem      Size  Used Avail Use% Mounted on\n    /dev/xvda1      7.9G  1.3G  6.3G  17% /\n    udev            288M  8.0K  288M   1% /dev\n    tmpfs           119M  172K  118M   1% /run\n    none            5.0M     0  5.0M   0% /run/lock\n    none            296M     0  296M   0% /run/shm\n    s3fs            256T     0  256T   0% /mnt/s3-drive\n\n以上就是如何讓 AWS EC2 Instance 在開機時自動掛載 AWS S3 Bucket 的簡易筆記～\n","html":"<p>當我們將 S3 bucket 掛到 EC2 instance 時（詳細設定筆記請見：<a href=\"https://coderwall.com/p/kdpssg\" title=\"如何掛載 AWS S3 到 AWS EC2 Instance - 環境安裝部份\">如何掛載 AWS S3 到 AWS EC2 Instance - 環境安裝部份</a> 、<a href=\"https://coderwall.com/p/c8ssvg\" title=\"如何掛載 AWS S3 到 AWS EC2 Instance - 掛載及卸載部份\">如何掛載 AWS S3 到 AWS EC2 Instance - 掛載及卸載部份</a> ），若將 EC2 instance 重開機，S3 bucket 是不會自動掛載上去的，這時我們可以寫一個 Script 加入排程來讓 EC2 instance 重開機能夠自動掛載 S3 bucket。 </p>\n\n<p>詳細步驟如下：</p>\n\n<p>Step 1：編寫 automount-s3 的 shell script</p>\n\n<pre><code>$ vim automount-s3\n</code></pre>\n\n<p>script 內容：</p>\n\n<pre><code>sudo mkdir /mnt/s3-drive\n/usr/bin/s3fs &lt;bucketname&gt; &lt;mount-point&gt; -o allow_other\n</code></pre>\n\n<p>Step 2：將 automount-s3 script 移至 /usr/sbin</p>\n\n<pre><code>$ sudo mv automount-s3 /usr/sbin\n</code></pre>\n\n<p>Step 3：更改 automount-s3 script 權限</p>\n\n<pre><code>$ sudo chown root:root /usr/sbin/automount-s3\n$ sudo chmod +x /usr/sbin/automount-s3\n</code></pre>\n\n<p>Step 4：將 automount-s3 script 加入 crontab 設定在重新開機時自動執行</p>\n\n<pre><code>$ crontab -e\n</code></pre>\n\n<p>crontab 內容</p>\n\n<pre><code>@reboot /usr/sbin/automount-s3\n</code></pre>\n\n<p>完成以上步驟我們就可以讓 EC2 instance 在重新開機時自動掛載 S3 bucket，我們可以實際重新開機測試：</p>\n\n<pre><code>sudo reboot\n</code></pre>\n\n<p>重新開機後，我們可以用 df -h 來確認是否有自動掛載：</p>\n\n<pre><code>$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/xvda1      7.9G  1.3G  6.3G  17% /\nudev            288M  8.0K  288M   1% /dev\ntmpfs           119M  172K  118M   1% /run\nnone            5.0M     0  5.0M   0% /run/lock\nnone            296M     0  296M   0% /run/shm\ns3fs            256T     0  256T   0% /mnt/s3-drive\n</code></pre>\n\n<p>以上就是如何讓 AWS EC2 Instance 在開機時自動掛載 AWS S3 Bucket 的簡易筆記～</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-03-10T17:06:23.000Z","created_by":1,"updated_at":"2014-03-10T17:06:23.000Z","updated_by":1,"published_at":"2014-03-10T17:06:23.000Z","published_by":1},{"id":15,"uuid":"7e4e0497-614c-42e8-86df-e45045161394","title":"如何開啓 Apache2 的 mod_rewrite","slug":"ru-he-kai-qi-apache2-de-mod_rewrite","markdown":"使用 Apache2 作為網頁伺服器時，尤其是 PHP Web Application，通常會使用 rewrite rule 來改寫網址，最近開 AWS 上的 Ubuntu 12.04 機器，安裝 Apache2 時 mod_rewrite 預設並非開啓的，所以我們就要自行開啓 mod_rewrite。\n\n指令如下：\n\n    $ sudo a2enmod rewrite\n    Enabling module rewrite.\n    To activate the new configuration, you need to run:\n    service apache2 restart \n\n開啓後，重開 apache2 即可啓用 mod_rewrite。","html":"<p>使用 Apache2 作為網頁伺服器時，尤其是 PHP Web Application，通常會使用 rewrite rule 來改寫網址，最近開 AWS 上的 Ubuntu 12.04 機器，安裝 Apache2 時 mod<em>rewrite 預設並非開啓的，所以我們就要自行開啓 mod</em>rewrite。</p>\n\n<p>指令如下：</p>\n\n<pre><code>$ sudo a2enmod rewrite\nEnabling module rewrite.\nTo activate the new configuration, you need to run:\nservice apache2 restart \n</code></pre>\n\n<p>開啓後，重開 apache2 即可啓用 mod_rewrite。</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-03-10T17:08:05.000Z","created_by":1,"updated_at":"2014-03-10T17:08:05.000Z","updated_by":1,"published_at":"2014-03-10T17:08:05.000Z","published_by":1},{"id":16,"uuid":"80a376f5-e10a-471f-bd79-ea69d42499ee","title":"如何在 OSX 安裝 Vagrant 開啓 Ubuntu 12.04 LTS 32-bit 虛擬環境","slug":"ru-he-zai-osx-an-zhuang-vagrant-kai-qi-ubuntu-12-04-lts-32-bit-xu-ni-huan-jing","markdown":"安裝開發環境一直是個麻煩的問題，因此使用 Vagrant 來無痛安裝一個乾淨的開發環境，將各個 project 所需要的開發環境獨立出來是目前最好的解決方案了。本篇文章將介紹如何在 OSX 上安裝 Vagrant 並開啓 Ubuntu 12.04 LTS 32-bit 虛擬環境。\n\nStep 1：安裝 [VirtualBox](https://www.virtualbox.org/wiki/Downloads)\n\nStep 2：安裝 [Vagrant](http://downloads.vagrantup.com/)\n\nStep 3：開一個資料夾來練習\n\n    $ mkdir vagrant-practice\n    $ cd vagrant-practice\n\nStep 4：初始化虛擬環境\n\n    $ vagrant init precise32 http://files.vagrantup.com/precise32.box\n\nStep 5：開啟虛擬環境（此步驟會等待比較久的時間，之後執行則會變快）\n\n    $ vagrant up\n\nStep 6：登入虛擬環境\n\n    $ vagrant ssh\n\n如此就有一個全新的 Ubuntu 12.04 LTS 32-bit 虛擬環境了！酷！ \n\n若想刪除虛擬環境，則執行以下指令：\n\n    $ vagrant destroy\n","html":"<p>安裝開發環境一直是個麻煩的問題，因此使用 Vagrant 來無痛安裝一個乾淨的開發環境，將各個 project 所需要的開發環境獨立出來是目前最好的解決方案了。本篇文章將介紹如何在 OSX 上安裝 Vagrant 並開啓 Ubuntu 12.04 LTS 32-bit 虛擬環境。</p>\n\n<p>Step 1：安裝 <a href=\"https://www.virtualbox.org/wiki/Downloads\">VirtualBox</a></p>\n\n<p>Step 2：安裝 <a href=\"http://downloads.vagrantup.com/\">Vagrant</a></p>\n\n<p>Step 3：開一個資料夾來練習</p>\n\n<pre><code>$ mkdir vagrant-practice\n$ cd vagrant-practice\n</code></pre>\n\n<p>Step 4：初始化虛擬環境</p>\n\n<pre><code>$ vagrant init precise32 http://files.vagrantup.com/precise32.box\n</code></pre>\n\n<p>Step 5：開啟虛擬環境（此步驟會等待比較久的時間，之後執行則會變快）</p>\n\n<pre><code>$ vagrant up\n</code></pre>\n\n<p>Step 6：登入虛擬環境</p>\n\n<pre><code>$ vagrant ssh\n</code></pre>\n\n<p>如此就有一個全新的 Ubuntu 12.04 LTS 32-bit 虛擬環境了！酷！ </p>\n\n<p>若想刪除虛擬環境，則執行以下指令：</p>\n\n<pre><code>$ vagrant destroy\n</code></pre>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-03-10T17:09:52.000Z","created_by":1,"updated_at":"2014-03-10T17:09:52.000Z","updated_by":1,"published_at":"2014-03-10T17:09:52.000Z","published_by":1},{"id":17,"uuid":"33389d81-62ce-47a7-8279-07fb63b3b9d2","title":"如何在 Sublime Text 2 搜尋所有非 ASCII 字元","slug":"ru-he-zai-sublime-text-2-sou-xun-suo-you-fei-ascii-zi-yuan","markdown":"寫程式有時會需要找出原始碼裡所有非 ASCII 字元，拜訪了一下 Google 大神得到了這個答案，筆記一下！\n\nRegular Expression:\n\n    [^\\x00-\\x7F]","html":"<p>寫程式有時會需要找出原始碼裡所有非 ASCII 字元，拜訪了一下 Google 大神得到了這個答案，筆記一下！</p>\n\n<p>Regular Expression:</p>\n\n<pre><code>[^\\x00-\\x7F]\n</code></pre>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-04-15T08:11:24.000Z","created_by":1,"updated_at":"2014-04-15T08:11:24.000Z","updated_by":1,"published_at":"2014-04-15T08:11:24.000Z","published_by":1},{"id":18,"uuid":"82ac2ccd-5018-4d82-a3a8-784af239326a","title":"人工進化電影一幕","slug":"ren-gong-jin-hua-dian-ying-mu","markdown":"<p style=\"text-align:center\">\n<img src=\"http://static.obeobe.com/image/blog-image/人工進化一幕.jpg\" alt=\"ice dog\">\n</p>\n\n某天騎車的時候突然很懷念小時候養蠶寶寶的時光，看著蠶寶寶成長茁壯，慢慢從小小隻變得肥滋滋的，然後再結成繭、變成蛹，最後蛻變成蛾，而養蠶寶寶最令人期待的就是當牠們變成蛾交配產卵的時刻，一切就是這麼的自然，讓人感到生命的生生不息。天真的小時候總是這麼企盼著。\n\n正當懷念著這天真的小時候，突然有種莫名的既視感。\n\n將生物放在盒子裡看牠們交配，這句乍看之下非常不人道的句子，用在養蠶寶寶這樣生物養成觀察的例子上似乎再正常不過，而電影人工進化裡也有這樣的一幕。\n\n人工進化電影中生化工程師克萊夫與艾爾莎，成功地在實驗室中製造出了一個富含高蛋白的生命體（就是圖片中的肉球），期盼未來將可能解決人類的多項疾病，在他們完成了一些階段性目標時，實驗室決定舉行一場公開展示來公開他們的研究成果。\n\n公開展示的時候，實驗室將一公一母的高蛋白生命體放在一個盒子裡，讓這兩個高蛋白生命體來個運命中浪漫的相遇，為了戲劇效果，他們甚至用了一些浪漫的話語來介紹這兩團肉球，搭配上浪漫的音樂，讓所有的觀眾都沈浸在浪漫的氣氛裡面，觀眾們一致發出讚嘆的聲音！\n\n一切就是這麼的美好。兩團肉球在人類扮演上帝的時候誕生了，而人類上帝又讓牠們相遇，期待牠們相愛、交配，生下牠們愛的結晶。即使你在看前面這句話的時候，似乎有那麼點覺得怪怪的，但仔細一想，這似乎跟我們小時候期待著蠶寶寶的蛾交配產卵那樣的單純。當觀眾為這兩團肉球發出讚嘆的歡呼聲時，<strong>我真心覺得他們就是單純著期待著生命美好的那一刻。</strong>\n\n當然，電影的下一幕不是那麼美好，高蛋白生命體性別狀態並不穩定，因此原本雌性的肉球突然變性成雄性，於是美好的戀情竟演變成了一場血腥的殺戮。\n\n<p style=\"text-align:center\">\n<img src=\"http://static.obeobe.com/image/subtitle-image/我給你錢，快點做.jpg\" alt=\"ice dog\">\n</p>\n\n這一幕就是我對人工進化這部電影最深刻的印象，當你看到這一幕時，會是感到生命的單純美好亦或是對這樣的情景感到噁心呢？還是會像星爺一樣吆喝著：「我給你錢，快點做！」\n\n我只想到小時候養蠶寶寶的時光！","html":"<p style=\"text-align:center\">  \n<img src=\"http://static.obeobe.com/image/blog-image/人工進化一幕.jpg\" alt=\"ice dog\">  \n</p>\n\n<p>某天騎車的時候突然很懷念小時候養蠶寶寶的時光，看著蠶寶寶成長茁壯，慢慢從小小隻變得肥滋滋的，然後再結成繭、變成蛹，最後蛻變成蛾，而養蠶寶寶最令人期待的就是當牠們變成蛾交配產卵的時刻，一切就是這麼的自然，讓人感到生命的生生不息。天真的小時候總是這麼企盼著。</p>\n\n<p>正當懷念著這天真的小時候，突然有種莫名的既視感。</p>\n\n<p>將生物放在盒子裡看牠們交配，這句乍看之下非常不人道的句子，用在養蠶寶寶這樣生物養成觀察的例子上似乎再正常不過，而電影人工進化裡也有這樣的一幕。</p>\n\n<p>人工進化電影中生化工程師克萊夫與艾爾莎，成功地在實驗室中製造出了一個富含高蛋白的生命體（就是圖片中的肉球），期盼未來將可能解決人類的多項疾病，在他們完成了一些階段性目標時，實驗室決定舉行一場公開展示來公開他們的研究成果。</p>\n\n<p>公開展示的時候，實驗室將一公一母的高蛋白生命體放在一個盒子裡，讓這兩個高蛋白生命體來個運命中浪漫的相遇，為了戲劇效果，他們甚至用了一些浪漫的話語來介紹這兩團肉球，搭配上浪漫的音樂，讓所有的觀眾都沈浸在浪漫的氣氛裡面，觀眾們一致發出讚嘆的聲音！</p>\n\n<p>一切就是這麼的美好。兩團肉球在人類扮演上帝的時候誕生了，而人類上帝又讓牠們相遇，期待牠們相愛、交配，生下牠們愛的結晶。即使你在看前面這句話的時候，似乎有那麼點覺得怪怪的，但仔細一想，這似乎跟我們小時候期待著蠶寶寶的蛾交配產卵那樣的單純。當觀眾為這兩團肉球發出讚嘆的歡呼聲時，<strong>我真心覺得他們就是單純著期待著生命美好的那一刻。</strong></p>\n\n<p>當然，電影的下一幕不是那麼美好，高蛋白生命體性別狀態並不穩定，因此原本雌性的肉球突然變性成雄性，於是美好的戀情竟演變成了一場血腥的殺戮。</p>\n\n<p style=\"text-align:center\">  \n<img src=\"http://static.obeobe.com/image/subtitle-image/我給你錢，快點做.jpg\" alt=\"ice dog\">  \n</p>\n\n<p>這一幕就是我對人工進化這部電影最深刻的印象，當你看到這一幕時，會是感到生命的單純美好亦或是對這樣的情景感到噁心呢？還是會像星爺一樣吆喝著：「我給你錢，快點做！」</p>\n\n<p>我只想到小時候養蠶寶寶的時光！</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-04-22T14:33:02.000Z","created_by":1,"updated_at":"2014-06-05T13:30:29.000Z","updated_by":1,"published_at":"2014-04-22T14:33:02.000Z","published_by":1},{"id":19,"uuid":"2c99f0bc-2ead-41ef-b7c4-b5abff0ddb0b","title":"使用 Private Key 登入 AWS EC2","slug":"shi-yong-private-key-deng-ru-aws-ec2","markdown":"首先在開 AWS EC2 之前，應該會先取得一組 Private Key，請好好保存這組 Private Key，它會是個 .pem 或 .cer 檔。\n\n開啓 Terminal 之後，請用以下指令登入 AWS EC2：\n\nStep 1：更改 Private Key 檔案權限\n\n\t＄chmod 600 path/to/private-key.pem\n    \nStep 2：使用 SSH 登入 AWS EC2\n\n\t$ssh -i path/to/private-key.pem ubuntu@ec2-X-X-X.compute-X.amazonaws.com\n    \n這樣應該就可以順利登入 AWS EC2 了，其中 Step 1 只要執行過一次就可以了，簡單。","html":"<p>首先在開 AWS EC2 之前，應該會先取得一組 Private Key，請好好保存這組 Private Key，它會是個 .pem 或 .cer 檔。</p>\n\n<p>開啓 Terminal 之後，請用以下指令登入 AWS EC2：</p>\n\n<p>Step 1：更改 Private Key 檔案權限</p>\n\n<pre><code>＄chmod 600 path/to/private-key.pem\n</code></pre>\n\n<p>Step 2：使用 SSH 登入 AWS EC2</p>\n\n<pre><code>$ssh -i path/to/private-key.pem ubuntu@ec2-X-X-X.compute-X.amazonaws.com\n</code></pre>\n\n<p>這樣應該就可以順利登入 AWS EC2 了，其中 Step 1 只要執行過一次就可以了，簡單。</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-04-22T18:22:03.000Z","created_by":1,"updated_at":"2014-04-22T18:22:03.000Z","updated_by":1,"published_at":"2014-04-22T18:22:03.000Z","published_by":1},{"id":20,"uuid":"c1951efb-2f86-4f90-a648-b99d4edb320a","title":"我的 Sublime Text 2 設定","slug":"wo-de-sublime-text-2-she-ding","markdown":"使用文字編輯器撰寫程式碼的時候，第一步就是要挑整適合於自己使用的編輯環境，在 Sublime Text 2 裡只要使用快捷鍵 <code>⌘,</code> 就可以開啟設定頁面，然後就可以依照個人使用情況來做調整啦～\n\n以下是我目前的 Sublime Text 2 設定值，大家可以參考看看\n\n\t{\n\t\t\"font_size\": 18.0,\n\t\t\"ignored_packages\":\n\t\t[\n\t\t\t\"Vintage\"\n\t\t],\n\t\t\"tab_size\": 4,\n\t\t\"translate_tabs_to_spaces\": true,\n    \t\"highlight_line\": true,\n    \t\"trim_trailing_white_space_on_save\": true\n\t}\n    \n我將 font_size 設成 18，這樣對眼睛比較好，畢竟要長時間看程式碼，還是大一點的字型比較好。\n\n另外，Sublime Text 可以透過 Vintage 這個內建的 package 提供 vi 模擬模式，讓使用者可以使用 vi 的指令模式來操作 Sublime Text，由於我個人不熟悉 vi，所以就在 ignored_packages 將這個 packeage ignore 掉，其實 Sublime Text 一開始預設就是 ignore 這個 package 的，畢竟都已經使用 Sublime Text 了，要使用 vi 就使用真正的 vi 吧。\n\ntab_size 我是設成 4，其實之前我都是使用 3，但實在太多 open source 的 project 都是使用 4，只好改變我的習慣。\n\ntranslate_tabs_to_spaces 設成 true 可以將 tab 都轉成空白，這樣使用別人的 code 比較不會造成排版亂掉（如果大家的 tab_size 不同的話）。\n\nhighlight_line 設成 true 可以讓游標所在的行高亮顯示，一樣是為了自己的眼睛好，當然要設成 true。\n\ntrim_trailing_white_space_on_save 設成 ture，可以在儲存檔案時自動將多餘的空白去除掉，有時行末多餘的空白可能會造成一些奇怪的問題，就讓 Sublime Text 來幫我們把關吧～","html":"<p>使用文字編輯器撰寫程式碼的時候，第一步就是要挑整適合於自己使用的編輯環境，在 Sublime Text 2 裡只要使用快捷鍵 <code>⌘,</code> 就可以開啟設定頁面，然後就可以依照個人使用情況來做調整啦～</p>\n\n<p>以下是我目前的 Sublime Text 2 設定值，大家可以參考看看</p>\n\n<pre><code>{\n    \"font_size\": 18.0,\n    \"ignored_packages\":\n    [\n        \"Vintage\"\n    ],\n    \"tab_size\": 4,\n    \"translate_tabs_to_spaces\": true,\n    \"highlight_line\": true,\n    \"trim_trailing_white_space_on_save\": true\n}\n</code></pre>\n\n<p>我將 font_size 設成 18，這樣對眼睛比較好，畢竟要長時間看程式碼，還是大一點的字型比較好。</p>\n\n<p>另外，Sublime Text 可以透過 Vintage 這個內建的 package 提供 vi 模擬模式，讓使用者可以使用 vi 的指令模式來操作 Sublime Text，由於我個人不熟悉 vi，所以就在 ignored_packages 將這個 packeage ignore 掉，其實 Sublime Text 一開始預設就是 ignore 這個 package 的，畢竟都已經使用 Sublime Text 了，要使用 vi 就使用真正的 vi 吧。</p>\n\n<p>tab_size 我是設成 4，其實之前我都是使用 3，但實在太多 open source 的 project 都是使用 4，只好改變我的習慣。</p>\n\n<p>translate_tabs_to_spaces 設成 true 可以將 tab 都轉成空白，這樣使用別人的 code 比較不會造成排版亂掉（如果大家的 tab_size 不同的話）。</p>\n\n<p>highlight_line 設成 true 可以讓游標所在的行高亮顯示，一樣是為了自己的眼睛好，當然要設成 true。</p>\n\n<p>trim_trailing_white_space_on_save 設成 ture，可以在儲存檔案時自動將多餘的空白去除掉，有時行末多餘的空白可能會造成一些奇怪的問題，就讓 Sublime Text 來幫我們把關吧～</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-04-24T10:03:53.000Z","created_by":1,"updated_at":"2014-04-24T10:04:37.000Z","updated_by":1,"published_at":"2014-04-24T10:03:53.000Z","published_by":1},{"id":21,"uuid":"91bd6c45-cd95-4ea3-839c-618dc78346e7","title":"我在 Sublime Text 2 常用的快捷鍵","slug":"wo-zai-sublime-text-2-chang-yong-de-kuai-jie-jian","markdown":"Sublime Text 2 好用的地方在於它有許多方便的快捷鍵，當然已經有人整理出這些方便的快捷鍵（請參考：[Sublime Text 2 – Useful Shortcuts (Mac OS X)](https://gist.github.com/lucasfais/1207002)），而我這篇主要是分享我個人常用的，其他冷門的快捷鍵其實不用浪費我們的腦容量去記。\n\n符號說明：\n\n- ⌘ 為俗稱的蘋果鍵 \n- ⇧ 為 shift 鍵\n- ⌃ 為 control 鍵\n- ⌥ 為 option/alt 鍵\n\n以下是我常用的快捷鍵列表：\n\n###一般\n<table>\n\t<tr>\n    \t<td width=\"150px\">⌘R</td>\n        <td>跳至某個 method</td>\n    </tr>\n    <tr>\n    \t<td>⌘⇧P</td>\n        <td>開啟 Sublime Text 的命令列</td>\n    </tr>\n    <tr>\n    \t<td>⌃ `</td>\n        <td>開啟 python console</td>\n    </tr>\n</table>\n\n###編輯\n<table>\n\t<tr>\n    \t<td width=\"150px\">⌘L</td>\n        <td>全選所在行，連續按則往下繼續選下一行</td>\n    </tr>\n    <tr>\n    \t<td>⌘D</td>\n        <td>全選所在單字，連續按則往下繼續選相同單字，可以一次同時編輯所有選擇的單字</td>\n    </tr>\n    <tr>\n    \t<td>⌃⇧M</td>\n        <td>選擇花刮號裡所有內容</td>\n    </tr>\n    <tr>\n    \t<td>⌃M</td>\n        <td>跳至相配對的花刮號</td>\n    </tr>\n    <tr>\n    \t<td>⌃⇧K</td>\n        <td>刪除所在行，連續按則往下繼續刪下一行</td>\n    </tr>\n    <tr>\n    \t<td>⌘Z</td>\n        <td>復原</td>\n    </tr>\n    <tr>\n    \t<td>⌘⇧Z</td>\n        <td>反復原</td>\n    </tr>\n    <tr>\n    \t<td>⌘⌥ + 滑鼠選擇</td>\n        <td>可以垂直選擇</td>\n    </tr>\n</table>\n\n###尋找/取代\n<table>\n\t<tr>\n    \t<td width=\"150px\">⌘F</td>\n        <td>尋找</td>\n    </tr>\n    <tr>\n    \t<td>⌘⌥F</td>\n        <td>取代</td>\n    </tr>\n    <tr>\n    \t<td>⌘⇧F</td>\n        <td>在整個 Project 尋找/取代</td>\n    </tr>\n</table>\n","html":"<p>Sublime Text 2 好用的地方在於它有許多方便的快捷鍵，當然已經有人整理出這些方便的快捷鍵（請參考：<a href=\"https://gist.github.com/lucasfais/1207002\">Sublime Text 2 – Useful Shortcuts (Mac OS X)</a>），而我這篇主要是分享我個人常用的，其他冷門的快捷鍵其實不用浪費我們的腦容量去記。</p>\n\n<p>符號說明：</p>\n\n<ul>\n<li>⌘ 為俗稱的蘋果鍵 </li>\n<li>⇧ 為 shift 鍵</li>\n<li>⌃ 為 control 鍵</li>\n<li>⌥ 為 option/alt 鍵</li>\n</ul>\n\n<p>以下是我常用的快捷鍵列表：</p>\n\n<h3 id=\"\">一般</h3>\n\n<table>  \n    <tr>\n        <td width=\"150px\">⌘R</td>\n        <td>跳至某個 method</td>\n    </tr>\n    <tr>\n        <td>⌘⇧P</td>\n        <td>開啟 Sublime Text 的命令列</td>\n    </tr>\n    <tr>\n        <td>⌃ `</td>\n        <td>開啟 python console</td>\n    </tr>\n</table>\n\n<h3 id=\"\">編輯</h3>\n\n<table>  \n    <tr>\n        <td width=\"150px\">⌘L</td>\n        <td>全選所在行，連續按則往下繼續選下一行</td>\n    </tr>\n    <tr>\n        <td>⌘D</td>\n        <td>全選所在單字，連續按則往下繼續選相同單字，可以一次同時編輯所有選擇的單字</td>\n    </tr>\n    <tr>\n        <td>⌃⇧M</td>\n        <td>選擇花刮號裡所有內容</td>\n    </tr>\n    <tr>\n        <td>⌃M</td>\n        <td>跳至相配對的花刮號</td>\n    </tr>\n    <tr>\n        <td>⌃⇧K</td>\n        <td>刪除所在行，連續按則往下繼續刪下一行</td>\n    </tr>\n    <tr>\n        <td>⌘Z</td>\n        <td>復原</td>\n    </tr>\n    <tr>\n        <td>⌘⇧Z</td>\n        <td>反復原</td>\n    </tr>\n    <tr>\n        <td>⌘⌥ + 滑鼠選擇</td>\n        <td>可以垂直選擇</td>\n    </tr>\n</table>\n\n<h3 id=\"\">尋找/取代</h3>\n\n<table>  \n    <tr>\n        <td width=\"150px\">⌘F</td>\n        <td>尋找</td>\n    </tr>\n    <tr>\n        <td>⌘⌥F</td>\n        <td>取代</td>\n    </tr>\n    <tr>\n        <td>⌘⇧F</td>\n        <td>在整個 Project 尋找/取代</td>\n    </tr>\n</table>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-04-24T10:12:29.000Z","created_by":1,"updated_at":"2014-04-24T10:50:56.000Z","updated_by":1,"published_at":"2014-04-24T10:43:45.000Z","published_by":1},{"id":22,"uuid":"38908d1b-d6e2-4cdd-b509-d6719445020a","title":"Git 簡易使用教學","slug":"git-jian-yi-shi-yong-jiao-xue","markdown":"###前言\n\n版本控制一直是軟體開發中非常重要的工具，而 Git 與 Subversion、CVS 不同的地方在於 Subversion 及 CVS 是屬於 Centralized VCS，Centralized VCS 的共同缺點是做什麼事都要跟伺服器連線，這樣開發會比較慢，且只要伺服器壞掉，就無法工作了。\n\nGit 則屬於分散式版本控制系統，讓本地端也維護完整的 Repository，即使沒網路，照常可以 commit 和看 history log，伺服器的 Repository 可以在將來有網路連線時再同步更新。\n\n###安裝設定 Git\n\n[Github](https://github.com/) 上有[各大平台完整的安裝及設定教學](https://help.github.com/articles/set-up-git)，建議直接參照這個教學來設定就可以了。 \n\n其中請特別注意設定好提交者的 name 及 Email，Git 會記錄每個 commit 是由誰提交的，這在版本控制上是很重要的資訊。\n\n我們可以使用以下的指令來進行設定：（<code>--global</code> 表示是全域設定）\n\n\t$ git config --global user.name \"Fukuball Lin\"\n    $ git config --global user.email \"fukuball@gmail.com\"\n\n設定完成後可以用以下指令來觀察是否有設定完成\n\n\t$ git config --list\n    user.name=fukuball\n\tuser.email=fukuball@gmail.com\n    \n### git init\n\n當 Git 安裝設定好之後，就可以開始使用 Git 版本控制了，假設現在你有一個 Hello-World 的資料夾，那在這個資料夾底下下以下指令就可以開啟一個 Git Repository：\n\n\t$ git init\n    Initialized empty Git repository in /Users/fukuball/Projects/Hello-World/.git/\n    \n請注意開啟 Git Repository 之後只是在自己的 local 端開啟了一個版本控制資料庫，雖然可以正常在 local 端進行所有版本控制的功能，但因還未連結至 Git Server，他人並無法加入共同開發。\n\n目前比較紅的 Git Server 服務就是使用 Github 了，Github 上也有[完整的教學](https://help.github.com/articles/create-a-repo)說明如何開 Git Repository，並連結至 Github 上的 Git Server 服務。\n\n### git clone\n\n當團隊中有人已開啟了一個在 Git Server 上的 Git Repository，那我們就可以使用 Git clone 來將這個 Repository 抓來自己的 local 端一起進行開發。\n\n首先找到 Git Repository 的位址：\n\n<p style=\"text-align:center\">\n<img src=\"http://static.obeobe.com/image/blog-image/git-1.png\" alt=\"git path\">\n</p>\n\n使用以下指令進行 Clone：\n\n\t$ git clone https://github.com/fukuball/Hello-World.git\n\n特別注意如果有寫入權限的話（被加入成Collaborators），就可以用 SSH 協定 Clone 下來：\n\n\t$ git clone git@github.com:fukuball/Hello-World.git\n    \n使用 SSH Clone 會比較方便，可以不必每次都輸入帳號密碼，但需要事先綁定 SSH Key，如何[綁定 SSH Key 在 Github 上也有完整的教學](https://help.github.com/articles/generating-ssh-keys)。\n\n### git status\n\n我們可以使用 git status 來觀察 Git Repository 的狀態，比如目前所在的 branch 及 哪些檔案還沒 commit 等等。\n\n\t$ git status\n    # On branch master\n\tnothing to commit, working directory clean\n\n### git add (stage)\n\n使用 git add 可以將新增檔案加入 git 版本控制，但我通常就直接使用 git add . 來將所有剛剛修改過或新增加的檔案一次 Add 進 stage 狀態，大部份人不推薦這樣做，認為太暴力，但既然都有版本控制系統了，我個人習慣就不這麼婆婆媽媽的了。\n\n\t$ git add .\n    \n### git commit (commit)\n\nstage 狀態的檔案的下一步就是準備提交了，一個 commit 在 Git 中就是一個節點，這些 commit 的節點就是未來可以回朔及追蹤的參考。當檔案都加入到 stage 了，那就可以使用以下指令來 commit：\n\n\t$ git commit -m \"這次 commit 的適當描述\"\n    \n每個 commit 有個適當的描述是非常重要的，這樣要回朔時會比較容易查找。\n\n當還有檔案沒有進 stage 就下 commit 指令，那就不能 commit，這時可使用 git commit -a -m 這樣的暴力法來一次加入檔案至 stage 然後進行 commit，大部份人不建議這麼做，但我個人習慣不這麼婆婆媽媽。\n\n\t$ git commit -a -m \"這次 commit 的適當描述\"\n    \n### git push\n\n當已經連結了 Git Server，就可以用 git push 來將 local 端的 commit 更新到 Server 上，請注意有修改的檔案還沒 commit 那就無法使用 git push，所以一定要將所有更新都 commit 之後，才有辦法使用 git push。\n\n\t$ git push\n    \n### git pull\n\n當已經連結了 Git Server，我們就可以使用 git pull 來將遠端更新的 code 抓回來，同樣如果 local 端有任何更新，一定都要 commit 之後才\n有辦法使用 git pull。\n\n\t$ git pull\n\n### git log\n\n我們可以使用 git log 的指令查看過去 commit 的紀錄，例如 commit 的版號、作者等等。\n\n\t$ git log\n    \n### .gitigore\n\nlog 檔及 build 出來的檔案及系統產生的檔案如 .DS_Store 等等，我們並不需要 commit 上去 Repository，所以我們會在 Repository 編寫一個 .gitignore 文字檔來忽略這些檔案。\n\n範例 .gitigore 如下：\n\n\t.DS_Store\n\t*.log\n\n###結語\n\n以上是一些 Git 指令的簡易使用教學，但我平常用還是用 GUI 比較多，個人推薦 Github 出品的 GUI 工具，工具只要簡單易用就好了，不太需要什麼複雜的功能啊。\n\nGithub GUI Client 下載：\n\n* [GitHub for Windows](https://windows.github.com/)\n* [GitHub for Mac](https://mac.github.com/)\n  \n ","html":"<h3 id=\"\">前言</h3>\n\n<p>版本控制一直是軟體開發中非常重要的工具，而 Git 與 Subversion、CVS 不同的地方在於 Subversion 及 CVS 是屬於 Centralized VCS，Centralized VCS 的共同缺點是做什麼事都要跟伺服器連線，這樣開發會比較慢，且只要伺服器壞掉，就無法工作了。</p>\n\n<p>Git 則屬於分散式版本控制系統，讓本地端也維護完整的 Repository，即使沒網路，照常可以 commit 和看 history log，伺服器的 Repository 可以在將來有網路連線時再同步更新。</p>\n\n<h3 id=\"git\">安裝設定 Git</h3>\n\n<p><a href=\"https://github.com/\">Github</a> 上有<a href=\"https://help.github.com/articles/set-up-git\">各大平台完整的安裝及設定教學</a>，建議直接參照這個教學來設定就可以了。 </p>\n\n<p>其中請特別注意設定好提交者的 name 及 Email，Git 會記錄每個 commit 是由誰提交的，這在版本控制上是很重要的資訊。</p>\n\n<p>我們可以使用以下的指令來進行設定：（<code>--global</code> 表示是全域設定）</p>\n\n<pre><code>$ git config --global user.name \"Fukuball Lin\"\n$ git config --global user.email \"fukuball@gmail.com\"\n</code></pre>\n\n<p>設定完成後可以用以下指令來觀察是否有設定完成</p>\n\n<pre><code>$ git config --list\nuser.name=fukuball\nuser.email=fukuball@gmail.com\n</code></pre>\n\n<h3 id=\"gitinit\">git init</h3>\n\n<p>當 Git 安裝設定好之後，就可以開始使用 Git 版本控制了，假設現在你有一個 Hello-World 的資料夾，那在這個資料夾底下下以下指令就可以開啟一個 Git Repository：</p>\n\n<pre><code>$ git init\nInitialized empty Git repository in /Users/fukuball/Projects/Hello-World/.git/\n</code></pre>\n\n<p>請注意開啟 Git Repository 之後只是在自己的 local 端開啟了一個版本控制資料庫，雖然可以正常在 local 端進行所有版本控制的功能，但因還未連結至 Git Server，他人並無法加入共同開發。</p>\n\n<p>目前比較紅的 Git Server 服務就是使用 Github 了，Github 上也有<a href=\"https://help.github.com/articles/create-a-repo\">完整的教學</a>說明如何開 Git Repository，並連結至 Github 上的 Git Server 服務。</p>\n\n<h3 id=\"gitclone\">git clone</h3>\n\n<p>當團隊中有人已開啟了一個在 Git Server 上的 Git Repository，那我們就可以使用 Git clone 來將這個 Repository 抓來自己的 local 端一起進行開發。</p>\n\n<p>首先找到 Git Repository 的位址：</p>\n\n<p style=\"text-align:center\">  \n<img src=\"http://static.obeobe.com/image/blog-image/git-1.png\" alt=\"git path\">  \n</p>\n\n<p>使用以下指令進行 Clone：</p>\n\n<pre><code>$ git clone https://github.com/fukuball/Hello-World.git\n</code></pre>\n\n<p>特別注意如果有寫入權限的話（被加入成Collaborators），就可以用 SSH 協定 Clone 下來：</p>\n\n<pre><code>$ git clone git@github.com:fukuball/Hello-World.git\n</code></pre>\n\n<p>使用 SSH Clone 會比較方便，可以不必每次都輸入帳號密碼，但需要事先綁定 SSH Key，如何<a href=\"https://help.github.com/articles/generating-ssh-keys\">綁定 SSH Key 在 Github 上也有完整的教學</a>。</p>\n\n<h3 id=\"gitstatus\">git status</h3>\n\n<p>我們可以使用 git status 來觀察 Git Repository 的狀態，比如目前所在的 branch 及 哪些檔案還沒 commit 等等。</p>\n\n<pre><code>$ git status\n# On branch master\nnothing to commit, working directory clean\n</code></pre>\n\n<h3 id=\"gitaddstage\">git add (stage)</h3>\n\n<p>使用 git add 可以將新增檔案加入 git 版本控制，但我通常就直接使用 git add . 來將所有剛剛修改過或新增加的檔案一次 Add 進 stage 狀態，大部份人不推薦這樣做，認為太暴力，但既然都有版本控制系統了，我個人習慣就不這麼婆婆媽媽的了。</p>\n\n<pre><code>$ git add .\n</code></pre>\n\n<h3 id=\"gitcommitcommit\">git commit (commit)</h3>\n\n<p>stage 狀態的檔案的下一步就是準備提交了，一個 commit 在 Git 中就是一個節點，這些 commit 的節點就是未來可以回朔及追蹤的參考。當檔案都加入到 stage 了，那就可以使用以下指令來 commit：</p>\n\n<pre><code>$ git commit -m \"這次 commit 的適當描述\"\n</code></pre>\n\n<p>每個 commit 有個適當的描述是非常重要的，這樣要回朔時會比較容易查找。</p>\n\n<p>當還有檔案沒有進 stage 就下 commit 指令，那就不能 commit，這時可使用 git commit -a -m 這樣的暴力法來一次加入檔案至 stage 然後進行 commit，大部份人不建議這麼做，但我個人習慣不這麼婆婆媽媽。</p>\n\n<pre><code>$ git commit -a -m \"這次 commit 的適當描述\"\n</code></pre>\n\n<h3 id=\"gitpush\">git push</h3>\n\n<p>當已經連結了 Git Server，就可以用 git push 來將 local 端的 commit 更新到 Server 上，請注意有修改的檔案還沒 commit 那就無法使用 git push，所以一定要將所有更新都 commit 之後，才有辦法使用 git push。</p>\n\n<pre><code>$ git push\n</code></pre>\n\n<h3 id=\"gitpull\">git pull</h3>\n\n<p>當已經連結了 Git Server，我們就可以使用 git pull 來將遠端更新的 code 抓回來，同樣如果 local 端有任何更新，一定都要 commit 之後才\n有辦法使用 git pull。</p>\n\n<pre><code>$ git pull\n</code></pre>\n\n<h3 id=\"gitlog\">git log</h3>\n\n<p>我們可以使用 git log 的指令查看過去 commit 的紀錄，例如 commit 的版號、作者等等。</p>\n\n<pre><code>$ git log\n</code></pre>\n\n<h3 id=\"gitigore\">.gitigore</h3>\n\n<p>log 檔及 build 出來的檔案及系統產生的檔案如 .DS_Store 等等，我們並不需要 commit 上去 Repository，所以我們會在 Repository 編寫一個 .gitignore 文字檔來忽略這些檔案。</p>\n\n<p>範例 .gitigore 如下：</p>\n\n<pre><code>.DS_Store\n*.log\n</code></pre>\n\n<h3 id=\"\">結語</h3>\n\n<p>以上是一些 Git 指令的簡易使用教學，但我平常用還是用 GUI 比較多，個人推薦 Github 出品的 GUI 工具，工具只要簡單易用就好了，不太需要什麼複雜的功能啊。</p>\n\n<p>Github GUI Client 下載：</p>\n\n<ul>\n<li><a href=\"https://windows.github.com/\">GitHub for Windows</a></li>\n<li><a href=\"https://mac.github.com/\">GitHub for Mac</a></li>\n</ul>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-04-24T12:19:31.000Z","created_by":1,"updated_at":"2014-04-24T13:50:49.000Z","updated_by":1,"published_at":"2014-04-24T13:48:47.000Z","published_by":1},{"id":23,"uuid":"f3a846c3-2cb6-4f89-bc2c-6a42f9b8991e","title":"Git Branch 簡易教學","slug":"git-branch-jian-yi-jiao-xue","markdown":"### 前言\n\n多人共同開發的專案，有時我們需要開發新功能，同時又要修 Bug，可能主程式也要不斷維護開發，我們需要同步進行以加速開發，這時我們通常會從主 branch（通常預設為 master）開出一個新的 branch 來開發，當完成所要開發的新功能或完成 bug 的修正時，就可以將這個 branch merge 回主 branch，因此使用 git branch 在軟體開發上是非常重要的技能。\n\n### git branch\n\n使用 git branch 可以列出所有的 branch 並告訴你目前正在哪個 branch：\n\n\t$ git branch\n      dev\n    * master\n    \n假設要再開一個 bug-fix 的 branch，就可以使用以下指令來開 branch：\n\n\t$ git branch bug-fix\n    \n若要刪除 branch 則使用 git branch -d 來刪除，-D 則為強制刪除\n\n\t$ git branch -d bug-fix\n    \n### git checkout\n\n目前我們已有多個 branch，我們可以使用 git checkout 來切換 branch：\n\n\t$ git checkout dev\n    \n### git merge\n\n當我們在 branch 完成工作之後，就要將更新的程式碼 merge 回主 branch，這時請先回到主 branch：\n\t\n    $ git checkout master\n    \n然後再使用 git merge 來將要 merge 的 branch merge 進去主 branch，比如將 dev merge 進 master：\n\n\t$ git merge dev\n    \n### conflict 的處理\n\n有時我們 merge 時會產生 conflict，其實如果兩個人同時在相同的 branch 修改相同一行程式碼也會產生 conflict，總之使用版本控制系統應該幾乎都會碰到 conflict，所以處理 conflict 也是相當重要的。\n\n產生 conflict 的時候 <<<<<<<<<< HEAD 到 ========== 的區域是目前你所要 commit 內容，而從 =========== 到 >>>>>>>>>>> dev 則是你要合併的 dev branch 的內容，總之就是將這一段 conflict 修正成你要的結果，然後再將這個修正 commit 上去就可以了。\n\n發生 confict 時的處理步驟大概就是這樣：\n\n1. 找到 confict 的檔案，修改成你要的結果。\n2. 使用 git add . 將處理好的檔案加入 stage。\n3. git commit 提交合併訊息。\n\n###結語\n\n以上是一些 Git Branch 的簡易使用教學，但我平常用還是用 GUI 比較多，個人推薦 Github 出品的 GUI 工具，用 Github 的 Client Merge 超簡單的！工具只要簡單易用就好了，不太需要什麼複雜的功能啊。\n\nGithub GUI Client 下載：\n\n* [GitHub for Windows](https://windows.github.com/)\n* [GitHub for Mac](https://mac.github.com/)\n\n\n ","html":"<h3 id=\"\">前言</h3>\n\n<p>多人共同開發的專案，有時我們需要開發新功能，同時又要修 Bug，可能主程式也要不斷維護開發，我們需要同步進行以加速開發，這時我們通常會從主 branch（通常預設為 master）開出一個新的 branch 來開發，當完成所要開發的新功能或完成 bug 的修正時，就可以將這個 branch merge 回主 branch，因此使用 git branch 在軟體開發上是非常重要的技能。</p>\n\n<h3 id=\"gitbranch\">git branch</h3>\n\n<p>使用 git branch 可以列出所有的 branch 並告訴你目前正在哪個 branch：</p>\n\n<pre><code>$ git branch\n  dev\n* master\n</code></pre>\n\n<p>假設要再開一個 bug-fix 的 branch，就可以使用以下指令來開 branch：</p>\n\n<pre><code>$ git branch bug-fix\n</code></pre>\n\n<p>若要刪除 branch 則使用 git branch -d 來刪除，-D 則為強制刪除</p>\n\n<pre><code>$ git branch -d bug-fix\n</code></pre>\n\n<h3 id=\"gitcheckout\">git checkout</h3>\n\n<p>目前我們已有多個 branch，我們可以使用 git checkout 來切換 branch：</p>\n\n<pre><code>$ git checkout dev\n</code></pre>\n\n<h3 id=\"gitmerge\">git merge</h3>\n\n<p>當我們在 branch 完成工作之後，就要將更新的程式碼 merge 回主 branch，這時請先回到主 branch：</p>\n\n<pre><code>$ git checkout master\n</code></pre>\n\n<p>然後再使用 git merge 來將要 merge 的 branch merge 進去主 branch，比如將 dev merge 進 master：</p>\n\n<pre><code>$ git merge dev\n</code></pre>\n\n<h3 id=\"conflict\">conflict 的處理</h3>\n\n<p>有時我們 merge 時會產生 conflict，其實如果兩個人同時在相同的 branch 修改相同一行程式碼也會產生 conflict，總之使用版本控制系統應該幾乎都會碰到 conflict，所以處理 conflict 也是相當重要的。</p>\n\n<p>產生 conflict 的時候 &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD 到 ========== 的區域是目前你所要 commit 內容，而從 =========== 到 >>>>>>>>>>> dev 則是你要合併的 dev branch 的內容，總之就是將這一段 conflict 修正成你要的結果，然後再將這個修正 commit 上去就可以了。</p>\n\n<p>發生 confict 時的處理步驟大概就是這樣：</p>\n\n<ol>\n<li>找到 confict 的檔案，修改成你要的結果。  </li>\n<li>使用 git add . 將處理好的檔案加入 stage。  </li>\n<li>git commit 提交合併訊息。</li>\n</ol>\n\n<h3 id=\"\">結語</h3>\n\n<p>以上是一些 Git Branch 的簡易使用教學，但我平常用還是用 GUI 比較多，個人推薦 Github 出品的 GUI 工具，用 Github 的 Client Merge 超簡單的！工具只要簡單易用就好了，不太需要什麼複雜的功能啊。</p>\n\n<p>Github GUI Client 下載：</p>\n\n<ul>\n<li><a href=\"https://windows.github.com/\">GitHub for Windows</a></li>\n<li><a href=\"https://mac.github.com/\">GitHub for Mac</a></li>\n</ul>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-04-24T14:27:16.000Z","created_by":1,"updated_at":"2014-04-24T14:44:24.000Z","updated_by":1,"published_at":"2014-04-24T14:38:22.000Z","published_by":1},{"id":24,"uuid":"29c0d6a8-f09f-473c-bc35-3b7da413d837","title":"如何建立 Heroku 環境","slug":"jian-li-heroku-huan-jing","markdown":"Heroku 是一個 PaaS 雲端服務，可以讓開發者快速在雲端上放上自己開發的服務，使用 PaaS 服務可以在開發初期省下不少資源，若目前的開發專案沒有使用到 Heroku 沒有支援的功能，使用 Heroku 是一個很好的選擇。\n\n使用 Heroku 所需要建立的環境步驟如下：\n\nStep 1：[註冊 Heroku 帳號](https://id.heroku.com/signup/dc)\n\nStep 2：[安裝 Heroku toolbelt](https://toolbelt.heroku.com/)\n\nStep 3：測試使用 Heroku CLI 登入\n\n\t$ heroku login\n    Enter your Heroku credentials.\n\tEmail: adam@example.com\n\tPassword:\n\tCould not find an existing public key.\n\tWould you like to generate one? [Yn]\n\tGenerating new SSH public key.\n\tUploading ssh public key /Users/adam/.ssh/id_rsa.pub\n    \n如果可以成功，就代表安裝已經完成\n\nStep 4：新增 SSH Key\n\n要 push project 到 Heroku 需要使用 SSH key，如果沒有 SSH key 的話，可以用以下指令新增一個 SSH key\n\n    $ ssh-keygen -t rsa\n    Generating public/private rsa key pair.\n\tEnter file in which to save the key (/Users/adam/.ssh/id_rsa):\n\tEnter passphrase (empty for no passphrase):\n\tEnter same passphrase again:\n\tYour identification has been saved in \t/Users/adam/.ssh/id_rsa.\n\tYour public key has been saved in /Users/adam/.ssh/id_rsa.pub.\n\tThe key fingerprint is:\n\nStep 5：將 SSH Key 新增到 Heroku 帳戸\n\n\t$ heroku keys:add\n    Found existing public key: /Users/adam/.ssh/id_rsa.pub\n\tUploading SSH public key /Users/adam/.ssh/id_rsa.pub... done\n\nStep 6：將現有專案 deploy 到 Heroku\n\n\t$ cd my-project\n\t$ heroku create my-project\n    $ git push heroku master\n    \n如此就可以建立好 Heroku 環境，並且將自己的專案發佈到 Heroku 的雲端環境，對於機器不熟的人，用 Heroku 其實還蠻方便的。\n\n\n","html":"<p>Heroku 是一個 PaaS 雲端服務，可以讓開發者快速在雲端上放上自己開發的服務，使用 PaaS 服務可以在開發初期省下不少資源，若目前的開發專案沒有使用到 Heroku 沒有支援的功能，使用 Heroku 是一個很好的選擇。</p>\n\n<p>使用 Heroku 所需要建立的環境步驟如下：</p>\n\n<p>Step 1：<a href=\"https://id.heroku.com/signup/dc\">註冊 Heroku 帳號</a></p>\n\n<p>Step 2：<a href=\"https://toolbelt.heroku.com/\">安裝 Heroku toolbelt</a></p>\n\n<p>Step 3：測試使用 Heroku CLI 登入</p>\n\n<pre><code>$ heroku login\nEnter your Heroku credentials.\nEmail: adam@example.com\nPassword:\nCould not find an existing public key.\nWould you like to generate one? [Yn]\nGenerating new SSH public key.\nUploading ssh public key /Users/adam/.ssh/id_rsa.pub\n</code></pre>\n\n<p>如果可以成功，就代表安裝已經完成</p>\n\n<p>Step 4：新增 SSH Key</p>\n\n<p>要 push project 到 Heroku 需要使用 SSH key，如果沒有 SSH key 的話，可以用以下指令新增一個 SSH key</p>\n\n<pre><code>$ ssh-keygen -t rsa\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/Users/adam/.ssh/id_rsa):\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in   /Users/adam/.ssh/id_rsa.\nYour public key has been saved in /Users/adam/.ssh/id_rsa.pub.\nThe key fingerprint is:\n</code></pre>\n\n<p>Step 5：將 SSH Key 新增到 Heroku 帳戸</p>\n\n<pre><code>$ heroku keys:add\nFound existing public key: /Users/adam/.ssh/id_rsa.pub\nUploading SSH public key /Users/adam/.ssh/id_rsa.pub... done\n</code></pre>\n\n<p>Step 6：將現有專案 deploy 到 Heroku</p>\n\n<pre><code>$ cd my-project\n$ heroku create my-project\n$ git push heroku master\n</code></pre>\n\n<p>如此就可以建立好 Heroku 環境，並且將自己的專案發佈到 Heroku 的雲端環境，對於機器不熟的人，用 Heroku 其實還蠻方便的。</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-04-30T11:50:14.000Z","created_by":1,"updated_at":"2014-04-30T12:09:30.000Z","updated_by":1,"published_at":"2014-04-30T12:04:05.000Z","published_by":1},{"id":25,"uuid":"2e731f66-e687-4645-906b-6b2945cc5bdb","title":"如何將 Laravel 專案發佈到 Heroku","slug":"ru-he-jiang-laravel-zhuan-an-fa-bu-dao-heroku","markdown":"目前最火紅的 PHP Framework 就是 [Laravel](http://laravel.com/) 了，以往 PHP Framework 的缺點就是沒有一個熱絡的社群，Laravel 的出現漸漸改變了這樣的現象，Laravel 社群比起其他 PHP Framework 的社群熱絡多了，雖然比起 Rails、Django 及 Node.js 確實還是差了一大截，但總是好現象。\n\n由於看好它的發展性，目前有一些 Side Project 是用 Laravel 來實作，這些 Side Project 如果沒有必要實在是不太想碰機器或者安裝環境，所以發佈到 Heroku 是最好的選擇，只要發佈到 Heroku，Heroku 就會幫忙安裝好所有需要的環境。\n\n步驟如下：\n\nStep 1：使用 CLI 在 Heroku 上開啟一個可以 Build Laravel 的專案\n\n    $ heroku create my-laravel-project --buildpack https://github.com/winglian/heroku-buildpack-php\n    \nStep 2：在 Laravel 的 Project 資料夾內發佈專案到 Heroku\n\n    $ git push heroku master\n    \n有時 Heroku 發佈專案會失敗，基本上發佈失敗只要再下一次指令就可能會成功，我也不知道為何有時會這樣，或許是因為是用 Laravel 的關係吧！總之，一次不成功，那就試第二次就對了！\n\n其實還蠻簡單的，還不用管機器，真的很方便～","html":"<p>目前最火紅的 PHP Framework 就是 <a href=\"http://laravel.com/\">Laravel</a> 了，以往 PHP Framework 的缺點就是沒有一個熱絡的社群，Laravel 的出現漸漸改變了這樣的現象，Laravel 社群比起其他 PHP Framework 的社群熱絡多了，雖然比起 Rails、Django 及 Node.js 確實還是差了一大截，但總是好現象。</p>\n\n<p>由於看好它的發展性，目前有一些 Side Project 是用 Laravel 來實作，這些 Side Project 如果沒有必要實在是不太想碰機器或者安裝環境，所以發佈到 Heroku 是最好的選擇，只要發佈到 Heroku，Heroku 就會幫忙安裝好所有需要的環境。</p>\n\n<p>步驟如下：</p>\n\n<p>Step 1：使用 CLI 在 Heroku 上開啟一個可以 Build Laravel 的專案</p>\n\n<pre><code>$ heroku create my-laravel-project --buildpack https://github.com/winglian/heroku-buildpack-php\n</code></pre>\n\n<p>Step 2：在 Laravel 的 Project 資料夾內發佈專案到 Heroku</p>\n\n<pre><code>$ git push heroku master\n</code></pre>\n\n<p>有時 Heroku 發佈專案會失敗，基本上發佈失敗只要再下一次指令就可能會成功，我也不知道為何有時會這樣，或許是因為是用 Laravel 的關係吧！總之，一次不成功，那就試第二次就對了！</p>\n\n<p>其實還蠻簡單的，還不用管機器，真的很方便～</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-04-30T12:28:21.000Z","created_by":1,"updated_at":"2014-04-30T12:28:54.000Z","updated_by":1,"published_at":"2014-04-30T12:28:21.000Z","published_by":1},{"id":26,"uuid":"3c1c7f40-b454-4d9b-9a18-9488579a9183","title":"如何在 Heroku 上建立開發環境及正式環境","slug":"ru-he-zai-heroku-shang-jian-li-kai-fa-huan-jing-ji-zheng-shi-huan-jing","markdown":"在開發網站的時候，我們至少會有一個正在開發中的開發環境，即真正上線開放給使用者使用的正式環境，通常我們會使用類似 git 的版本控制系統開一個 dev branch 及 ㄧ個 master branch，分別對應到開發環境及正式環境。\n\n所以在 Heroku 上，我們也會希望開一個給使用者使用的 Heroku app（正式環境），另一個就是線上開發版的 Heroku app（開發環境），開發者可能在自己的 local 端開發完之後，將自己的開發的成果 merge 到 dev branch，我們就可以看在線上開發版的運作情況，沒問題了我們才會發佈到正式環境。\n\n首先我們需要先 create 兩個 Heroku App，這邊我們以 Laravel 專案為例：\n\nStep 1：新增 Heroku App\n\n    $ heroku create my-laravel-project --buildpack \n    // 新增正式環境 Heroku Laravel App\n    $ heroku create dev-my-laravel-project --buildpack \n    // 新增開發環境 Heroku Laravel App\n    \nStep 2：將 dev branch 專案發佈到開發環境\n\t\n    $ git config remote.heroku.url \"git@heroku.com:dev-my-laravel-project.git\"\n    $ git push -f heroku dev:master\n    \nStep 3：將 merge 好的 master branch 專案發佈到正式環境\n\n    $ git config remote.heroku.url \"git@heroku.com:my-laravel-project.git\"\n    $ git push heroku master\n    \n如此就可以視情況將 dev branch 發佈到 Heroku 開發環境、master branch 發佈到 Heroku 正式環境，這樣就不會讓正在開發的功能影響到獻上的使用者了。","html":"<p>在開發網站的時候，我們至少會有一個正在開發中的開發環境，即真正上線開放給使用者使用的正式環境，通常我們會使用類似 git 的版本控制系統開一個 dev branch 及 ㄧ個 master branch，分別對應到開發環境及正式環境。</p>\n\n<p>所以在 Heroku 上，我們也會希望開一個給使用者使用的 Heroku app（正式環境），另一個就是線上開發版的 Heroku app（開發環境），開發者可能在自己的 local 端開發完之後，將自己的開發的成果 merge 到 dev branch，我們就可以看在線上開發版的運作情況，沒問題了我們才會發佈到正式環境。</p>\n\n<p>首先我們需要先 create 兩個 Heroku App，這邊我們以 Laravel 專案為例：</p>\n\n<p>Step 1：新增 Heroku App</p>\n\n<pre><code>$ heroku create my-laravel-project --buildpack \n// 新增正式環境 Heroku Laravel App\n$ heroku create dev-my-laravel-project --buildpack \n// 新增開發環境 Heroku Laravel App\n</code></pre>\n\n<p>Step 2：將 dev branch 專案發佈到開發環境</p>\n\n<pre><code>$ git config remote.heroku.url \"git@heroku.com:dev-my-laravel-project.git\"\n$ git push -f heroku dev:master\n</code></pre>\n\n<p>Step 3：將 merge 好的 master branch 專案發佈到正式環境</p>\n\n<pre><code>$ git config remote.heroku.url \"git@heroku.com:my-laravel-project.git\"\n$ git push heroku master\n</code></pre>\n\n<p>如此就可以視情況將 dev branch 發佈到 Heroku 開發環境、master branch 發佈到 Heroku 正式環境，這樣就不會讓正在開發的功能影響到獻上的使用者了。</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-04-30T12:41:15.000Z","created_by":1,"updated_at":"2014-04-30T12:45:33.000Z","updated_by":1,"published_at":"2014-04-30T12:45:33.000Z","published_by":1},{"id":27,"uuid":"7be64ec3-e064-48d0-820f-71e10a66054c","title":"送行者：禮儀師的樂章電影一幕","slug":"song-xing-zhe-li-yi-shi-de-le-zhang-dian-ying-mu","markdown":"<p style=\"text-align:center\">\n<img src=\"http://static.obeobe.com/image/blog-image/送行者 (Departures) ：禮儀師的樂章(2009)一幕.png\">\n</p>\n\n送行者是我蠻喜歡的一部電影，但若要我詳細描述這部電影，或是告訴你我從這部電影得到了什麼體悟，我一時也難以說明；看電影就是一種感覺嘛！我就是喜歡這部電影啊！說出來就不好玩了嘛！所以我會建議直接去看電影，親自感受！\n\n看這部電影時，有好多地方會讓人忍不住想流淚，會將電影中的情景對應到自身為親人送行的情景，總覺得台灣傳統的喪禮無法讓人能好好地從悲傷中平復，人們總是在喪禮之後才隨著時間忘記傷痛。\n\n看這部電影某種程度是在自我療癒。\n\n對這部電影印象最深的一幕就是葬儀社員工在聖誕節時一起吃著炸雞喝著酒的那一幕，這幕的時間非常長，而且還一直拍特寫，每個人都不顧形象的張口大吃，大家一口接著一口地吃得津津有味，狼吞虎嚥的聲音一直「咕嚕」「咕嚕」的震天作響，看著看著不知不覺自己也會開始流起口水想吃炸雞，正當要幹譙導演亂拍的時候，忽然開始對這一幕深深著迷，霎那間讓人在心裡呼喊：「啊！原來這就是活著的感覺」\n\n電影中有蠻多地方會用類似這種生與死的象徵，讓人感覺死去與活著的區別，但這一幕是最長的，看完這段時讓我一時陷入沈思，或許是對生命如此卑微而感嘆，抑或是對自己也還能夠這樣大口吃炸雞感到慶幸！<strong>我沒想到狼吞虎嚥的「咕嚕」「咕嚕」聲也能這樣帶給我衝擊啊～</strong>\n\n<p style=\"text-align:center\">\n<img src=\"http://static.obeobe.com/image/subtitle-image/越快升天就越應該要拼命吃.jpg\" alt=\"ice dog\">\n</p>\n\n難怪星爺的電影裡會說「越快升天就越應該要拼命吃」了 XD\n\n","html":"<p style=\"text-align:center\">  \n<img src=\"http://static.obeobe.com/image/blog-image/送行者 (Departures) ：禮儀師的樂章(2009)一幕.png\">  \n</p>\n\n<p>送行者是我蠻喜歡的一部電影，但若要我詳細描述這部電影，或是告訴你我從這部電影得到了什麼體悟，我一時也難以說明；看電影就是一種感覺嘛！我就是喜歡這部電影啊！說出來就不好玩了嘛！所以我會建議直接去看電影，親自感受！</p>\n\n<p>看這部電影時，有好多地方會讓人忍不住想流淚，會將電影中的情景對應到自身為親人送行的情景，總覺得台灣傳統的喪禮無法讓人能好好地從悲傷中平復，人們總是在喪禮之後才隨著時間忘記傷痛。</p>\n\n<p>看這部電影某種程度是在自我療癒。</p>\n\n<p>對這部電影印象最深的一幕就是葬儀社員工在聖誕節時一起吃著炸雞喝著酒的那一幕，這幕的時間非常長，而且還一直拍特寫，每個人都不顧形象的張口大吃，大家一口接著一口地吃得津津有味，狼吞虎嚥的聲音一直「咕嚕」「咕嚕」的震天作響，看著看著不知不覺自己也會開始流起口水想吃炸雞，正當要幹譙導演亂拍的時候，忽然開始對這一幕深深著迷，霎那間讓人在心裡呼喊：「啊！原來這就是活著的感覺」</p>\n\n<p>電影中有蠻多地方會用類似這種生與死的象徵，讓人感覺死去與活著的區別，但這一幕是最長的，看完這段時讓我一時陷入沈思，或許是對生命如此卑微而感嘆，抑或是對自己也還能夠這樣大口吃炸雞感到慶幸！<strong>我沒想到狼吞虎嚥的「咕嚕」「咕嚕」聲也能這樣帶給我衝擊啊～</strong></p>\n\n<p style=\"text-align:center\">  \n<img src=\"http://static.obeobe.com/image/subtitle-image/越快升天就越應該要拼命吃.jpg\" alt=\"ice dog\">  \n</p>\n\n<p>難怪星爺的電影裡會說「越快升天就越應該要拼命吃」了 XD</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-06-05T12:29:40.000Z","created_by":1,"updated_at":"2014-07-23T10:59:18.000Z","updated_by":1,"published_at":"2014-06-05T13:32:44.000Z","published_by":1},{"id":28,"uuid":"5b63ef5e-4fa8-4346-9499-7b415eb76056","title":"如何使用 skrollr 做出簡易的 Parallax Scrolling 網頁","slug":"ru-he-shi-yong-skrollr-zuo-chu-jian-yi-de-parallax-scrolling-wang-ye","markdown":"前一陣子大家非常瘋 Parallax Scrolling 網頁，主要是利用人們瀏覽網頁時最習慣的動作「滾動」來做一些特效，讓使用者透過簡易的滾動就能瀏覽完整個網頁。\n\nParallax Scrolling 中文翻成視差滾動，wiki 上的定義上說明這是電腦圖學中一種特別的滑動特效技巧，原理是把背景圖片的移動速度放慢，讓前景圖片移動較快，因而在2D畫面上產生多層次的佈景深度。\n\n但說了這麼多，不如還是透過實際的例子來感受一下什麼是 Parallax Scrolling，<a href=\"http://www.awwwards.com/20-best-websites-with-parallax-scrolling-of-2013.html\" target=\"_blank\" >20 Best Websites with Parallax Scrolling of 2013</a> 中就有許多有趣的例子！\n\n如果仔細去研究這些例子的原始碼就會發現，要做這樣的網頁實在很費工，如果可以的話當然想找找有什麼方法可以快速的做好一個 Parallax Scrolling 網頁，於是就在 <a href=\"https://github.com/\" target=\"_blank\" >github</a> 上找到了 <a href=\"https://github.com/Prinzhorn/skrollr\" target=\"_blank\" >skrollr</a>。\n\nskrollr 的使用方法真的非常簡單，步驟如下：\n\nStep1：引入函式庫到網頁中：\n\n    <script type=\"text/javascript\" src=\"/path/to/skrollr.min.js\">\n    </script>\n    \nStep2：在網頁底端初始化 skrollr\n\n\t<script type=\"text/javascript\">\n    \tvar skrollr_obj = skrollr.init();\n    </script>\n    \nStep3：利用以下語法來安排網頁中元素的轉場\n\n\t<div data-0=\"background-color:rgb(0,0,255);\" data-500=\"background-color:rgb(255,0,0);\">\n    \tWOOOT\n    </div>\n    \n上面語法的意義就是使用者滾動位置從 0 滾動到 500 時 div 的 CSS 樣式變化，也就是說使用者滾動位置到 500 時，div 的背景色會從原來的藍色慢慢變為紅色。\n\n如果要做淡出功能可以這樣寫：\n\n\t<div data-2100=\"opacity: 1;\" data-2400=\"opacity: 0;\">\n    \t<img src=\"/public/image/show-case/skrollr-demo/cell.png\">\n    </div>\n\n如果要做漸漸模糊功能可以這樣寫：\n\n\t<div data-2100=\"-webkit-filter: blur(0px);\" data-2400=\"-webkit-filter: blur(5px);\">\n    \t<img src=\"/public/image/show-case/skrollr-demo/cell.png\">\n    </div>\n\n以此類推，當然我們也可以寫 data-100 至 data-1000 的 CSS 樣式變化，因此我們可以很方便的安排網頁中元素的轉場，做出簡易的 Parallax Scrolling 網頁。\n\n這麼簡易好用，應該所有剛接觸網頁設計的人都會用吧！所以我就現學現賣做了一個 <a href=\"http://www.fukuball.com/show-case/skrollr-demo\" target=\"_blank\" >天下第一武道大會</a> 網頁！小時候的回憶湧上心頭啊！\n\n### 會踩雷的地方請特別注意\n\n撰寫 CSS 轉場變化時要特別注意，每一個階段的 CSS 樣式一定要一致，舉個例子來說，當某個階段的 CSS 樣式有 <code>opacity: 0;</code> 及 <code>-webkit-filter: blur(0px);</code>，其他階段的 CSS 樣式也一定要有 <code>opacity: *;</code> 及 <code>-webkit-filter: blur(*px);</code>，否則會得到非預期的結果。\n\n例如這樣是錯的寫法（在 data-2400 少寫了 opacity 的樣式）：\n\n\t<div data-2100=\"opacity: 1;-webkit-filter: blur(0px);\" data-2400=\"-webkit-filter: blur(5px);\">\n    \t<img src=\"/public/image/show-case/skrollr-demo/cell.png\">\n    </div>\n    \n這樣才是正確的寫法（data-2100 有寫到 opacity, -webkit-filter，data-2400 就要寫到 opacity, -webkit-filter）：\n\n\t<div data-2100=\"opacity: 1;-webkit-filter: blur(0px);\" data-2400=\"opacity: 0;-webkit-filter: blur(5px);\">\n    \t<img src=\"/public/image/show-case/skrollr-demo/cell.png\">\n    </div>\n\n\n\n","html":"<p>前一陣子大家非常瘋 Parallax Scrolling 網頁，主要是利用人們瀏覽網頁時最習慣的動作「滾動」來做一些特效，讓使用者透過簡易的滾動就能瀏覽完整個網頁。</p>\n\n<p>Parallax Scrolling 中文翻成視差滾動，wiki 上的定義上說明這是電腦圖學中一種特別的滑動特效技巧，原理是把背景圖片的移動速度放慢，讓前景圖片移動較快，因而在2D畫面上產生多層次的佈景深度。</p>\n\n<p>但說了這麼多，不如還是透過實際的例子來感受一下什麼是 Parallax Scrolling，<a href=\"http://www.awwwards.com/20-best-websites-with-parallax-scrolling-of-2013.html\" target=\"_blank\" >20 Best Websites with Parallax Scrolling of 2013</a> 中就有許多有趣的例子！</p>\n\n<p>如果仔細去研究這些例子的原始碼就會發現，要做這樣的網頁實在很費工，如果可以的話當然想找找有什麼方法可以快速的做好一個 Parallax Scrolling 網頁，於是就在 <a href=\"https://github.com/\" target=\"_blank\" >github</a> 上找到了 <a href=\"https://github.com/Prinzhorn/skrollr\" target=\"_blank\" >skrollr</a>。</p>\n\n<p>skrollr 的使用方法真的非常簡單，步驟如下：</p>\n\n<p>Step1：引入函式庫到網頁中：</p>\n\n<pre><code>&lt;script type=\"text/javascript\" src=\"/path/to/skrollr.min.js\"&gt;\n&lt;/script&gt;\n</code></pre>\n\n<p>Step2：在網頁底端初始化 skrollr</p>\n\n<pre><code>&lt;script type=\"text/javascript\"&gt;\n    var skrollr_obj = skrollr.init();\n&lt;/script&gt;\n</code></pre>\n\n<p>Step3：利用以下語法來安排網頁中元素的轉場</p>\n\n<pre><code>&lt;div data-0=\"background-color:rgb(0,0,255);\" data-500=\"background-color:rgb(255,0,0);\"&gt;\n    WOOOT\n&lt;/div&gt;\n</code></pre>\n\n<p>上面語法的意義就是使用者滾動位置從 0 滾動到 500 時 div 的 CSS 樣式變化，也就是說使用者滾動位置到 500 時，div 的背景色會從原來的藍色慢慢變為紅色。</p>\n\n<p>如果要做淡出功能可以這樣寫：</p>\n\n<pre><code>&lt;div data-2100=\"opacity: 1;\" data-2400=\"opacity: 0;\"&gt;\n    &lt;img src=\"/public/image/show-case/skrollr-demo/cell.png\"&gt;\n&lt;/div&gt;\n</code></pre>\n\n<p>如果要做漸漸模糊功能可以這樣寫：</p>\n\n<pre><code>&lt;div data-2100=\"-webkit-filter: blur(0px);\" data-2400=\"-webkit-filter: blur(5px);\"&gt;\n    &lt;img src=\"/public/image/show-case/skrollr-demo/cell.png\"&gt;\n&lt;/div&gt;\n</code></pre>\n\n<p>以此類推，當然我們也可以寫 data-100 至 data-1000 的 CSS 樣式變化，因此我們可以很方便的安排網頁中元素的轉場，做出簡易的 Parallax Scrolling 網頁。</p>\n\n<p>這麼簡易好用，應該所有剛接觸網頁設計的人都會用吧！所以我就現學現賣做了一個 <a href=\"http://www.fukuball.com/show-case/skrollr-demo\" target=\"_blank\" >天下第一武道大會</a> 網頁！小時候的回憶湧上心頭啊！</p>\n\n<h3 id=\"\">會踩雷的地方請特別注意</h3>\n\n<p>撰寫 CSS 轉場變化時要特別注意，每一個階段的 CSS 樣式一定要一致，舉個例子來說，當某個階段的 CSS 樣式有 <code>opacity: 0;</code> 及 <code>-webkit-filter: blur(0px);</code>，其他階段的 CSS 樣式也一定要有 <code>opacity: <em>;</code> 及 <code>-webkit-filter: blur(</em>px);</code>，否則會得到非預期的結果。</p>\n\n<p>例如這樣是錯的寫法（在 data-2400 少寫了 opacity 的樣式）：</p>\n\n<pre><code>&lt;div data-2100=\"opacity: 1;-webkit-filter: blur(0px);\" data-2400=\"-webkit-filter: blur(5px);\"&gt;\n    &lt;img src=\"/public/image/show-case/skrollr-demo/cell.png\"&gt;\n&lt;/div&gt;\n</code></pre>\n\n<p>這樣才是正確的寫法（data-2100 有寫到 opacity, -webkit-filter，data-2400 就要寫到 opacity, -webkit-filter）：</p>\n\n<pre><code>&lt;div data-2100=\"opacity: 1;-webkit-filter: blur(0px);\" data-2400=\"opacity: 0;-webkit-filter: blur(5px);\"&gt;\n    &lt;img src=\"/public/image/show-case/skrollr-demo/cell.png\"&gt;\n&lt;/div&gt;\n</code></pre>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-06-11T13:19:43.000Z","created_by":1,"updated_at":"2014-06-11T13:49:14.000Z","updated_by":1,"published_at":"2014-06-11T13:45:51.000Z","published_by":1},{"id":29,"uuid":"e2549d58-c7c0-4db8-8983-86125650bf97","title":"如何做出網頁版 iTunes 11 的封面背景特效","slug":"ru-he-shi-yong-html5-xiang-guan-ji-shu-zuo-chu-itunes-11-de-feng-mian-bei-jing-te-xiao","markdown":"### 前言\n\n最近為了 training 新人，我在 iNDIEVOX 主持了每兩週會舉行一次的技術分享會，每位與會人員都需要準備一個分享講題，講題可以是任何開發技術相關的主題，甚至是相關產業新聞。\n\n不過最近有些忙碌，加上世界杯熬夜的影響，我自己得準備的分享講題眼看就要開天窗了，只好拿出去年在 [HappyDesigner Mini 分享會 #3](http://happydesigner.kktix.cc/events/happydesigner-mini-3) 的講題來充充場面，剛好也可以逼自己整理成 blog！\n\n那麼...... 究竟要如何做出網頁版 iTunes 11 的封面背景特效呢？在哪裡才能買得到呢？\n\n<p style=\"text-align:center\">\n<img src=\"http://static.obeobe.com/image/subtitle-image/那麼在哪裡才能買得到呢？.jpg\">\n</p>\n\n... ... 我們這邊會直接說明如何實作，所以是買不到的喔！\n\n### 觀察分析\n\n首先讓我們觀察分析一下 iTunes 11 的封面特效：\n\n<p style=\"text-align:center\">\n<img src=\"http://static.obeobe.com/image/blog-image/itunes11.png?1\">\n</p>\n\n透過我們精準的觀察與分析之後，我們可以將 iTunes 11 的封面特效歸納成以下三點：\n\n1. 專輯資訊字體顏色配色與專輯封面配色相似\n2. 專輯資訊區塊背景色與專輯封面背景色相似\n3. 專輯資訊區塊背景色形成一個模糊遮罩蓋在專輯封面上\n\n也就是說我們只要做到以上三點，大概就可以做到類似 iTunes 11 的封面特效了！\n\n### 實作\n\n### Step 1：實作蓋在專輯封面上模糊遮罩\n\n首先我們先不管顏色，看看是否能用 CSS 3 來實作蓋在專輯封面上模糊遮罩，研究一番之後發現並不難實作，CSS 3 的語法如下：\n\n\t.mask {\n    \tbox-shadow: rgb(9, 8, 9) 14px 17px 25px inset,\n                    rgb(9, 8, 9) -1px -1px 170px inset;\n\t}\n    \n我們使用 CSS 3 中的 <code>box-shadow</code> 這個 property 來實做陰影效果，將這個陰影蓋在封面上便可以形成一個模糊遮罩，其中 <code>rgb(9, 8, 8)</code> 表示陰影的顏色，<code>inset</code> 表示陰影是往區塊內部長，<code>14px 17px 25px</code> 分別表示陰影向水平方向長的長度、陰影向垂直方向長的長度、陰影模糊的長度，由於陰影是向區塊內長，分別就表示水平方向<strong>由左向內</strong>長 14px 的陰影、垂直方向<strong>由上向內</strong>長 17px、然後都長 25px 的模糊程度。\n\n同理類推，底下的 <code>rgb(9, 8, 9) -1px -1px 170px inset</code> 就表示水平方向<strong>由右向內</strong>長 1px 的陰影、垂直方向<strong>由下向內</strong>長 1px、然後都長 170px 的模糊程度。\n\n這個語法會形成像這樣的效果：\n\n<p style=\"text-align:center\">\n<img src=\"http://static.obeobe.com/image/blog-image/itunes11_2.png?1\">\n</p>\n\n太棒了！小小的一段 CSS 3 語法居然帶給我們這麼大的成就感，聰明的我們把這個遮罩蓋在封面像就會形成這樣的效果：\n\n<p style=\"text-align:center\">\n<img src=\"http://static.obeobe.com/image/blog-image/itunes11_3.png?1\">\n</p>\n\n如此我們就可以暫時使用黑色背景、白色字體、黑色遮罩來完成第一版的 iTunes 11 封面背景效果。\n\n### <a href=\"http://www.fukuball.com/show-case/5tunes11-v1\" target=\"_blank\">第一版成品展示</a>\n\n<p style=\"text-align:center\">\n<img src=\"http://static.obeobe.com/image/blog-image/itunes11_4.png?1\">\n</p>\n\n### Step 2：萃取出專輯封面主要顏色，讓背景色、字體顏色、模糊遮罩配色與專輯封面配色相似\n\n實作完模糊遮罩之後，剛剛第一步我們先擱在一邊的顏色就是接下來的重點了，所謂「萃取出專輯封面主要顏色」究竟是怎麼一回事呢？感覺好像需要用到演算法？沒有錯！要取出專輯封面的<strong>主要顏色</strong>確實需要借助演算法這個高深的技巧！\n\n我們可以以人類最直觀的想法來猜猜演算法應該會怎麼實作：首先每張圖片都是由若干像素組合而成，而每個像素都有一個顏色，我們可以用數數的方式找出最多相同顏色的像素取出成為一個代表色，若要找出多個代表色，那就可以用同的想法類推，讓顏色接近的像素聚集在一起，分成一堆一堆的，若要取十個顏色，就分成十堆，讓每一堆的像素數都相同，然後將這十個顏色堆的顏色取平均色，我們就可以萃取出圖片的十個代表色了！\n\n類似想法的演算法其實已經有許多人想出來、也實作好程式了，其中一個最有名的演算法就叫做 [Color Quantization](http://en.wikipedia.org/wiki/Color_quantization)，它的核心精神就像我們上述的做法一樣把圖片的顏色像<strong>切豆腐</strong>一樣去分堆，去找出代表色，各種不同的演算法其實就只是在於<strong>切法</strong>的不同而已！\n\n其實這樣的演算法最早是用來壓縮圖像使用的，我們希望儘量用較少的顏色來重畫原來的圖片，又不希望重畫的圖片與原來的圖片差太多，因此就要<strong>找出圖片的主要代表色來重畫原圖</strong>，這樣重畫的壓縮圖片就不會與原來的圖片差太多，又能達到壓縮效果！\n\n詳細演算法可以寫成好幾篇部落格，因此我就不在這邊贅述，有興趣詳細了解的可以參考[這個網站](http://www.csie.ntnu.edu.tw/~u91029/Image.html)！\n\n我們現在已經知道要取出圖片的主要代表色就要使用 Color Quantization 演算法，那接下來要自己實做演算法嗎？哈哈哈！當然不必啊！像這種有名的演算法一定有一大堆已經寫好的開源碼可以使用！\n\n這邊我們使用 [Color Thief](https://github.com/lokesh/color-thief) 來取出圖片的主要代表色，首先引入函式庫到網頁中：\n\n    <script type=\"text/javascript\" src=\"/path/to/color-thief.min.js\">\n    </script>\n    \n然後用以下語法來得到圖片的主要代表色：\n\n\tmyImage = $('#myImage');\n    var colorThief = new ColorThief();\n\tcolorThief.getPalette(myImage, 8);\n    \n其中的 <code>8</code> 就是表示要取出圖片八個主要代表色，同理你要取出十個主要代表色，就改成 <code>colorThief.getPalette(myImage, 10)</code> 就可以了！很簡單吧！！\n\n如此我們就可以使用 Color Thief 取出的第一個主要顏色來畫背景及遮罩、第二個主要顏色畫字體完成第二版的 iTunes 11 封面背景效果，這樣的效果就已經很不錯了！\n\n### <a href=\"http://www.fukuball.com/show-case/5tunes11-v2\" target=\"_blank\">第二版成品展示</a>\n\n<p style=\"text-align:center\">\n<img src=\"http://static.obeobe.com/image/blog-image/itunes11_5.png?2\">\n</p>\n\n### Step 3：調整、調整、再調整\n\n從第二版的 iTunes 11 封面背景效果展示中，其實我們已經得到了不錯的效果，但我們會發現有些地方會怪怪的，比如：\n\n<p style=\"text-align:center\">\n<img src=\"http://static.obeobe.com/image/blog-image/itunes11_6.png?2\">\n</p>\n\n這個封面中，Color Thief 取出的第一個代表色是灰色，所以造成了我們不想要的結果，但其實又不能說是 Color Thief 或是 Color Quantization 演算法的錯，因為這張封面確實有蠻多像素是屬於灰色系列的。\n\n我們認為較好的結果就是讓專輯資訊區塊的背景色及遮罩能和封面的背景色是類似的，在這個例子裡我們希望得到的是<strong>米分糸工色</strong>的！我們開怎麼辦呢？\n\n我們可以做的就是去調整演算法，讓 Color Thief 取出代表色時只算封面中屬於背景部份的像素！感覺很直觀嘛！\n\n問題在於...\n\n### 請翻譯翻譯他媽的要怎麼告訴程式只算封面背景部份的像素啊啊啊啊！！！！\n\n<p style=\"text-align:center\">\n<img src=\"http://static.obeobe.com/image/subtitle-image/我就想讓你翻譯翻譯，什麼叫驚喜.jpg\">\n</p>\n\n其實找出圖片中屬於背景的部份這個問題我們並不是第一個遇到的，這個問題也已經探討很久了，目前也有許多方法被研發出來，有很多方法甚至牽涉到[機器學習](http://en.wikipedia.org/wiki/Machine_learning)領域的研究，我們這邊當然也可以用這些方法來解，但我就不引用這些方法了，我們可以利用一個更直觀的方法來完成！\n\n我們可以再觀察一下 iTunes 11 的封面特效，我們可以發現背景色的遮罩效果會與封面的上半部及左半部溶在一起，那我們可不可以動一些手腳來讓 Color Thief 只計算封面上半部及左半部的像素呢？當然可以啊！\n\n<p style=\"text-align:center\">\n<img src=\"http://static.obeobe.com/image/blog-image/itunes11_7.png?2\">\n</p>\n\n所以我們就以這樣的想法對 Color Thief 做了一點 Hack，修改的語法大致如下：\n\n\tfor (var i = 0; i<image_data.length; i=i+4) {\n\t\tr = image_data[i + 0];\n     \tg = image_data[i + 1];\n     \tb = image_data[i + 2];\n     \ta = image_data[i + 3];\n\n     \tif ( i<(image_data.length*0.30) || (i%(image.width*4))<(image.width*0.30) ) {\n        \tbg_image_data_array.push([r, g, b]);\n     \t}\n\n  \t}\n\n  \tvar bg_cmap = MMCQ.quantize(bg_image_data_array, 5);\n  \tvar bg_palette = bg_cmap.palette();\n    \n其中 <code>i<(image_data.length*0.30)</code> 就代表圖片中上面 30% 部分的像素資料，<code>(i%(image.width*4))<(image.width*0.30)</code> 就代表圖片中左邊 30% 部分的像素資料，這樣就可以讓 Color Thief 只計算封面上半部及左半部的像素了！\n\n像這樣的方法在學術上就稱為 Heuristic Algorithm，以最直觀的方式來設計演算法，雖然可能不會得到最佳解，但常常能得到一些不錯的效果！\n\n如此我們就可以使用 Hack 調整過後的 Color Thief 取出的第一個主要顏色來畫背景及遮罩、第二個主要顏色畫字體來完成完全體的 iTunes 11 封面背景效果，感覺一切都對了啊！！！\n\n### <a href=\"http://www.fukuball.com/show-case/5tunes11\" target=\"_blank\">完全體成品展示</a>\n\n<p style=\"text-align:center\">\n<img src=\"http://static.obeobe.com/image/blog-image/itunes11_8.png?2\">\n</p>\n\n### 結語\n\n關於這個主題，我有製作 HappyDesigner Mini 分享會 #3 時使用的投影片，[投影片連結在此](http://www.fukuball.com/slide-show/5tunes11)，有興趣的人可以參考看看。\n\n我們這邊完整說明了如何做出網頁版 iTunes 11 的封面背景特效，使用了許多第三方的函式庫來幫助實作，也可以在這樣的練習中學習如何一步一步的完成我們最終想完成的目標。\n\n其中也發現了許多可以精進的地方，例如取得主要顏色的演算法、機器學習自動偵測圖片背景部份這樣的問題，若要取得更好的結果，就是要更深入去研究相關演算法來得到更令人滿意的結果，而演算法這個領域其實如同偉大航道的新世界一樣充滿驚奇啊！未來有機會更深入研究的話希望也能跟大家分享～\n","html":"<h3 id=\"\">前言</h3>\n\n<p>最近為了 training 新人，我在 iNDIEVOX 主持了每兩週會舉行一次的技術分享會，每位與會人員都需要準備一個分享講題，講題可以是任何開發技術相關的主題，甚至是相關產業新聞。</p>\n\n<p>不過最近有些忙碌，加上世界杯熬夜的影響，我自己得準備的分享講題眼看就要開天窗了，只好拿出去年在 <a href=\"http://happydesigner.kktix.cc/events/happydesigner-mini-3\">HappyDesigner Mini 分享會 #3</a> 的講題來充充場面，剛好也可以逼自己整理成 blog！</p>\n\n<p>那麼...... 究竟要如何做出網頁版 iTunes 11 的封面背景特效呢？在哪裡才能買得到呢？</p>\n\n<p style=\"text-align:center\">  \n<img src=\"http://static.obeobe.com/image/subtitle-image/那麼在哪裡才能買得到呢？.jpg\">  \n</p>\n\n<p>... ... 我們這邊會直接說明如何實作，所以是買不到的喔！</p>\n\n<h3 id=\"\">觀察分析</h3>\n\n<p>首先讓我們觀察分析一下 iTunes 11 的封面特效：</p>\n\n<p style=\"text-align:center\">  \n<img src=\"http://static.obeobe.com/image/blog-image/itunes11.png?1\">  \n</p>\n\n<p>透過我們精準的觀察與分析之後，我們可以將 iTunes 11 的封面特效歸納成以下三點：</p>\n\n<ol>\n<li>專輯資訊字體顏色配色與專輯封面配色相似  </li>\n<li>專輯資訊區塊背景色與專輯封面背景色相似  </li>\n<li>專輯資訊區塊背景色形成一個模糊遮罩蓋在專輯封面上</li>\n</ol>\n\n<p>也就是說我們只要做到以上三點，大概就可以做到類似 iTunes 11 的封面特效了！</p>\n\n<h3 id=\"\">實作</h3>\n\n<h3 id=\"step1\">Step 1：實作蓋在專輯封面上模糊遮罩</h3>\n\n<p>首先我們先不管顏色，看看是否能用 CSS 3 來實作蓋在專輯封面上模糊遮罩，研究一番之後發現並不難實作，CSS 3 的語法如下：</p>\n\n<pre><code>.mask {\n    box-shadow: rgb(9, 8, 9) 14px 17px 25px inset,\n                rgb(9, 8, 9) -1px -1px 170px inset;\n}\n</code></pre>\n\n<p>我們使用 CSS 3 中的 <code>box-shadow</code> 這個 property 來實做陰影效果，將這個陰影蓋在封面上便可以形成一個模糊遮罩，其中 <code>rgb(9, 8, 8)</code> 表示陰影的顏色，<code>inset</code> 表示陰影是往區塊內部長，<code>14px 17px 25px</code> 分別表示陰影向水平方向長的長度、陰影向垂直方向長的長度、陰影模糊的長度，由於陰影是向區塊內長，分別就表示水平方向<strong>由左向內</strong>長 14px 的陰影、垂直方向<strong>由上向內</strong>長 17px、然後都長 25px 的模糊程度。</p>\n\n<p>同理類推，底下的 <code>rgb(9, 8, 9) -1px -1px 170px inset</code> 就表示水平方向<strong>由右向內</strong>長 1px 的陰影、垂直方向<strong>由下向內</strong>長 1px、然後都長 170px 的模糊程度。</p>\n\n<p>這個語法會形成像這樣的效果：</p>\n\n<p style=\"text-align:center\">  \n<img src=\"http://static.obeobe.com/image/blog-image/itunes11_2.png?1\">  \n</p>\n\n<p>太棒了！小小的一段 CSS 3 語法居然帶給我們這麼大的成就感，聰明的我們把這個遮罩蓋在封面像就會形成這樣的效果：</p>\n\n<p style=\"text-align:center\">  \n<img src=\"http://static.obeobe.com/image/blog-image/itunes11_3.png?1\">  \n</p>\n\n<p>如此我們就可以暫時使用黑色背景、白色字體、黑色遮罩來完成第一版的 iTunes 11 封面背景效果。</p>\n\n<h3 id=\"ahrefhttpwwwfukuballcomshowcase5tunes11v1target_blanka\"><a href=\"http://www.fukuball.com/show-case/5tunes11-v1\" target=\"_blank\">第一版成品展示</a></h3>\n\n<p style=\"text-align:center\">  \n<img src=\"http://static.obeobe.com/image/blog-image/itunes11_4.png?1\">  \n</p>\n\n<h3 id=\"step2\">Step 2：萃取出專輯封面主要顏色，讓背景色、字體顏色、模糊遮罩配色與專輯封面配色相似</h3>\n\n<p>實作完模糊遮罩之後，剛剛第一步我們先擱在一邊的顏色就是接下來的重點了，所謂「萃取出專輯封面主要顏色」究竟是怎麼一回事呢？感覺好像需要用到演算法？沒有錯！要取出專輯封面的<strong>主要顏色</strong>確實需要借助演算法這個高深的技巧！</p>\n\n<p>我們可以以人類最直觀的想法來猜猜演算法應該會怎麼實作：首先每張圖片都是由若干像素組合而成，而每個像素都有一個顏色，我們可以用數數的方式找出最多相同顏色的像素取出成為一個代表色，若要找出多個代表色，那就可以用同的想法類推，讓顏色接近的像素聚集在一起，分成一堆一堆的，若要取十個顏色，就分成十堆，讓每一堆的像素數都相同，然後將這十個顏色堆的顏色取平均色，我們就可以萃取出圖片的十個代表色了！</p>\n\n<p>類似想法的演算法其實已經有許多人想出來、也實作好程式了，其中一個最有名的演算法就叫做 <a href=\"http://en.wikipedia.org/wiki/Color_quantization\">Color Quantization</a>，它的核心精神就像我們上述的做法一樣把圖片的顏色像<strong>切豆腐</strong>一樣去分堆，去找出代表色，各種不同的演算法其實就只是在於<strong>切法</strong>的不同而已！</p>\n\n<p>其實這樣的演算法最早是用來壓縮圖像使用的，我們希望儘量用較少的顏色來重畫原來的圖片，又不希望重畫的圖片與原來的圖片差太多，因此就要<strong>找出圖片的主要代表色來重畫原圖</strong>，這樣重畫的壓縮圖片就不會與原來的圖片差太多，又能達到壓縮效果！</p>\n\n<p>詳細演算法可以寫成好幾篇部落格，因此我就不在這邊贅述，有興趣詳細了解的可以參考<a href=\"http://www.csie.ntnu.edu.tw/~u91029/Image.html\">這個網站</a>！</p>\n\n<p>我們現在已經知道要取出圖片的主要代表色就要使用 Color Quantization 演算法，那接下來要自己實做演算法嗎？哈哈哈！當然不必啊！像這種有名的演算法一定有一大堆已經寫好的開源碼可以使用！</p>\n\n<p>這邊我們使用 <a href=\"https://github.com/lokesh/color-thief\">Color Thief</a> 來取出圖片的主要代表色，首先引入函式庫到網頁中：</p>\n\n<pre><code>&lt;script type=\"text/javascript\" src=\"/path/to/color-thief.min.js\"&gt;\n&lt;/script&gt;\n</code></pre>\n\n<p>然後用以下語法來得到圖片的主要代表色：</p>\n\n<pre><code>myImage = $('#myImage');\nvar colorThief = new ColorThief();\ncolorThief.getPalette(myImage, 8);\n</code></pre>\n\n<p>其中的 <code>8</code> 就是表示要取出圖片八個主要代表色，同理你要取出十個主要代表色，就改成 <code>colorThief.getPalette(myImage, 10)</code> 就可以了！很簡單吧！！</p>\n\n<p>如此我們就可以使用 Color Thief 取出的第一個主要顏色來畫背景及遮罩、第二個主要顏色畫字體完成第二版的 iTunes 11 封面背景效果，這樣的效果就已經很不錯了！</p>\n\n<h3 id=\"ahrefhttpwwwfukuballcomshowcase5tunes11v2target_blanka\"><a href=\"http://www.fukuball.com/show-case/5tunes11-v2\" target=\"_blank\">第二版成品展示</a></h3>\n\n<p style=\"text-align:center\">  \n<img src=\"http://static.obeobe.com/image/blog-image/itunes11_5.png?2\">  \n</p>\n\n<h3 id=\"step3\">Step 3：調整、調整、再調整</h3>\n\n<p>從第二版的 iTunes 11 封面背景效果展示中，其實我們已經得到了不錯的效果，但我們會發現有些地方會怪怪的，比如：</p>\n\n<p style=\"text-align:center\">  \n<img src=\"http://static.obeobe.com/image/blog-image/itunes11_6.png?2\">  \n</p>\n\n<p>這個封面中，Color Thief 取出的第一個代表色是灰色，所以造成了我們不想要的結果，但其實又不能說是 Color Thief 或是 Color Quantization 演算法的錯，因為這張封面確實有蠻多像素是屬於灰色系列的。</p>\n\n<p>我們認為較好的結果就是讓專輯資訊區塊的背景色及遮罩能和封面的背景色是類似的，在這個例子裡我們希望得到的是<strong>米分糸工色</strong>的！我們開怎麼辦呢？</p>\n\n<p>我們可以做的就是去調整演算法，讓 Color Thief 取出代表色時只算封面中屬於背景部份的像素！感覺很直觀嘛！</p>\n\n<p>問題在於...</p>\n\n<h3 id=\"\">請翻譯翻譯他媽的要怎麼告訴程式只算封面背景部份的像素啊啊啊啊！！！！</h3>\n\n<p style=\"text-align:center\">  \n<img src=\"http://static.obeobe.com/image/subtitle-image/我就想讓你翻譯翻譯，什麼叫驚喜.jpg\">  \n</p>\n\n<p>其實找出圖片中屬於背景的部份這個問題我們並不是第一個遇到的，這個問題也已經探討很久了，目前也有許多方法被研發出來，有很多方法甚至牽涉到<a href=\"http://en.wikipedia.org/wiki/Machine_learning\">機器學習</a>領域的研究，我們這邊當然也可以用這些方法來解，但我就不引用這些方法了，我們可以利用一個更直觀的方法來完成！</p>\n\n<p>我們可以再觀察一下 iTunes 11 的封面特效，我們可以發現背景色的遮罩效果會與封面的上半部及左半部溶在一起，那我們可不可以動一些手腳來讓 Color Thief 只計算封面上半部及左半部的像素呢？當然可以啊！</p>\n\n<p style=\"text-align:center\">  \n<img src=\"http://static.obeobe.com/image/blog-image/itunes11_7.png?2\">  \n</p>\n\n<p>所以我們就以這樣的想法對 Color Thief 做了一點 Hack，修改的語法大致如下：</p>\n\n<pre><code>for (var i = 0; i&lt;image_data.length; i=i+4) {\n    r = image_data[i + 0];\n     g = image_data[i + 1];\n     b = image_data[i + 2];\n     a = image_data[i + 3];\n\n     if ( i&lt;(image_data.length*0.30) || (i%(image.width*4))&lt;(image.width*0.30) ) {\n        bg_image_data_array.push([r, g, b]);\n     }\n\n  }\n\n  var bg_cmap = MMCQ.quantize(bg_image_data_array, 5);\n  var bg_palette = bg_cmap.palette();\n</code></pre>\n\n<p>其中 <code>i&lt;(image_data.length<em>0.30)</code> 就代表圖片中上面 30% 部分的像素資料，<code>(i%(image.width</em>4))&lt;(image.width*0.30)</code> 就代表圖片中左邊 30% 部分的像素資料，這樣就可以讓 Color Thief 只計算封面上半部及左半部的像素了！</p>\n\n<p>像這樣的方法在學術上就稱為 Heuristic Algorithm，以最直觀的方式來設計演算法，雖然可能不會得到最佳解，但常常能得到一些不錯的效果！</p>\n\n<p>如此我們就可以使用 Hack 調整過後的 Color Thief 取出的第一個主要顏色來畫背景及遮罩、第二個主要顏色畫字體來完成完全體的 iTunes 11 封面背景效果，感覺一切都對了啊！！！</p>\n\n<h3 id=\"ahrefhttpwwwfukuballcomshowcase5tunes11target_blanka\"><a href=\"http://www.fukuball.com/show-case/5tunes11\" target=\"_blank\">完全體成品展示</a></h3>\n\n<p style=\"text-align:center\">  \n<img src=\"http://static.obeobe.com/image/blog-image/itunes11_8.png?2\">  \n</p>\n\n<h3 id=\"\">結語</h3>\n\n<p>關於這個主題，我有製作 HappyDesigner Mini 分享會 #3 時使用的投影片，<a href=\"http://www.fukuball.com/slide-show/5tunes11\">投影片連結在此</a>，有興趣的人可以參考看看。</p>\n\n<p>我們這邊完整說明了如何做出網頁版 iTunes 11 的封面背景特效，使用了許多第三方的函式庫來幫助實作，也可以在這樣的練習中學習如何一步一步的完成我們最終想完成的目標。</p>\n\n<p>其中也發現了許多可以精進的地方，例如取得主要顏色的演算法、機器學習自動偵測圖片背景部份這樣的問題，若要取得更好的結果，就是要更深入去研究相關演算法來得到更令人滿意的結果，而演算法這個領域其實如同偉大航道的新世界一樣充滿驚奇啊！未來有機會更深入研究的話希望也能跟大家分享～</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-06-25T10:21:33.000Z","created_by":1,"updated_at":"2014-06-25T21:11:15.000Z","updated_by":1,"published_at":"2014-06-25T21:02:44.000Z","published_by":1},{"id":30,"uuid":"53ed81e0-7b52-444f-a19d-11e0f39822bd","title":"如何使用 CSS3 Animation","slug":"rru-he-shi-yong-css3-animation","markdown":"### 前言\n\n在這個浮誇的時代，如果網頁上沒有酷炫的功能或特效，似乎就遜掉了；如何讓網頁變得酷炫呢？其中一個方法就是使用 CSS3 Animation！只要使用了 CSS3 Animation 就可以讓網頁中的元素動起來，立馬讓你的網頁酷炫度超越世界上 80% 的網頁！\n\n而我個人身為一個全端工程師，稍微研究一下 CSS3 Animation 也是合理的，而且只要會一點點就可以拿來唬唬人，何樂而不為？很可惜這個技能就是唬不了正妹，哎，誰叫正妹只喜歡魔術呢～\n\n不過請特別注意 CSS3 的 Animation 與 Transition 並不相同喔！雖然都可以做到相似的效果，有時看起來也真的很像，但實際上 Animation 與 Transition 依操作的程度不同所以適合使用的地方也會有所不同，或許我之後會再寫一篇文章介紹 Transition。（吧？）\n\n就讓我們一起來看看 CSS3 Animation 大法怎麼練吧！\n\n### CSS3 Animation 第一級\n\n假設我們要在一個 div 元素上加上動畫特效：\n\n#### Step 1：定義要使用哪個關鍵影格(keyframe)來執行動畫\n\n\tdiv {\n        width: 100px;\n        height: 100px;\n        background: red;\n        position: relative;\n    \t-webkit-animation: animation-keyframe-name 5s; /* Chrome, Safari, Opera */\n        -webkit-animation-iteration-count: infinite;\n        animation: animation-keyframe-name 5s;\n        animation-iteration-count: infinite;\n    }\n    \n這段 CSS 語法就是代表 div 元素要用哪個關鍵影格來執行動畫，詳細的動畫動作都會寫在關鍵影格裡面，在這邊的意思就是要使用一個名稱叫 <code>animation-keyframe-name</code> 的關鍵影格來進行動畫，後面的 <code>5s</code> 就代表這個動畫執行的時間會是 5 秒鐘。而 <code>animation-iteration-count: infinite</code> 則代表這個關鍵影格動畫會不斷重複執行。\n\n特別注意有 <code>-webkit-</code> 這個前綴的 CSS 語法是為了支援 webkit 核心的瀏覽器，例如：Chrome、Safari、Opera 等等。\n\n所以這段語法翻譯成白話文就是：div 這個元素要使用一個叫做 <code>animation-keyframe-name</code> 的關鍵影格來進行動畫，動畫執行的時間長度為 5 秒鐘，且動畫會不斷重複執行。\n\nkeyframe 的名稱如果不想要叫 <code>animation-keyframe-name</code>，也可以改成其他的，比如：<code>fadein-keyframe</code>、<code>fadeout-keyframe</code> 等等。\n\n#### Step 2：定義關鍵影格(keyframe)中動畫如何變化\n\n\t/* Chrome, Safari, Opera */\n  \t@-webkit-keyframes animation-keyframe-name {\n    \tfrom {background: red;}\n        to {background: yellow;}\n    }\n    \n    /* Standard syntax */\n    @keyframes animation-keyframe-name {\n    \tfrom {background: red;}\n        to {background: yellow;}\n    }\n    \n這段 CSS 語法就定義了 <code>animation-keyframe-name</code> 這個關鍵影格的動畫變化，它會在 5 秒之內從原本背景色紅色（<code>from {background: red;}</code>）變為背景色黃色（<code>to {background: yellow;}</code>），就這樣我們就可以寫好 CSS3 動畫了，超簡單 der！\n\n#### <a href=\"http://codepen.io/fukuball/pen/hzHAE\" target=\"_blank\">CSS3 Animation 第一級展示</a>\n\n### CSS3 Animation 第二級\n\nCSS3 Animation 第一級只有 from to 這兩種狀態可以指定動畫的變化，總感覺缺少了點什麼，如果好死不死動畫想要指定三種狀態變化怎麼辦？這時就要使用 CSS3 Animation 第二級了！在 CSS3 Animation 第二級我們可以使用動畫執行時間百分比來指定動畫變化的狀態。\n\n    /* Chrome, Safari, Opera */\n    @-webkit-keyframes animation-keyframe-name {\n        0%   {background: red; left:0px; top:0px;}\n        25%  {background: yellow; left:200px; top:0px;}\n        50%  {background: blue; left:200px; top:200px;}\n        75%  {background: green; left:0px; top:200px;}\n        100% {background: red; left:0px; top:0px;}\n    }\n\n    /* Standard syntax */\n    @keyframes animation-keyframe-name {\n        0%   {background: red; left:0px; top:0px;}\n        25%  {background: yellow; left:200px; top:0px;}\n        50%  {background: blue; left:200px; top:200px;}\n        75%  {background: green; left:0px; top:200px;}\n        100% {background: red; left:0px; top:0px;}\n    }\n    \n這段 CSS 語法將動畫的變化分成五個狀態，在動畫執行時間 0% 的時候是背景紅色的狀態（<code>0% {background: red;}</code>），在動畫執行時間 25% 的時候是背景黃色的狀態（<code>25% {background: yellow;}</code>），在動畫執行時間 50% 的時候是背景藍色的狀態（<code>50% {background: blue;}</code>），在動畫執行時間 75% 的時候是背景綠色的狀態（<code>75% {background: green;}</code>），在動畫執行時間 100% 的時候是背景變回紅色的狀態（<code>100% {background: red;}</code>），在我們這個例子裡動畫執行時間 100% 也就是動畫執行時間在第五秒的時候。如此我們就可以做更多細緻的動畫了。\n\n#### <a href=\"http://codepen.io/fukuball/pen/lGwnx\" target=\"_blank\">CSS3 Animation 第二級展示</a>\n\n### CSS3 Animation 第三級\n\n在 CSS3 Animation 第三級我們可以使用更多屬性（property）來讓動畫有更多調整的空間，這些 animation 相關的 property 列表如下：\n\n<table>\n\t<thead>\n    \t<tr>\n          \t<td>\n              \tCSS 屬性\n          \t</td>\n          \t<td>\n              \t說明\n          \t</td>\n    \t</tr>\n    </thead>\n    <tbody>\n    \t<tr>\n        \t<td>\n              \tanimation-delay\n          \t</td>\n          \t<td>\n              \t設定元素在被載入之後到開始執行動畫之間的延遲時間\n          \t</td>\n        </tr>\n        <tr>\n        \t<td>\n              \tanimation-direction\n          \t</td>\n          \t<td>\n              \t設定元素在動畫執行完之後，是否要以相反方向的方式播放，或是從頭開始以原來的方向重複播放。\n          \t</td>\n        </tr>\n        <tr>\n        \t<td>\n              \tanimation-duration\n          \t</td>\n          \t<td>\n              \t設定整個動畫執行一次的時間長度\n          \t</td>\n        </tr>\n        <tr>\n        \t<td>\n              \tanimation-iteration-count\n          \t</td>\n          \t<td>\n              \t設定動畫執行的次數，若要不斷重複執行，則可設為 infinite\n          \t</td>\n        </tr>\n        <tr>\n        \t<td>\n              \tanimation-play-state\n          \t</td>\n          \t<td>\n              \t可用來暫停或繼續動畫播放\n          \t</td>\n        </tr>\n        <tr>\n        \t<td>\n              \tanimation-fill-mode\n          \t</td>\n          \t<td>\n              \t設定元素在動畫執行前後如何套用 CSS 的樣式\n          \t</td>\n        </tr>\n        <tr>\n        \t<td>\n              \tanimation-timing-function\n          \t</td>\n          \t<td>\n              \t設定動畫執行的時間函數\n          \t</td>\n        </tr>\n    </tbody>\n</table>\n<br>\n\n我們將所有動畫相關的屬性都使用看看：\n\n\tdiv {\n        width: 100px;\n        height: 100px;\n        background: red;\n        position: relative;\n    \t/* Chrome, Safari, Opera */\n        -webkit-animation-name: animation-keyframe-name;\n        -webkit-animation-duration: 5s;\n        -webkit-animation-timing-function: linear;\n        -webkit-animation-delay: 2s;\n        -webkit-animation-iteration-count: infinite;\n        -webkit-animation-direction: alternate;\n        -webkit-animation-play-state: running;\n        /* Standard syntax */\n        animation-name: animation-keyframe-name;\n        animation-duration: 5s;\n        animation-timing-function: linear;\n        animation-delay: 2s;\n        animation-iteration-count: infinite;\n        animation-direction: alternate;\n        animation-play-state: running;\n    }\n    \n    \n#### <a href=\"http://codepen.io/fukuball/pen/dGKfo\" target=\"_blank\">CSS3 Animation 第三級展示</a>\n\n感覺越來越強了！\n\n### CSS3 Animation 第四級\n\n在 CSS3 Animation 第三級我們學會使用更多動畫相關屬性了，但 CSS 的語法卻變得很冗長，這時就要使出 CSS3 Animation 第四級，這樣就可以寫出更精簡的 CSS3 Animation 語法了！\n\n上面例子的：\n\t\n    animation-name: animation-keyframe-name;\n    animation-duration: 5s;\n    animation-timing-function: linear;\n    animation-delay: 2s;\n    animation-iteration-count: infinite;\n    animation-direction: alternate;\n    animation-play-state: running;\n\n可以改寫成：\n\n\tanimation: animation-keyframe-name 5s linear 2s infinite alternate;\n    \n所以就會變成：\n\n\tdiv {\n        -webkit-animation: animation-keyframe-name 5s linear 2s infinite alternate; /* Chrome, Safari, Opera */\n        animation: animation-keyframe-name 5s linear 2s infinite alternate; /* Standard syntax */\n    }\n    \nCSS3 Animation 大法大功告成！\n    \n#### <a href=\"http://codepen.io/fukuball/pen/ADFco\" target=\"_blank\">CSS3 Animation 第四級展示</a>\n\n### 結語\n\n我們已經練完了 CSS3 Animation 大法的前四級，其實只要學會這些技巧，大概就能做出不錯的特效，只是動畫如何安排才會吸引人就要看個人的 Sense 了，像我就沒有什麼 Sense～\n\n或許可以去請教對動畫最有 Sense 的 <a href=\"http://ricetseng.com/\" target=\"_blank\">Rice Tseng</a> 公主！","html":"<h3 id=\"\">前言</h3>\n\n<p>在這個浮誇的時代，如果網頁上沒有酷炫的功能或特效，似乎就遜掉了；如何讓網頁變得酷炫呢？其中一個方法就是使用 CSS3 Animation！只要使用了 CSS3 Animation 就可以讓網頁中的元素動起來，立馬讓你的網頁酷炫度超越世界上 80% 的網頁！</p>\n\n<p>而我個人身為一個全端工程師，稍微研究一下 CSS3 Animation 也是合理的，而且只要會一點點就可以拿來唬唬人，何樂而不為？很可惜這個技能就是唬不了正妹，哎，誰叫正妹只喜歡魔術呢～</p>\n\n<p>不過請特別注意 CSS3 的 Animation 與 Transition 並不相同喔！雖然都可以做到相似的效果，有時看起來也真的很像，但實際上 Animation 與 Transition 依操作的程度不同所以適合使用的地方也會有所不同，或許我之後會再寫一篇文章介紹 Transition。（吧？）</p>\n\n<p>就讓我們一起來看看 CSS3 Animation 大法怎麼練吧！</p>\n\n<h3 id=\"css3animation\">CSS3 Animation 第一級</h3>\n\n<p>假設我們要在一個 div 元素上加上動畫特效：</p>\n\n<h4 id=\"step1keyframe\">Step 1：定義要使用哪個關鍵影格(keyframe)來執行動畫</h4>\n\n<pre><code>div {\n    width: 100px;\n    height: 100px;\n    background: red;\n    position: relative;\n    -webkit-animation: animation-keyframe-name 5s; /* Chrome, Safari, Opera */\n    -webkit-animation-iteration-count: infinite;\n    animation: animation-keyframe-name 5s;\n    animation-iteration-count: infinite;\n}\n</code></pre>\n\n<p>這段 CSS 語法就是代表 div 元素要用哪個關鍵影格來執行動畫，詳細的動畫動作都會寫在關鍵影格裡面，在這邊的意思就是要使用一個名稱叫 <code>animation-keyframe-name</code> 的關鍵影格來進行動畫，後面的 <code>5s</code> 就代表這個動畫執行的時間會是 5 秒鐘。而 <code>animation-iteration-count: infinite</code> 則代表這個關鍵影格動畫會不斷重複執行。</p>\n\n<p>特別注意有 <code>-webkit-</code> 這個前綴的 CSS 語法是為了支援 webkit 核心的瀏覽器，例如：Chrome、Safari、Opera 等等。</p>\n\n<p>所以這段語法翻譯成白話文就是：div 這個元素要使用一個叫做 <code>animation-keyframe-name</code> 的關鍵影格來進行動畫，動畫執行的時間長度為 5 秒鐘，且動畫會不斷重複執行。</p>\n\n<p>keyframe 的名稱如果不想要叫 <code>animation-keyframe-name</code>，也可以改成其他的，比如：<code>fadein-keyframe</code>、<code>fadeout-keyframe</code> 等等。</p>\n\n<h4 id=\"step2keyframe\">Step 2：定義關鍵影格(keyframe)中動畫如何變化</h4>\n\n<pre><code>/* Chrome, Safari, Opera */\n  @-webkit-keyframes animation-keyframe-name {\n    from {background: red;}\n    to {background: yellow;}\n}\n\n/* Standard syntax */\n@keyframes animation-keyframe-name {\n    from {background: red;}\n    to {background: yellow;}\n}\n</code></pre>\n\n<p>這段 CSS 語法就定義了 <code>animation-keyframe-name</code> 這個關鍵影格的動畫變化，它會在 5 秒之內從原本背景色紅色（<code>from {background: red;}</code>）變為背景色黃色（<code>to {background: yellow;}</code>），就這樣我們就可以寫好 CSS3 動畫了，超簡單 der！</p>\n\n<h4 id=\"ahrefhttpcodepeniofukuballpenhzhaetarget_blankcss3animationa\"><a href=\"http://codepen.io/fukuball/pen/hzHAE\" target=\"_blank\">CSS3 Animation 第一級展示</a></h4>\n\n<h3 id=\"css3animation\">CSS3 Animation 第二級</h3>\n\n<p>CSS3 Animation 第一級只有 from to 這兩種狀態可以指定動畫的變化，總感覺缺少了點什麼，如果好死不死動畫想要指定三種狀態變化怎麼辦？這時就要使用 CSS3 Animation 第二級了！在 CSS3 Animation 第二級我們可以使用動畫執行時間百分比來指定動畫變化的狀態。</p>\n\n<pre><code>/* Chrome, Safari, Opera */\n@-webkit-keyframes animation-keyframe-name {\n    0%   {background: red; left:0px; top:0px;}\n    25%  {background: yellow; left:200px; top:0px;}\n    50%  {background: blue; left:200px; top:200px;}\n    75%  {background: green; left:0px; top:200px;}\n    100% {background: red; left:0px; top:0px;}\n}\n\n/* Standard syntax */\n@keyframes animation-keyframe-name {\n    0%   {background: red; left:0px; top:0px;}\n    25%  {background: yellow; left:200px; top:0px;}\n    50%  {background: blue; left:200px; top:200px;}\n    75%  {background: green; left:0px; top:200px;}\n    100% {background: red; left:0px; top:0px;}\n}\n</code></pre>\n\n<p>這段 CSS 語法將動畫的變化分成五個狀態，在動畫執行時間 0% 的時候是背景紅色的狀態（<code>0% {background: red;}</code>），在動畫執行時間 25% 的時候是背景黃色的狀態（<code>25% {background: yellow;}</code>），在動畫執行時間 50% 的時候是背景藍色的狀態（<code>50% {background: blue;}</code>），在動畫執行時間 75% 的時候是背景綠色的狀態（<code>75% {background: green;}</code>），在動畫執行時間 100% 的時候是背景變回紅色的狀態（<code>100% {background: red;}</code>），在我們這個例子裡動畫執行時間 100% 也就是動畫執行時間在第五秒的時候。如此我們就可以做更多細緻的動畫了。</p>\n\n<h4 id=\"ahrefhttpcodepeniofukuballpenlgwnxtarget_blankcss3animationa\"><a href=\"http://codepen.io/fukuball/pen/lGwnx\" target=\"_blank\">CSS3 Animation 第二級展示</a></h4>\n\n<h3 id=\"css3animation\">CSS3 Animation 第三級</h3>\n\n<p>在 CSS3 Animation 第三級我們可以使用更多屬性（property）來讓動畫有更多調整的空間，這些 animation 相關的 property 列表如下：</p>\n\n<table>  \n    <thead>\n        <tr>\n              <td>\n                  CSS 屬性\n              </td>\n              <td>\n                  說明\n              </td>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>\n                  animation-delay\n              </td>\n              <td>\n                  設定元素在被載入之後到開始執行動畫之間的延遲時間\n              </td>\n        </tr>\n        <tr>\n            <td>\n                  animation-direction\n              </td>\n              <td>\n                  設定元素在動畫執行完之後，是否要以相反方向的方式播放，或是從頭開始以原來的方向重複播放。\n              </td>\n        </tr>\n        <tr>\n            <td>\n                  animation-duration\n              </td>\n              <td>\n                  設定整個動畫執行一次的時間長度\n              </td>\n        </tr>\n        <tr>\n            <td>\n                  animation-iteration-count\n              </td>\n              <td>\n                  設定動畫執行的次數，若要不斷重複執行，則可設為 infinite\n              </td>\n        </tr>\n        <tr>\n            <td>\n                  animation-play-state\n              </td>\n              <td>\n                  可用來暫停或繼續動畫播放\n              </td>\n        </tr>\n        <tr>\n            <td>\n                  animation-fill-mode\n              </td>\n              <td>\n                  設定元素在動畫執行前後如何套用 CSS 的樣式\n              </td>\n        </tr>\n        <tr>\n            <td>\n                  animation-timing-function\n              </td>\n              <td>\n                  設定動畫執行的時間函數\n              </td>\n        </tr>\n    </tbody>\n</table>  \n\n<p><br></p>\n\n<p>我們將所有動畫相關的屬性都使用看看：</p>\n\n<pre><code>div {\n    width: 100px;\n    height: 100px;\n    background: red;\n    position: relative;\n    /* Chrome, Safari, Opera */\n    -webkit-animation-name: animation-keyframe-name;\n    -webkit-animation-duration: 5s;\n    -webkit-animation-timing-function: linear;\n    -webkit-animation-delay: 2s;\n    -webkit-animation-iteration-count: infinite;\n    -webkit-animation-direction: alternate;\n    -webkit-animation-play-state: running;\n    /* Standard syntax */\n    animation-name: animation-keyframe-name;\n    animation-duration: 5s;\n    animation-timing-function: linear;\n    animation-delay: 2s;\n    animation-iteration-count: infinite;\n    animation-direction: alternate;\n    animation-play-state: running;\n}\n</code></pre>\n\n<h4 id=\"ahrefhttpcodepeniofukuballpendgkfotarget_blankcss3animationa\"><a href=\"http://codepen.io/fukuball/pen/dGKfo\" target=\"_blank\">CSS3 Animation 第三級展示</a></h4>\n\n<p>感覺越來越強了！</p>\n\n<h3 id=\"css3animation\">CSS3 Animation 第四級</h3>\n\n<p>在 CSS3 Animation 第三級我們學會使用更多動畫相關屬性了，但 CSS 的語法卻變得很冗長，這時就要使出 CSS3 Animation 第四級，這樣就可以寫出更精簡的 CSS3 Animation 語法了！</p>\n\n<p>上面例子的：</p>\n\n<pre><code>animation-name: animation-keyframe-name;\nanimation-duration: 5s;\nanimation-timing-function: linear;\nanimation-delay: 2s;\nanimation-iteration-count: infinite;\nanimation-direction: alternate;\nanimation-play-state: running;\n</code></pre>\n\n<p>可以改寫成：</p>\n\n<pre><code>animation: animation-keyframe-name 5s linear 2s infinite alternate;\n</code></pre>\n\n<p>所以就會變成：</p>\n\n<pre><code>div {\n    -webkit-animation: animation-keyframe-name 5s linear 2s infinite alternate; /* Chrome, Safari, Opera */\n    animation: animation-keyframe-name 5s linear 2s infinite alternate; /* Standard syntax */\n}\n</code></pre>\n\n<p>CSS3 Animation 大法大功告成！</p>\n\n<h4 id=\"ahrefhttpcodepeniofukuballpenadfcotarget_blankcss3animationa\"><a href=\"http://codepen.io/fukuball/pen/ADFco\" target=\"_blank\">CSS3 Animation 第四級展示</a></h4>\n\n<h3 id=\"\">結語</h3>\n\n<p>我們已經練完了 CSS3 Animation 大法的前四級，其實只要學會這些技巧，大概就能做出不錯的特效，只是動畫如何安排才會吸引人就要看個人的 Sense 了，像我就沒有什麼 Sense～</p>\n\n<p>或許可以去請教對動畫最有 Sense 的 <a href=\"http://ricetseng.com/\" target=\"_blank\">Rice Tseng</a> 公主！</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-07-08T12:45:38.000Z","created_by":1,"updated_at":"2014-07-09T07:45:33.000Z","updated_by":1,"published_at":"2014-07-08T14:55:36.000Z","published_by":1},{"id":31,"uuid":"11374866-5c7d-49e7-9ee5-a1f0531fb0c1","title":"Welcome to Ghost","slug":"welcome-to-ghost-2","markdown":"You're live! Nice. We've put together a little post to introduce you to the Ghost editor and get you started. You can manage your content by signing in to the admin area at `<your blog URL>/ghost/`. When you arrive, you can select this post from a list on the left and see a preview of it on the right. Click the little pencil icon at the top of the preview to edit this post and read the next section!\n\n## Getting Started\n\nGhost uses something called Markdown for writing. Essentially, it's a shorthand way to manage your post formatting as you write!\n\nWriting in Markdown is really easy. In the left hand panel of Ghost, you simply write as you normally would. Where appropriate, you can use *shortcuts* to **style** your content. For example, a list:\n\n* Item number one\n* Item number two\n    * A nested item\n* A final item\n\nor with numbers!\n\n1. Remember to buy some milk\n2. Drink the milk\n3. Tweet that I remembered to buy the milk, and drank it\n\n### Links\n\nWant to link to a source? No problem. If you paste in url, like http://ghost.org - it'll automatically be linked up. But if you want to customise your anchor text, you can do that too! Here's a link to [the Ghost website](http://ghost.org). Neat.\n\n### What about Images?\n\nImages work too! Already know the URL of the image you want to include in your article? Simply paste it in like this to make it show up:\n\n![The Ghost Logo](https://ghost.org/images/ghost.png)\n\nNot sure which image you want to use yet? That's ok too. Leave yourself a descriptive placeholder and keep writing. Come back later and drag and drop the image in to upload:\n\n![A bowl of bananas]\n\n\n### Quoting\n\nSometimes a link isn't enough, you want to quote someone on what they've said. It was probably very wisdomous. Is wisdomous a word? Find out in a future release when we introduce spellcheck! For now - it's definitely a word.\n\n> Wisdomous - it's definitely a word.\n\n### Working with Code\n\nGot a streak of geek? We've got you covered there, too. You can write inline `<code>` blocks really easily with back ticks. Want to show off something more comprehensive? 4 spaces of indentation gets you there.\n\n    .awesome-thing {\n        display: block;\n        width: 100%;\n    }\n\n### Ready for a Break? \n\nThrow 3 or more dashes down on any new line and you've got yourself a fancy new divider. Aw yeah.\n\n---\n\n### Advanced Usage\n\nThere's one fantastic secret about Markdown. If you want, you can  write plain old HTML and it'll still work! Very flexible.\n\n<input type=\"text\" placeholder=\"I'm an input field!\" />\n\nThat should be enough to get you started. Have fun - and let us know what you think :)","html":"<p>You're live! Nice. We've put together a little post to introduce you to the Ghost editor and get you started. You can manage your content by signing in to the admin area at <code>&lt;your blog URL&gt;/ghost/</code>. When you arrive, you can select this post from a list on the left and see a preview of it on the right. Click the little pencil icon at the top of the preview to edit this post and read the next section!</p>\n\n<h2 id=\"gettingstarted\">Getting Started</h2>\n\n<p>Ghost uses something called Markdown for writing. Essentially, it's a shorthand way to manage your post formatting as you write!</p>\n\n<p>Writing in Markdown is really easy. In the left hand panel of Ghost, you simply write as you normally would. Where appropriate, you can use <em>shortcuts</em> to <strong>style</strong> your content. For example, a list:</p>\n\n<ul>\n<li>Item number one</li>\n<li>Item number two\n<ul><li>A nested item</li></ul></li>\n<li>A final item</li>\n</ul>\n\n<p>or with numbers!</p>\n\n<ol>\n<li>Remember to buy some milk  </li>\n<li>Drink the milk  </li>\n<li>Tweet that I remembered to buy the milk, and drank it</li>\n</ol>\n\n<h3 id=\"links\">Links</h3>\n\n<p>Want to link to a source? No problem. If you paste in url, like <a href='http://ghost.org'>http://ghost.org</a> - it'll automatically be linked up. But if you want to customise your anchor text, you can do that too! Here's a link to <a href=\"http://ghost.org\">the Ghost website</a>. Neat.</p>\n\n<h3 id=\"whataboutimages\">What about Images?</h3>\n\n<p>Images work too! Already know the URL of the image you want to include in your article? Simply paste it in like this to make it show up:</p>\n\n<p><img src=\"https://ghost.org/images/ghost.png\" alt=\"The Ghost Logo\" /></p>\n\n<p>Not sure which image you want to use yet? That's ok too. Leave yourself a descriptive placeholder and keep writing. Come back later and drag and drop the image in to upload:</p>\n\n<h3 id=\"quoting\">Quoting</h3>\n\n<p>Sometimes a link isn't enough, you want to quote someone on what they've said. It was probably very wisdomous. Is wisdomous a word? Find out in a future release when we introduce spellcheck! For now - it's definitely a word.</p>\n\n<blockquote>\n  <p>Wisdomous - it's definitely a word.</p>\n</blockquote>\n\n<h3 id=\"workingwithcode\">Working with Code</h3>\n\n<p>Got a streak of geek? We've got you covered there, too. You can write inline <code>&lt;code&gt;</code> blocks really easily with back ticks. Want to show off something more comprehensive? 4 spaces of indentation gets you there.</p>\n\n<pre><code>.awesome-thing {\n    display: block;\n    width: 100%;\n}\n</code></pre>\n\n<h3 id=\"readyforabreak\">Ready for a Break?</h3>\n\n<p>Throw 3 or more dashes down on any new line and you've got yourself a fancy new divider. Aw yeah.</p>\n\n<hr />\n\n<h3 id=\"advancedusage\">Advanced Usage</h3>\n\n<p>There's one fantastic secret about Markdown. If you want, you can  write plain old HTML and it'll still work! Very flexible.</p>\n\n<p><input type=\"text\" placeholder=\"I'm an input field!\" /></p>\n\n<p>That should be enough to get you started. Have fun - and let us know what you think :)</p>","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-01-28T06:43:15.000Z","created_by":1,"updated_at":"2014-01-29T19:07:06.000Z","updated_by":1,"published_at":"2014-01-28T06:43:15.000Z","published_by":1},{"id":32,"uuid":"e30e077b-5fc1-4fb1-a470-6b1868e43141","title":"將 Ghost 部落格搬到 Heroku 自己 Host","slug":"jiang-ghost-bu-luo-ge-ban-dao-heroku-zi-ji-host","markdown":"之前心血來潮（詳見[開始使用 GHOST 寫部落格](http://blog.fukuball.com/kai-shi-shi-yong-ghost-xie-bu-luo-ge/)）使用了 [Ghost](https://ghost.org/) 寫部落格，沒想到竟也堅持了一陣子了，眼見之前因為在 [Kickstarter](https://www.kickstarter.com/) 參與了 Ghost 投資而得到的免費六個月使用權就要過了，之後就需要付每個月至少 5 美元的使用費，實在也不是筆小錢，但身為程式設計師自己 Host 一個網站其實也不是件難事，所以就在參加 COSCUP 的空檔時間將部落格搬到 [Heroku](https://dashboard.heroku.com/apps) 自己 Host 了～ 自己的部落格自己 Host！\n\n原本想說搬移的過程也可以寫成一篇教學文，但是真的有點簡單，雖然有些細節需要注意，但基本上我實在是不知如何湊成一篇教學文啊！總之如果會使用 Heroku，基本上在 Heroku 上安裝 Ghost 是沒什麼問題的，這樣就可以在 Heroku 上免費 Host Ghost 部落格了！省下每個月 5 美元的感覺真爽啊！\n\n<p style=\"text-align:center\">\n<img src=\"http://static.obeobe.com/2014/Jul/i__e88e__-1406113001957.jpg\" alt=\"非常好\">\n</p>\n\n如果大家還是想知道如何在 Heroku 上安裝 Ghost 部落格，可以到我 Github 上 [blog.fukuball.com](https://github.com/fukuball/blog.fukuball.com) 這個 Repo 參考一下安裝的指令，不過由於我自己有使用 AWS，這個 Repo 的後端資料庫是接 AWS RDS MySQL，圖片的 Storage 則是接 AWS S3，如果沒有使用 AWS 的話，就建議使用原本的設定就好，可以參考 [Deploy Ghost to heroku for free](http://www.therightcode.net/deploy-ghost-to-heroku-for-free/) 這篇文章來試試～\n\n話說雖然我已經將 Ghost 上的部落格關掉了，但我個人的 Profile 仍然還在 Ghost 網站上，[funder badge](https://ghost.org/fukuball/) 還是好好地掛在網頁上面。現在真心覺得有 funder badge 的 backer 應該要可以永久免費使用 Ghost 部落格才對啊！不然以 Ghost 這種 backer 組成幾乎都是程式設計師的平台，只要一收費大家就是怒搬部落格到 Heroku 了！\n\n![ghost funder badge](http://static.obeobe.com/2014/Jul/Screen_Shot_2014_07_23_at_7_00_18_PM-1406113694581.png)\n\n<blockquote>\n圖片取自於 Google 搜尋，絕無意侵犯智財權。若有侵犯請告知，我會馬上刪除，感謝！\n</blockquote>","html":"<p>之前心血來潮（詳見<a href=\"http://blog.fukuball.com/kai-shi-shi-yong-ghost-xie-bu-luo-ge/\">開始使用 GHOST 寫部落格</a>）使用了 <a href=\"https://ghost.org/\">Ghost</a> 寫部落格，沒想到竟也堅持了一陣子了，眼見之前因為在 <a href=\"https://www.kickstarter.com/\">Kickstarter</a> 參與了 Ghost 投資而得到的免費六個月使用權就要過了，之後就需要付每個月至少 5 美元的使用費，實在也不是筆小錢，但身為程式設計師自己 Host 一個網站其實也不是件難事，所以就在參加 COSCUP 的空檔時間將部落格搬到 <a href=\"https://dashboard.heroku.com/apps\">Heroku</a> 自己 Host 了～ 自己的部落格自己 Host！</p>\n\n<p>原本想說搬移的過程也可以寫成一篇教學文，但是真的有點簡單，雖然有些細節需要注意，但基本上我實在是不知如何湊成一篇教學文啊！總之如果會使用 Heroku，基本上在 Heroku 上安裝 Ghost 是沒什麼問題的，這樣就可以在 Heroku 上免費 Host Ghost 部落格了！省下每個月 5 美元的感覺真爽啊！</p>\n\n<p style=\"text-align:center\">  \n<img src=\"http://static.obeobe.com/2014/Jul/i__e88e__-1406113001957.jpg\" alt=\"非常好\">  \n</p>\n\n<p>如果大家還是想知道如何在 Heroku 上安裝 Ghost 部落格，可以到我 Github 上 <a href=\"https://github.com/fukuball/blog.fukuball.com\">blog.fukuball.com</a> 這個 Repo 參考一下安裝的指令，不過由於我自己有使用 AWS，這個 Repo 的後端資料庫是接 AWS RDS MySQL，圖片的 Storage 則是接 AWS S3，如果沒有使用 AWS 的話，就建議使用原本的設定就好，可以參考 <a href=\"http://www.therightcode.net/deploy-ghost-to-heroku-for-free/\">Deploy Ghost to heroku for free</a> 這篇文章來試試～</p>\n\n<p>話說雖然我已經將 Ghost 上的部落格關掉了，但我個人的 Profile 仍然還在 Ghost 網站上，<a href=\"https://ghost.org/fukuball/\">funder badge</a> 還是好好地掛在網頁上面。現在真心覺得有 funder badge 的 backer 應該要可以永久免費使用 Ghost 部落格才對啊！不然以 Ghost 這種 backer 組成幾乎都是程式設計師的平台，只要一收費大家就是怒搬部落格到 Heroku 了！</p>\n\n<p><img src=\"http://static.obeobe.com/2014/Jul/Screen_Shot_2014_07_23_at_7_00_18_PM-1406113694581.png\" alt=\"ghost funder badge\" /></p>\n\n<blockquote>  \n圖片取自於 Google 搜尋，絕無意侵犯智財權。若有侵犯請告知，我會馬上刪除，感謝！\n</blockquote>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-07-23T10:26:31.000Z","created_by":1,"updated_at":"2014-07-23T11:14:18.000Z","updated_by":1,"published_at":"2014-07-23T11:14:18.000Z","published_by":1},{"id":33,"uuid":"ffef0191-334d-43ab-89f0-628f6bfe9968","title":"Laravel 學習筆記 Lesson 1","slug":"laravel-xue-xi-bi-ji-lesson-1","markdown":"### 前言\n\n近來 Web Programming 相關技術及工具已經越來越成熟，許多功能可能都不再需要自己動手從頭刻，需要的反而是學習如何去使用這些工具，Laravel 是目前最熱門的 PHP Framework，它就是一個為了 Web Programming 而生的開發工具。\n\n學習工具看似是一個方便且輕鬆的路，但其實也有它的難處，比如會常常忘記如何使用、使用的的語法也可能會是這個 Framework 所特有的、對於實際上程式怎麼運作會一知半解，這對初學者來說還是有學習進入門檻。\n\n我編寫了這系列部落格僅是為了記錄我個人的學習記錄，並非為了教學，所以可能會有不夠詳盡的地方，但多少希望能夠回饋一下社群，當然有問題也歡迎各位跟我討論，大家一起互相學習！\n\n### 建立開發環境\n\n1. Linux 作業系統，例如 Ubuntu 或 Debian\n2. 安裝 Git\n3. 安裝 PHP 5.4 以上版本（需要有 MCrypt PHP 套件）\n4. 安裝 Composer（PHP 套件管理工具）\n5. 安裝 Nginx\n6. 安裝 HHVM\n7. 安裝 Laravel\n\n其實 Laravel 也又內建 web server，但使用 Nginx 及 HHVM 作為 PHP 網頁伺服器是為了建立大型網站，這可以帶來比較好的穩定度及效能。\n\n建立開發環境的詳細細節可以參考這篇教學文章： http://fideloper.com/hhvm-nginx-laravel\n\n### 設定 Git 的 SSH Key\n\n設定好 Git 的 SSH Key 之後，伺服器就可以透過 SSH 的方式存取 Git Server 的 Repo，我個人是 Github 及 Bitbucket 都有使用，因此兩個 Git Server 都會設定好 SSH Key。\n\n設定方法如下：\n\n1. [Set up SSH for Git On Github](https://help.github.com/articles/generating-ssh-keys#platform-linux)\n2. [Set up SSH for Git On Bitbucket](https://confluence.atlassian.com/display/BITBUCKET/Set+up+SSH+for+Git)\n\n若是初學者不了解 Git 如何使用，可以參考一下我這篇部落格 [GIT 簡易使用教學](http://blog.fukuball.com/git-jian-yi-shi-yong-jiao-xue/)，至少了解一下基本使用方法。\n\n### 建立 Laravel Project\n\n開發環境建立好之後，我們就可以開始新建一個 Laravel Project，建立 Laravel Project 的指令如下：\n\n\tcomposer create-project laravel/laravel your-project-name --prefer-dist\n    \nyour-project-name 請填入想要的 Project 名稱，比如我想做一個叫 car 的網站，那我的指令就會是：\n\n\tcomposer create-project laravel/laravel car --prefer-dist\n    \n就這麼簡單的一個指令就可以安裝好 Laravel 的 Project 了！\n\n### 將 Laravel 加入 Git Repo\n\n接下來我們要將 Laravel Project 加入 Git Repo 做版本控制，指令如下：\n\n#### Step 0：在 Git Server 開好 Git Repo\n\n首先需要在 Git Server 上開好 Project 的 Git Repo，Git Server 可以是 Github 或是 bitbucket 等服務，如此我們就可以得到 Git Server 的 Git Repo 網址。\n\n#### Step 1：進入 Laravel Project 資料夾\n\n\tcd /path/to/your/project \n    \n比如我的 Project 的實體位置在 /var/www/car，那指令就會是：\n    \n    cd /var/www/car\n    \n#### Step 2：初始化一個 Git Repo\n\n\tgit init \n    \n#### Step 3：將 Local Git Repo 連結 Remote Git Repo\n\n在 Step 0 可以取得 Git Server 上的 Project Git Repo 網址，接下來我們要將 local 的 git repo 與 server 上的 git repo 連結起來，以 bitbucket 為例：\n\n\tgit remote add origin git@bitbucket.org/username/bbreponame.git\n    \n指令中的 username 指的是在 git server 上的使用者名稱，bbreponame 指的是 step 0 時在 git server 上開好的 repo 名稱，所以 car 這個 prject 的 server git repo 網址可能會像這樣：\n\n\tgit remote add origin ssh://git@bitbucket.org/fukuball/car.git\n    \n#### Step 4：將目前的 Laravel Project 所有檔案加入 Git Repo\n\n\tgit add .\n    \n#### Step 5：將目前的 Laravel Project Commit 進 Git Repo\n\n\tgit commit -a -m \"Init laravel project\"\n    \n#### Step 6：將所有的 Commit Push 至 Git Server\n\n\tgit push origin master\n    \n完成以上步驟就完成了 Laravel Project 的建立，並且將 Laravel Project 加入了 Git 版本控制，這是所有 Web 專案開發的起手式～\n\n### 設定 Laravel Project 使用 Nginx Virtual Host\n\n若是大型網站開發，我們通常會使用 Nginx 等網頁伺服器，而同一台機器上可能不只有一個 Project，可能會同時有 <code>car.fukuball.com</code> 及 <code>bike.fukuball.com</code> 在指向同一台機器，這使我們就要使用 Nginx 的 Virtual Host 設定將 <code>car.fukuball.com</code> 指向 car 這個 project 的實體位置，這邊的例子是 <code>/var/www/car</code>，而 \n<code>bike.fukuball.com</code> 指向 bike 這個 project 的實體位置，如： <code>/var/www/bike</code>。\n\nVirtual Host 設定檔位置通常會是在 <code>/etc/nginx/sites-available</code> 或是 <code>/etc/nginx/conf.d</code>，現在我們就在這個位置下新增一個設定檔：\n\n\tvim /etc/nginx/conf.d/car.fukuball.com.conf\n    \n設定檔的範例內容如下：\n\n\tserver {\n    \tlisten 80;\n        root /var/www/car/public/;\n        index index.php index.html index.htm;\n        server_name car.fukuball.com;\n        location / {\n\t\t\ttry_files $uri $uri/ /index.php?$args;\n\t\t}\n        location ~* ^.+\\.(ogg|ogv|svg|svgz|eot|otf|woff|mp4|ttf|rss|atom|jpg|jpeg|gif|png|ico|zip|tgz|gz|rar|bz2|doc|xls|exe|ppt|tar|mid|midi|wav|bmp|rtf)$ {\n\t\t\taccess_log off; log_not_found off; expires max;\n\t\t}\n\t\tlocation ~* \\.(css|js)$ {\n\t\t\texpires 365d;\n\t\t}\n        error_page 500 502 503 504 /50x.html;\n\t\tlocation = /50x.html {\n\t\t\troot /usr/share/nginx/html;\n\t\t}\n        location ~ \\.(hh|php)$ {\n\t\t\tfastcgi_split_path_info ^(.+\\.php)(/.+)$;\n\t\t\tfastcgi_keep_conn on;\n\t\t\tfastcgi_pass 127.0.0.1:9000;\n\t\t\tfastcgi_param  SCRIPT_FILENAME $document_root$fastcgi_script_name;\n            fastcgi_param REDIRECT_QUERY_STRING \"$query_string\";\n            include fastcgi_params;\n\t\t}\n\t\tlocation ~ /\\. {\n\t\t\tdeny all;\n\t\t}\n    }\n    \n設定好之後，將 nginx 重開就完成了。\n    \n\tservice nginx restart\n        \n### 結語\n\n以上完成之後，終於可以開始開發專案，我們也終於可以進入如何使用及開發 Laravel PHP Framework，Laravel 學習筆記 Lesson 1 主要就是記錄如何從無到有建立一個可以使用 Laravel 的開發環境，接下來就是打開編輯器，開始寫 Code 囉！\n\nps. 建議初學者可以使用 Sublime Text 等 IDE 作為開發的編輯器，這篇部落格有介紹 [我在 SUBLIME TEXT 2 常用的快捷鍵](http://blog.fukuball.com/wo-zai-sublime-text-2-chang-yong-de-kuai-jie-jian/)，在開發上會有很多幫助。\n    \n\n","html":"<h3 id=\"\">前言</h3>\n\n<p>近來 Web Programming 相關技術及工具已經越來越成熟，許多功能可能都不再需要自己動手從頭刻，需要的反而是學習如何去使用這些工具，Laravel 是目前最熱門的 PHP Framework，它就是一個為了 Web Programming 而生的開發工具。</p>\n\n<p>學習工具看似是一個方便且輕鬆的路，但其實也有它的難處，比如會常常忘記如何使用、使用的的語法也可能會是這個 Framework 所特有的、對於實際上程式怎麼運作會一知半解，這對初學者來說還是有學習進入門檻。</p>\n\n<p>我編寫了這系列部落格僅是為了記錄我個人的學習記錄，並非為了教學，所以可能會有不夠詳盡的地方，但多少希望能夠回饋一下社群，當然有問題也歡迎各位跟我討論，大家一起互相學習！</p>\n\n<h3 id=\"\">建立開發環境</h3>\n\n<ol>\n<li>Linux 作業系統，例如 Ubuntu 或 Debian  </li>\n<li>安裝 Git  </li>\n<li>安裝 PHP 5.4 以上版本（需要有 MCrypt PHP 套件）  </li>\n<li>安裝 Composer（PHP 套件管理工具）  </li>\n<li>安裝 Nginx  </li>\n<li>安裝 HHVM  </li>\n<li>安裝 Laravel</li>\n</ol>\n\n<p>其實 Laravel 也又內建 web server，但使用 Nginx 及 HHVM 作為 PHP 網頁伺服器是為了建立大型網站，這可以帶來比較好的穩定度及效能。</p>\n\n<p>建立開發環境的詳細細節可以參考這篇教學文章： <a href='http://fideloper.com/hhvm-nginx-laravel'>http://fideloper.com/hhvm-nginx-laravel</a></p>\n\n<h3 id=\"gitsshkey\">設定 Git 的 SSH Key</h3>\n\n<p>設定好 Git 的 SSH Key 之後，伺服器就可以透過 SSH 的方式存取 Git Server 的 Repo，我個人是 Github 及 Bitbucket 都有使用，因此兩個 Git Server 都會設定好 SSH Key。</p>\n\n<p>設定方法如下：</p>\n\n<ol>\n<li><a href=\"https://help.github.com/articles/generating-ssh-keys#platform-linux\">Set up SSH for Git On Github</a>  </li>\n<li><a href=\"https://confluence.atlassian.com/display/BITBUCKET/Set+up+SSH+for+Git\">Set up SSH for Git On Bitbucket</a></li>\n</ol>\n\n<p>若是初學者不了解 Git 如何使用，可以參考一下我這篇部落格 <a href=\"http://blog.fukuball.com/git-jian-yi-shi-yong-jiao-xue/\">GIT 簡易使用教學</a>，至少了解一下基本使用方法。</p>\n\n<h3 id=\"laravelproject\">建立 Laravel Project</h3>\n\n<p>開發環境建立好之後，我們就可以開始新建一個 Laravel Project，建立 Laravel Project 的指令如下：</p>\n\n<pre><code>composer create-project laravel/laravel your-project-name --prefer-dist\n</code></pre>\n\n<p>your-project-name 請填入想要的 Project 名稱，比如我想做一個叫 car 的網站，那我的指令就會是：</p>\n\n<pre><code>composer create-project laravel/laravel car --prefer-dist\n</code></pre>\n\n<p>就這麼簡單的一個指令就可以安裝好 Laravel 的 Project 了！</p>\n\n<h3 id=\"laravelgitrepo\">將 Laravel 加入 Git Repo</h3>\n\n<p>接下來我們要將 Laravel Project 加入 Git Repo 做版本控制，指令如下：</p>\n\n<h4 id=\"step0gitservergitrepo\">Step 0：在 Git Server 開好 Git Repo</h4>\n\n<p>首先需要在 Git Server 上開好 Project 的 Git Repo，Git Server 可以是 Github 或是 bitbucket 等服務，如此我們就可以得到 Git Server 的 Git Repo 網址。</p>\n\n<h4 id=\"step1laravelproject\">Step 1：進入 Laravel Project 資料夾</h4>\n\n<pre><code>cd /path/to/your/project \n</code></pre>\n\n<p>比如我的 Project 的實體位置在 /var/www/car，那指令就會是：</p>\n\n<pre><code>cd /var/www/car\n</code></pre>\n\n<h4 id=\"step2gitrepo\">Step 2：初始化一個 Git Repo</h4>\n\n<pre><code>git init \n</code></pre>\n\n<h4 id=\"step3localgitreporemotegitrepo\">Step 3：將 Local Git Repo 連結 Remote Git Repo</h4>\n\n<p>在 Step 0 可以取得 Git Server 上的 Project Git Repo 網址，接下來我們要將 local 的 git repo 與 server 上的 git repo 連結起來，以 bitbucket 為例：</p>\n\n<pre><code>git remote add origin git@bitbucket.org/username/bbreponame.git\n</code></pre>\n\n<p>指令中的 username 指的是在 git server 上的使用者名稱，bbreponame 指的是 step 0 時在 git server 上開好的 repo 名稱，所以 car 這個 prject 的 server git repo 網址可能會像這樣：</p>\n\n<pre><code>git remote add origin ssh://git@bitbucket.org/fukuball/car.git\n</code></pre>\n\n<h4 id=\"step4laravelprojectgitrepo\">Step 4：將目前的 Laravel Project 所有檔案加入 Git Repo</h4>\n\n<pre><code>git add .\n</code></pre>\n\n<h4 id=\"step5laravelprojectcommitgitrepo\">Step 5：將目前的 Laravel Project Commit 進 Git Repo</h4>\n\n<pre><code>git commit -a -m \"Init laravel project\"\n</code></pre>\n\n<h4 id=\"step6commitpushgitserver\">Step 6：將所有的 Commit Push 至 Git Server</h4>\n\n<pre><code>git push origin master\n</code></pre>\n\n<p>完成以上步驟就完成了 Laravel Project 的建立，並且將 Laravel Project 加入了 Git 版本控制，這是所有 Web 專案開發的起手式～</p>\n\n<h3 id=\"laravelprojectnginxvirtualhost\">設定 Laravel Project 使用 Nginx Virtual Host</h3>\n\n<p>若是大型網站開發，我們通常會使用 Nginx 等網頁伺服器，而同一台機器上可能不只有一個 Project，可能會同時有 <code>car.fukuball.com</code> 及 <code>bike.fukuball.com</code> 在指向同一台機器，這使我們就要使用 Nginx 的 Virtual Host 設定將 <code>car.fukuball.com</code> 指向 car 這個 project 的實體位置，這邊的例子是 <code>/var/www/car</code>，而 \n<code>bike.fukuball.com</code> 指向 bike 這個 project 的實體位置，如： <code>/var/www/bike</code>。</p>\n\n<p>Virtual Host 設定檔位置通常會是在 <code>/etc/nginx/sites-available</code> 或是 <code>/etc/nginx/conf.d</code>，現在我們就在這個位置下新增一個設定檔：</p>\n\n<pre><code>vim /etc/nginx/conf.d/car.fukuball.com.conf\n</code></pre>\n\n<p>設定檔的範例內容如下：</p>\n\n<pre><code>server {\n    listen 80;\n    root /var/www/car/public/;\n    index index.php index.html index.htm;\n    server_name car.fukuball.com;\n    location / {\n        try_files $uri $uri/ /index.php?$args;\n    }\n    location ~* ^.+\\.(ogg|ogv|svg|svgz|eot|otf|woff|mp4|ttf|rss|atom|jpg|jpeg|gif|png|ico|zip|tgz|gz|rar|bz2|doc|xls|exe|ppt|tar|mid|midi|wav|bmp|rtf)$ {\n        access_log off; log_not_found off; expires max;\n    }\n    location ~* \\.(css|js)$ {\n        expires 365d;\n    }\n    error_page 500 502 503 504 /50x.html;\n    location = /50x.html {\n        root /usr/share/nginx/html;\n    }\n    location ~ \\.(hh|php)$ {\n        fastcgi_split_path_info ^(.+\\.php)(/.+)$;\n        fastcgi_keep_conn on;\n        fastcgi_pass 127.0.0.1:9000;\n        fastcgi_param  SCRIPT_FILENAME $document_root$fastcgi_script_name;\n        fastcgi_param REDIRECT_QUERY_STRING \"$query_string\";\n        include fastcgi_params;\n    }\n    location ~ /\\. {\n        deny all;\n    }\n}\n</code></pre>\n\n<p>設定好之後，將 nginx 重開就完成了。</p>\n\n<pre><code>service nginx restart\n</code></pre>\n\n<h3 id=\"\">結語</h3>\n\n<p>以上完成之後，終於可以開始開發專案，我們也終於可以進入如何使用及開發 Laravel PHP Framework，Laravel 學習筆記 Lesson 1 主要就是記錄如何從無到有建立一個可以使用 Laravel 的開發環境，接下來就是打開編輯器，開始寫 Code 囉！</p>\n\n<p>ps. 建議初學者可以使用 Sublime Text 等 IDE 作為開發的編輯器，這篇部落格有介紹 <a href=\"http://blog.fukuball.com/wo-zai-sublime-text-2-chang-yong-de-kuai-jie-jian/\">我在 SUBLIME TEXT 2 常用的快捷鍵</a>，在開發上會有很多幫助。</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-08-05T08:37:49.000Z","created_by":1,"updated_at":"2014-08-09T06:40:42.000Z","updated_by":1,"published_at":"2014-08-05T09:35:49.000Z","published_by":1},{"id":34,"uuid":"8395fe1c-461f-485b-a318-ff37c12ef34e","title":"如何使用 jieba 結巴中文分詞程式","slug":"ru-he-shi-yong-jieba-jie-ba-zhong-wen-fen-ci-cheng-shi","markdown":"### 前言\n\n自然語言處理的其中一個重要環節就是中文斷詞的處理，比起英文斷詞，中文斷詞在先天上就比較難處理，比如電腦要怎麼知道「全台大停電」要斷詞成「全台 / 大 / 停電」呢？如果是英文「Power outage all over Taiwan」，就可以直接用空白斷成「Power / outage / all / over /  Taiwan」，可見中文斷詞真的是一個大問題啊～\n\n這樣的問題其實已經有很多解法，比如中研院也有提供「[中文斷詞系統](http://ckipsvr.iis.sinica.edu.tw/)」，但就是很難用，不僅 API Call 的次數有限制，還很難串，Server 也常常掛掉，真不曉得為何中研院不將核心開源出來，讓大家可以一起來改善這種現象，總之我要棄中研院的斷詞系統而去了。\n\n近來玩了一下 [jieba](https://github.com/fxsjy/jieba) 結巴這個 Python Based 的開源中文斷詞程式，感覺大好，順手發了一些 pull request，今天早上就成為 [contributor](http://bit.ly/1pWOuzp) 了！ 感覺真爽！每次發 pull request 總是有種莫名的爽感，既期待被 merge 又怕被 reject，就跟告白的感覺類似啊～\n\n這麼好用的開源中文斷詞系統，當然要介紹給大家用啊！\n\n### 背後演算法\n\njieba 中文斷詞所使用的演算法是基於 Trie Tree 結構去生成句子中中文字所有可能成詞的情況，然後使用動態規劃（Dynamic programming）算法來找出最大機率的路徑，這個路徑就是基於詞頻的最大斷詞結果。對於辨識新詞（字典詞庫中不存在的詞）則使用了 HMM 模型（Hidden Markov Model）及 Viterbi 算法來辨識出來。基本上這樣就可以完成具有斷詞功能的程式了，或許我之後可以找個時間寫幾篇部落格來介紹這幾個演算法。\n\n### 如何安裝\n\n推薦用 pip 安裝 jieba 套件，或者使用 Virtualenv 安裝（未來可能會介紹如何使用 Virtualevn，這樣就可以同時在一台機器上跑不同的 Python 環境）：\n\n\tpip install jieba\n    \n### 基本斷詞用法，使用預設詞庫\n\nSample Code：\n\njieba-default-mode.py\n\n\t#encoding=utf-8\n\timport jieba\n\n\tsentence = \"獨立音樂需要大家一起來推廣，歡迎加入我們的行列！\"\n\tprint \"Input：\", sentence\n\twords = jieba.cut(sentence, cut_all=False)\n\tprint \"Output 精確模式 Full Mode：\"\n\tfor word in words:\n    \tprint word\n\n\tsentence = \"独立音乐需要大家一起来推广，欢迎加入我们的行列！\"\n\tprint \"Input：\", sentence\n\twords = jieba.cut(sentence, cut_all=False)\n\tprint \"Output 精確模式 Full Mode：\"\n\tfor word in words:\n    \tprint word\n        \n得到的斷詞結果會是：\n\t\n    獨立 / 音樂 / 需要 / 大家 / 一起 / 來 / 推廣 / ， / 歡迎 / 加入 / 我們 / 的 / 行列\n    \n    独立 / 音乐 / 需要 / 大家 / 一 / 起来 / 推广 / ， / 欢迎 / 加入 / 我们 / 的 / 行列\n    \n據原作者的說法，使用預設詞庫的話，繁體中文的斷詞結果應該會比較差，畢竟原來的詞庫是簡體中文，但在這個例子中，我感覺是繁體中文的斷詞結果比較好，這應該只是特例，我們接下來試試看中文歌詞的斷詞結果如何。\n\n### 中文歌詞斷詞，使用預設詞庫\n\n現在我們使用 [回聲樂團 - 座右銘](http://www.indievox.com/song/1) 的歌詞作為中文斷詞測試範例，歌詞我們先做成一個純文字檔，內容如下：\n\nlyric.txt\n\n\t我沒有心\n\t我沒有真實的自我\n\t我只有消瘦的臉孔\n\t所謂軟弱\n\t所謂的順從一向是我\n\t的座右銘\n\n\t而我\n\t沒有那海洋的寬闊\n\t我只要熱情的撫摸\n\t所謂空洞\n\t所謂不安全感是我\n\t的墓誌銘\n\n\t而你\n\t是否和我一般怯懦\n\t是否和我一般矯作\n\t和我一般囉唆\n\n\t而你\n\t是否和我一般退縮\n\t是否和我一般肌迫\n\t一般地困惑\n\n\t我沒有力\n\t我沒有滿腔的熱火\n\t我只有滿肚的如果\n\t所謂勇氣\n\t所謂的認同感是我\n\t隨便說說\n\n\t而你\n\t是否和我一般怯懦\n\t是否和我一般矯作\n\t是否對你來說\n\t只是一場遊戲\n\t雖然沒有把握\n\n\t而你\n\t是否和我一般退縮\n\t是否和我一般肌迫\n\t是否對你來說\n\t只是逼不得已\n\t雖然沒有藉口\n    \nSample Code：\n\njieba_cut_lyric.py\n\n\t#encoding=utf-8\n\timport jieba\n\n\tcontent = open('lyric.txt', 'rb').read()\n\n\tprint \"Input：\", content\n\n\twords = jieba.cut(content, cut_all=False)\n\n\tprint \"Output 精確模式 Full Mode：\"\n\tfor word in words:\n    \tprint word\n        \n得到的斷詞結果會是：\n\n\t我 / 沒 / 有心 / 我 / 沒 / 有 / 真實 / 的 / 自我 / 我 / 只有 / 消瘦 / 的 / 臉孔 / 所謂 / 軟弱 / 所謂 / 的 / 順 / 從 / 一向 / 是 / 我 / 的 / 座 / 右銘 / 而 / 我 / 沒有 / 那 / 海洋 / 的 / 寬闊 / 我 / 只要 / 熱情 / 的 / 撫 / 摸 / 所謂 / 空洞 / / 所謂 / 不安全感 / 是 / 我 / 的 / 墓誌 / 銘 / 而 / 你 / 是否 / 和 / 我 / 一般 / 怯懦 / 是否 / 和 / 我 / 一般 / 矯作 / 和 / 我 / 一般 / 囉 / 唆 / 而 / 你 / 是否 / 和 / 我 / 一般 / 退縮 / 是否 / 和 / 我 / 一般 / 肌迫 / 一般 / 地 / 困惑 / 我 / 沒 / 有力 / 我 / 沒 / 有 / 滿腔 / 的 / 熱火 / 我 / 只有 / 滿肚 / 的 / 如果 / 所謂 / 勇氣 / 所謂 / 的 / 認 / 同感 / 是 / 我 / 隨便 / 說 / 說 / 而 / 你 / 是否 / 和 / 我 / 一般 / 怯懦 / 是否 / 和 / 我 / 一般 / 矯作 / 是否 / 對 / 你 / 來 / 說 / 只是 / 一場 / 遊戲 / 雖然 / 沒 / 有把握 / 而 / 你 / 是否 / 和 / 我 / 一般 / 退縮 / 是否 / 和 / 我 / 一般 / 肌迫 / 是否 / 對 / 你 / 來 / 說 / 只是 / 逼不得已 / 雖然 / 沒有 / 藉口\n    \n我們可以從結果看出斷詞已經開始出了一些問題，比如**「座右銘」**被斷成了**「座 / 右銘」**，**「墓誌銘」**被斷成了**「墓誌 / 銘」**，這應該就是因為預設詞庫是簡體中文所造成，因此繁體中文的斷詞結果會比較差，還好 jieba 也提供了可以切換詞庫的功能，並提供了一個繁體中文詞庫，所以我們可以使用切換詞庫的功能來改善斷詞結果。\n\n### 中文歌詞斷詞，使用繁體詞庫\n\nSample Code：\n\njieba_cut_lyric_zh.py\n\n\t#encoding=utf-8\n\timport jieba\n    \n    jieba.set_dictionary('dict.txt.big')\n\n\tcontent = open('lyric.txt', 'rb').read()\n\n\tprint \"Input：\", content\n\n\twords = jieba.cut(content, cut_all=False)\n\n\tprint \"Output 精確模式 Full Mode：\"\n\tfor word in words:\n    \tprint word\n        \n我們在程式中多加一行 <code>jieba.set_dictionary('dict.txt.big')</code>，這樣就可以將斷詞詞庫切換到 dic.txt.big 這個檔案。\n\n得到的斷詞結果會是：\n\n\t我 / 沒有 / 心 / 我 / 沒有 / 真實 / 的 / 自我 / 我 / 只有 / 消瘦 / 的 / 臉孔 / 所謂 / 軟弱 / 所謂 / 的 / 順從 / 一向 / 是 / 我 / 的 / 座右銘 / 而 / 我 / 沒有 / 那 / 海洋 / 的 / 寬闊 / 我 / 只要 / 熱情 / 的 / 撫摸 / 所謂 / 空洞 / 所謂 / 不安全感 / 是 / 我 / 的 / 墓誌銘 / 而 / 你 / 是否 / 和 / 我 / 一般 / 怯懦 / 是否 / 和 / 我 / 一般 / 矯作 / 和 / 我 / 一般 / 囉唆 / 而 / 你 / 是否 / 和 / 我 / 一般 / 退縮 / 是否 / 和 / 我 / 一般 / 肌迫 / 一般 / 地 / 困惑 / 我 / 沒有 / 力 / 我 / 沒有 / 滿腔 / 的 / 熱火 / 我 / 只有 / 滿肚 / 的 / 如果 / 所謂 / 勇氣 / 所謂 / 的 / 認同感 / 是 / 我 / 隨便說說 / 而 / 你 / 是否 / 和 / 我 / 一般 / 怯懦 / 是否 / 和 / 我 / 一般 / 矯作 / 是否 / 對 / 你 / 來說 / 只是 / 一場 / 遊戲 / 雖然 / 沒有 / 把握 / 而 / 你 / 是否 / 和 / 我 / 一般 / 退縮 / 是否 / 和 / 我 / 一般 / 肌迫 / 是否 / 對 / 你 / 來說 / 只是 / 逼不得已 / 雖然 / 沒有 / 藉口\n    \n我們可以看到**「座右銘」**成功斷成**「座右銘」**了！**「墓誌銘」**也成功斷成**「墓誌銘」**了！果然切換成繁體中文詞庫還是有用的！\n\n### 台語歌詞斷詞，使用繁體詞庫\n\n既然中文歌詞斷詞能夠得到不錯的斷詞結果了，那我們來試試看台語歌詞斷詞會是如何？在這邊我們使用 [滅火器 - 島嶼天光](http://www.indievox.com/song/73716) 的歌詞作為台語斷詞測試範例，歌詞我們先做成一個純文字檔，內容如下：\n\nlyric_tw.txt\n\n\t親愛的媽媽\n\t請你毋通煩惱我\n\t原諒我\n\t行袂開跤\n\t我欲去對抗袂當原諒的人\n\n\t歹勢啦\n\t愛人啊\n\t袂當陪你去看電影\n\t原諒我\n\t行袂開跤\n\t我欲去對抗欺負咱的人\n\n\t天色漸漸光\n\t遮有一陣人\n\t為了守護咱的夢\n\t成做更加勇敢的人\n\n\t天色漸漸光\n\t已經不再驚惶\n\t現在就是彼一工\n\t換阮做守護恁的人\n\n\t已經袂記\n\t是第幾工\n\t請毋通煩惱我\n\t因為阮知道\n\t無行過寒冬\n\t袂有花開的一工\n\n\t天色漸漸光\n\t天色漸漸光\n\t已經是更加勇敢的人\n\n\t天色漸漸光\n\t咱就大聲來唱著歌\n\t一直到希望的光線\n\t照光島嶼每一個人\n\n\t天色漸漸光\n\t咱就大聲來唱著歌\n\t日頭一爬上山\n\t就會使轉去啦\n\t現在是彼一工\n\t勇敢的台灣人\n    \nSample Code：\n\njieba_cut_lyric_zh_tw.py\n\n\t#encoding=utf-8\n\timport jieba\n\n\tjieba.set_dictionary('dict.txt.big')\n\n\tcontent = open('lyric_tw.txt', 'rb').read()\n\n\tprint \"Input：\", content\n\n\twords = jieba.cut(content, cut_all=False)\n\n\tprint \"Output 精確模式 Full Mode：\"\n\tfor word in words:\n    \tprint word\n    \n得到的斷詞結果會是：\n\n\t親愛 / 的 / 媽媽 / 請 / 你 / 毋通 / 煩惱 / 我 / 原諒 / 我 / 行袂 / 開跤 / 我 / 欲 / 去 / 對抗 / 袂 / 當 / 原諒 / 的 / 人 / 歹勢 / 啦 / 愛人 / 啊 / 袂 / 當 / 陪你去 / 看 / 電影 / 原諒 / 我 / 行袂 / 開跤 / 我 / 欲 / 去 / 對抗 / 欺負 / 咱 / 的 / 人 / 天色 / 漸漸 / 光 / 遮有 / 一陣 / 人 / 為 / 了 / 守護 / 咱 / 的 / 夢 / 成 / 做 / 更加 / 勇敢的人 / 天色 / 漸漸 / 光 / 已經 / 不再 / 驚惶 / 現在 / 就是 / 彼一工 / 換阮 / 做 / 守護 / 恁 / 的 / 人 / 已經 / 袂 / 記 / 是 / 第幾 / 工 / 請 / 毋通 / 煩惱 / 我 / 因為 / 阮 / 知道 / 無行過 / 寒冬 / 袂 / 有 / 花開 / 的 / 一工 / 天色 / 漸漸 / 光 / 天色 / 漸漸 / 光 / 已經 / 是 / 更加 / 勇敢的人 / 天色 / 漸漸 / 光 / 咱 / 就 / 大聲 / 來 / 唱 / 著歌 / 一直 / 到 / 希望 / 的 / 光線 / 照光 / 島嶼 / 每 / 一個 / 人 / 天色 / 漸漸 / 光 / 咱 / 就 / 大聲 / 來 / 唱 / 著歌 / 日頭 / 一爬 / 上山 / 就 / 會 / 使 / 轉去 / 啦 / 現在 / 是 / 彼 / 一工 / 勇敢 / 的 / 台灣 / 人\n    \n原本猜想結果應該會蠻差的，畢竟詞庫中沒有台語的用詞，但是因為 HMM 的關係猜出了一些新詞，讓我們還是得到不錯的結果，**「袂當」**斷成了**「袂」「當」**，**「袂記」**斷成了**「袂」「記」**，**「袂有」**斷成了**「袂」「有」**等等，我們要如何改善這些結果呢？\n\njieba 提供了一個功能讓使用者可以增加自定義詞庫，這種無法用 HMM 判斷出來的新詞就可以得到改善，我們就來試試看吧！\n\n### 台語歌詞斷詞，使用繁體詞庫加自定義詞庫\n\n首先我們新增一個純文字檔建立自定義詞庫，格式如下：\n\nuserdict.txt\n\n\t行袂開跤 2 v\n\t袂當 4 d\n\t袂記 4 v\n\t袂有 4 d\n\t唱著 4 v\n\t每一個 4 m\n\t會使 70 d\n\n其中每一行代表一筆語料資料，首先填上自定義詞如：**「袂當」**、**「袂記」**，然後填上權重，權重值可以依照斷詞結果做自己想做的調整，最後填上詞性，但詞性非必要填寫，詞性列表可以參考 [词性对照说明.中科院版本](https://github.com/ansjsun/ansj_seg/wiki/%E8%AF%8D%E6%80%A7%E5%AF%B9%E7%85%A7%E8%AF%B4%E6%98%8E.%E4%B8%AD%E7%A7%91%E9%99%A2%E7%89%88%E6%9C%AC)。\n\nSample Code：\n\njieba_cut_lyric_zh_tw_custom.py\n\n\t#encoding=utf-8\n\timport jieba\n\n\tjieba.set_dictionary('dict.txt.big')\n    jieba.load_userdict(\"userdict.txt\")\n\n\tcontent = open('lyric_tw.txt', 'rb').read()\n\n\tprint \"Input：\", content\n\n\twords = jieba.cut(content, cut_all=False)\n\n\tprint \"Output 精確模式 Full Mode：\"\n\tfor word in words:\n    \tprint word\n        \n我們在程式中多加一行 <code>jieba.load_userdict(\"userdict.txt\")</code>，這樣就可以將自定義詞庫加進來了，超級簡單的。\n\n得到的斷詞結果會是：\n\n\t親愛 / 的 / 媽媽 / 請 / 你 / 毋通 / 煩惱 / 我 / 原諒 / 我 / 行袂開跤 / 我 / 欲 / 去 / 對抗 / 袂當 / 原諒 / 的 / 人 / 歹勢 / 啦 / 愛人 / 啊 / 袂當 / 陪你去 / 看 / 電影 / 原諒 / 我 / 行袂開跤 / 我 / 欲 / 去 / 對抗 / 欺負 / 咱 / 的 / 人 / 天色 / 漸漸 / 光 / 遮有 / 一陣 / 人 / 為 / 了 / 守護 / 咱 / 的 / 夢 / 成 / 做 / 更加 / 勇敢的人 / 天色 / 漸漸 / 光 / 已經 / 不再 / 驚惶 / 現在 / 就是 / 彼一工 / 換阮 / 做 / 守護 / 恁 / 的 / 人 / 已經 / 袂記 / 是 / 第幾 / 工 / 請 / 毋通 / 煩惱 / 我 / 因為 / 阮 / 知道 / 無行過 / 寒冬 / 袂有 / 花開 / 的 / 一工 / 天色 / 漸漸 / 光 / 天色 / 漸漸 / 光 / 已經 / 是 / 更加 / 勇敢的人 / 天色 / 漸漸 / 光 / 咱 / 就 / 大聲 / 來 / 唱著 / 歌 / 一直 / 到 / 希望 / 的 / 光線 / 照光 / 島嶼 / 每 / 一個 / 人 / 天色 / 漸漸 / 光 / 咱 / 就 / 大聲 / 來 / 唱著 / 歌 / 日頭 / 一爬 / 上山 / 就 / 會使 / 轉去 / 啦 / 現在 / 是 / 彼 / 一工 / 勇敢 / 的 / 台灣 / 人\n    \n完美！\n\n### 取出斷詞詞性\n\n大部份的斷詞系統都可以列出斷詞的詞性，jieba 也有這個功能，但結果可能不是那麼好，這其實是跟所使用的語料庫有關係，不過既然是 Open Source，希望未來能有語言學家可以加入，讓 jieba 可以得到更好的效果。\n\nSample Code：\n\njieba_cut_lyric_zh_flag.py\n\n\t#encoding=utf-8\n\timport jieba\n\timport jieba.posseg as pseg\n\n\tjieba.set_dictionary('dict.txt.big')\n\n\tcontent = open('lyric.txt', 'rb').read()\n\n\tprint \"Input：\", content\n\n\twords = pseg.cut(content)\n\n\tprint \"Output 精確模式 Full Mode：\"\n\tfor word in words:\n    \tprint word.word, word.flag\n        \n得到的結果會是：\n\n\t我 r\n\t沒有 x\n\t心 n\n    \n\t我 r\n\t沒有 x\n\t真實 x\n\t的 uj\n\t自我 r\n    \n    ...\n    \n### 取出斷詞位置\n\n有時我們會需要得到斷詞在文章中的位置：\n    \nSample Code：\n\njieba_cut_lyric_zh_tokenize.py\n\n\t#encoding=utf-8\n\timport jieba\n\n\tjieba.set_dictionary('dict.txt.big')\n\n\tcontent = open('lyric.txt', 'rb').read()\n\n\tprint \"Input：\", content\n\n\twords = jieba.tokenize(unicode(content, 'utf-8'))\n\n\tprint \"Output 精確模式 Full Mode：\"\n\tfor tk in words:\n    \tprint \"word %s\\t\\t start: %d \\t\\t end:%d\" % (tk[0],tk[1],tk[2])\n        \n得到的結果會是：\n\n\tword 我 start: 0 end:1\n\tword 沒有 start: 1 end:3\n\tword 心 start: 3 end:4\n\tword start: 4 end:5\n\tword 我 start: 5 end:6\n\tword 沒有 start: 6 end:8\n\tword 真實 start: 8 end:10\n\tword 的 start: 10 end:11\n\tword 自我 start: 11 end:13\n    \n    ...\n\n### 取出文章中的關鍵詞\n\njieba 使用了 tf-idf 方法來實作萃取出文章中關鍵詞的功能：\n\nSample Code：\n\njieba_cut_lyric_zh_keyword.py\n\n\t#encoding=utf-8\n\timport jieba\n\timport jieba.analyse\n\n\tjieba.set_dictionary('dict.txt.big')\n\n\tcontent = open('lyric.txt', 'rb').read()\n\n\tprint \"Input：\", content\n\n\ttags = jieba.analyse.extract_tags(content, 10)\n\n\tprint \"Output：\"\n\tprint \",\".join(tags)\n    \n程式中的 <code>jieba.analyse.extract_tags(content, 10)</code>，就是告訴 jieba 我們要從這個文章中取出前 10 個 tf-idf 值最大的關鍵詞。\n\n得到的結果會是：\n\n\t沒有,所謂,是否,一般,雖然,退縮,肌迫,矯作,來說,怯懦\n    \n一開始使用這個功能的時候，會不知道 jieba 的 idf 值是從哪裡來的，看了一下 souce code 才知道原來 jieba 有提供一個 idf 的語料庫，但在實務上每個人所使用的語料庫可能會不太一樣，有時我們會想要使用自己的idf 語料庫，stop words 的語料庫也可能會想換成自己的，比如目前的結果中，最重要的「座右銘」並沒有出現在關鍵詞裡，我就會想要將「座右銘」加到 idf 語料庫，並讓 idf 值高一點，而「沒有」這個關鍵詞對我來說是沒有用的，我就會想把它加到 stop words 語料庫，這樣「沒有」就不會出現在關鍵詞裡。\n\n可惜目前 pip 安裝的 jieba 版本並不能切換 idf 及 stop words 語料庫，所以我才會修改了一下 jieba，讓它可以支援 idf 及 stop words 語料庫的切換，目前在 github 上的版本已經可以支援 idf 及 stop words 切換的功能了！\n\n### 結語\n\n使用了 jieba 之後，其實有蠻深的感嘆，其實中研院的斷詞核心必非不好，想要收費也不是問題，但是 API 做得這麼差，根本就沒人有信心敢花錢下去使用這樣不可靠的系統，目前又有 jieba 這樣的 open source project，中研院的斷詞系統前途堪慮啊！\n\t","html":"<h3 id=\"\">前言</h3>\n\n<p>自然語言處理的其中一個重要環節就是中文斷詞的處理，比起英文斷詞，中文斷詞在先天上就比較難處理，比如電腦要怎麼知道「全台大停電」要斷詞成「全台 / 大 / 停電」呢？如果是英文「Power outage all over Taiwan」，就可以直接用空白斷成「Power / outage / all / over /  Taiwan」，可見中文斷詞真的是一個大問題啊～</p>\n\n<p>這樣的問題其實已經有很多解法，比如中研院也有提供「<a href=\"http://ckipsvr.iis.sinica.edu.tw/\">中文斷詞系統</a>」，但就是很難用，不僅 API Call 的次數有限制，還很難串，Server 也常常掛掉，真不曉得為何中研院不將核心開源出來，讓大家可以一起來改善這種現象，總之我要棄中研院的斷詞系統而去了。</p>\n\n<p>近來玩了一下 <a href=\"https://github.com/fxsjy/jieba\">jieba</a> 結巴這個 Python Based 的開源中文斷詞程式，感覺大好，順手發了一些 pull request，今天早上就成為 <a href=\"http://bit.ly/1pWOuzp\">contributor</a> 了！ 感覺真爽！每次發 pull request 總是有種莫名的爽感，既期待被 merge 又怕被 reject，就跟告白的感覺類似啊～</p>\n\n<p>這麼好用的開源中文斷詞系統，當然要介紹給大家用啊！</p>\n\n<h3 id=\"\">背後演算法</h3>\n\n<p>jieba 中文斷詞所使用的演算法是基於 Trie Tree 結構去生成句子中中文字所有可能成詞的情況，然後使用動態規劃（Dynamic programming）算法來找出最大機率的路徑，這個路徑就是基於詞頻的最大斷詞結果。對於辨識新詞（字典詞庫中不存在的詞）則使用了 HMM 模型（Hidden Markov Model）及 Viterbi 算法來辨識出來。基本上這樣就可以完成具有斷詞功能的程式了，或許我之後可以找個時間寫幾篇部落格來介紹這幾個演算法。</p>\n\n<h3 id=\"\">如何安裝</h3>\n\n<p>推薦用 pip 安裝 jieba 套件，或者使用 Virtualenv 安裝（未來可能會介紹如何使用 Virtualevn，這樣就可以同時在一台機器上跑不同的 Python 環境）：</p>\n\n<pre><code>pip install jieba\n</code></pre>\n\n<h3 id=\"\">基本斷詞用法，使用預設詞庫</h3>\n\n<p>Sample Code：</p>\n\n<p>jieba-default-mode.py</p>\n\n<pre><code>#encoding=utf-8\nimport jieba\n\nsentence = \"獨立音樂需要大家一起來推廣，歡迎加入我們的行列！\"\nprint \"Input：\", sentence\nwords = jieba.cut(sentence, cut_all=False)\nprint \"Output 精確模式 Full Mode：\"\nfor word in words:\n    print word\n\nsentence = \"独立音乐需要大家一起来推广，欢迎加入我们的行列！\"\nprint \"Input：\", sentence\nwords = jieba.cut(sentence, cut_all=False)\nprint \"Output 精確模式 Full Mode：\"\nfor word in words:\n    print word\n</code></pre>\n\n<p>得到的斷詞結果會是：</p>\n\n<pre><code>獨立 / 音樂 / 需要 / 大家 / 一起 / 來 / 推廣 / ， / 歡迎 / 加入 / 我們 / 的 / 行列\n\n独立 / 音乐 / 需要 / 大家 / 一 / 起来 / 推广 / ， / 欢迎 / 加入 / 我们 / 的 / 行列\n</code></pre>\n\n<p>據原作者的說法，使用預設詞庫的話，繁體中文的斷詞結果應該會比較差，畢竟原來的詞庫是簡體中文，但在這個例子中，我感覺是繁體中文的斷詞結果比較好，這應該只是特例，我們接下來試試看中文歌詞的斷詞結果如何。</p>\n\n<h3 id=\"\">中文歌詞斷詞，使用預設詞庫</h3>\n\n<p>現在我們使用 <a href=\"http://www.indievox.com/song/1\">回聲樂團 - 座右銘</a> 的歌詞作為中文斷詞測試範例，歌詞我們先做成一個純文字檔，內容如下：</p>\n\n<p>lyric.txt</p>\n\n<pre><code>我沒有心\n我沒有真實的自我\n我只有消瘦的臉孔\n所謂軟弱\n所謂的順從一向是我\n的座右銘\n\n而我\n沒有那海洋的寬闊\n我只要熱情的撫摸\n所謂空洞\n所謂不安全感是我\n的墓誌銘\n\n而你\n是否和我一般怯懦\n是否和我一般矯作\n和我一般囉唆\n\n而你\n是否和我一般退縮\n是否和我一般肌迫\n一般地困惑\n\n我沒有力\n我沒有滿腔的熱火\n我只有滿肚的如果\n所謂勇氣\n所謂的認同感是我\n隨便說說\n\n而你\n是否和我一般怯懦\n是否和我一般矯作\n是否對你來說\n只是一場遊戲\n雖然沒有把握\n\n而你\n是否和我一般退縮\n是否和我一般肌迫\n是否對你來說\n只是逼不得已\n雖然沒有藉口\n</code></pre>\n\n<p>Sample Code：</p>\n\n<p>jieba_cut_lyric.py</p>\n\n<pre><code>#encoding=utf-8\nimport jieba\n\ncontent = open('lyric.txt', 'rb').read()\n\nprint \"Input：\", content\n\nwords = jieba.cut(content, cut_all=False)\n\nprint \"Output 精確模式 Full Mode：\"\nfor word in words:\n    print word\n</code></pre>\n\n<p>得到的斷詞結果會是：</p>\n\n<pre><code>我 / 沒 / 有心 / 我 / 沒 / 有 / 真實 / 的 / 自我 / 我 / 只有 / 消瘦 / 的 / 臉孔 / 所謂 / 軟弱 / 所謂 / 的 / 順 / 從 / 一向 / 是 / 我 / 的 / 座 / 右銘 / 而 / 我 / 沒有 / 那 / 海洋 / 的 / 寬闊 / 我 / 只要 / 熱情 / 的 / 撫 / 摸 / 所謂 / 空洞 / / 所謂 / 不安全感 / 是 / 我 / 的 / 墓誌 / 銘 / 而 / 你 / 是否 / 和 / 我 / 一般 / 怯懦 / 是否 / 和 / 我 / 一般 / 矯作 / 和 / 我 / 一般 / 囉 / 唆 / 而 / 你 / 是否 / 和 / 我 / 一般 / 退縮 / 是否 / 和 / 我 / 一般 / 肌迫 / 一般 / 地 / 困惑 / 我 / 沒 / 有力 / 我 / 沒 / 有 / 滿腔 / 的 / 熱火 / 我 / 只有 / 滿肚 / 的 / 如果 / 所謂 / 勇氣 / 所謂 / 的 / 認 / 同感 / 是 / 我 / 隨便 / 說 / 說 / 而 / 你 / 是否 / 和 / 我 / 一般 / 怯懦 / 是否 / 和 / 我 / 一般 / 矯作 / 是否 / 對 / 你 / 來 / 說 / 只是 / 一場 / 遊戲 / 雖然 / 沒 / 有把握 / 而 / 你 / 是否 / 和 / 我 / 一般 / 退縮 / 是否 / 和 / 我 / 一般 / 肌迫 / 是否 / 對 / 你 / 來 / 說 / 只是 / 逼不得已 / 雖然 / 沒有 / 藉口\n</code></pre>\n\n<p>我們可以從結果看出斷詞已經開始出了一些問題，比如<strong>「座右銘」</strong>被斷成了<strong>「座 / 右銘」</strong>，<strong>「墓誌銘」</strong>被斷成了<strong>「墓誌 / 銘」</strong>，這應該就是因為預設詞庫是簡體中文所造成，因此繁體中文的斷詞結果會比較差，還好 jieba 也提供了可以切換詞庫的功能，並提供了一個繁體中文詞庫，所以我們可以使用切換詞庫的功能來改善斷詞結果。</p>\n\n<h3 id=\"\">中文歌詞斷詞，使用繁體詞庫</h3>\n\n<p>Sample Code：</p>\n\n<p>jieba_cut_lyric_zh.py</p>\n\n<pre><code>#encoding=utf-8\nimport jieba\n\njieba.set_dictionary('dict.txt.big')\n\ncontent = open('lyric.txt', 'rb').read()\n\nprint \"Input：\", content\n\nwords = jieba.cut(content, cut_all=False)\n\nprint \"Output 精確模式 Full Mode：\"\nfor word in words:\n    print word\n</code></pre>\n\n<p>我們在程式中多加一行 <code>jieba.set_dictionary('dict.txt.big')</code>，這樣就可以將斷詞詞庫切換到 dic.txt.big 這個檔案。</p>\n\n<p>得到的斷詞結果會是：</p>\n\n<pre><code>我 / 沒有 / 心 / 我 / 沒有 / 真實 / 的 / 自我 / 我 / 只有 / 消瘦 / 的 / 臉孔 / 所謂 / 軟弱 / 所謂 / 的 / 順從 / 一向 / 是 / 我 / 的 / 座右銘 / 而 / 我 / 沒有 / 那 / 海洋 / 的 / 寬闊 / 我 / 只要 / 熱情 / 的 / 撫摸 / 所謂 / 空洞 / 所謂 / 不安全感 / 是 / 我 / 的 / 墓誌銘 / 而 / 你 / 是否 / 和 / 我 / 一般 / 怯懦 / 是否 / 和 / 我 / 一般 / 矯作 / 和 / 我 / 一般 / 囉唆 / 而 / 你 / 是否 / 和 / 我 / 一般 / 退縮 / 是否 / 和 / 我 / 一般 / 肌迫 / 一般 / 地 / 困惑 / 我 / 沒有 / 力 / 我 / 沒有 / 滿腔 / 的 / 熱火 / 我 / 只有 / 滿肚 / 的 / 如果 / 所謂 / 勇氣 / 所謂 / 的 / 認同感 / 是 / 我 / 隨便說說 / 而 / 你 / 是否 / 和 / 我 / 一般 / 怯懦 / 是否 / 和 / 我 / 一般 / 矯作 / 是否 / 對 / 你 / 來說 / 只是 / 一場 / 遊戲 / 雖然 / 沒有 / 把握 / 而 / 你 / 是否 / 和 / 我 / 一般 / 退縮 / 是否 / 和 / 我 / 一般 / 肌迫 / 是否 / 對 / 你 / 來說 / 只是 / 逼不得已 / 雖然 / 沒有 / 藉口\n</code></pre>\n\n<p>我們可以看到<strong>「座右銘」</strong>成功斷成<strong>「座右銘」</strong>了！<strong>「墓誌銘」</strong>也成功斷成<strong>「墓誌銘」</strong>了！果然切換成繁體中文詞庫還是有用的！</p>\n\n<h3 id=\"\">台語歌詞斷詞，使用繁體詞庫</h3>\n\n<p>既然中文歌詞斷詞能夠得到不錯的斷詞結果了，那我們來試試看台語歌詞斷詞會是如何？在這邊我們使用 <a href=\"http://www.indievox.com/song/73716\">滅火器 - 島嶼天光</a> 的歌詞作為台語斷詞測試範例，歌詞我們先做成一個純文字檔，內容如下：</p>\n\n<p>lyric_tw.txt</p>\n\n<pre><code>親愛的媽媽\n請你毋通煩惱我\n原諒我\n行袂開跤\n我欲去對抗袂當原諒的人\n\n歹勢啦\n愛人啊\n袂當陪你去看電影\n原諒我\n行袂開跤\n我欲去對抗欺負咱的人\n\n天色漸漸光\n遮有一陣人\n為了守護咱的夢\n成做更加勇敢的人\n\n天色漸漸光\n已經不再驚惶\n現在就是彼一工\n換阮做守護恁的人\n\n已經袂記\n是第幾工\n請毋通煩惱我\n因為阮知道\n無行過寒冬\n袂有花開的一工\n\n天色漸漸光\n天色漸漸光\n已經是更加勇敢的人\n\n天色漸漸光\n咱就大聲來唱著歌\n一直到希望的光線\n照光島嶼每一個人\n\n天色漸漸光\n咱就大聲來唱著歌\n日頭一爬上山\n就會使轉去啦\n現在是彼一工\n勇敢的台灣人\n</code></pre>\n\n<p>Sample Code：</p>\n\n<p>jieba_cut_lyric_zh_tw.py</p>\n\n<pre><code>#encoding=utf-8\nimport jieba\n\njieba.set_dictionary('dict.txt.big')\n\ncontent = open('lyric_tw.txt', 'rb').read()\n\nprint \"Input：\", content\n\nwords = jieba.cut(content, cut_all=False)\n\nprint \"Output 精確模式 Full Mode：\"\nfor word in words:\n    print word\n</code></pre>\n\n<p>得到的斷詞結果會是：</p>\n\n<pre><code>親愛 / 的 / 媽媽 / 請 / 你 / 毋通 / 煩惱 / 我 / 原諒 / 我 / 行袂 / 開跤 / 我 / 欲 / 去 / 對抗 / 袂 / 當 / 原諒 / 的 / 人 / 歹勢 / 啦 / 愛人 / 啊 / 袂 / 當 / 陪你去 / 看 / 電影 / 原諒 / 我 / 行袂 / 開跤 / 我 / 欲 / 去 / 對抗 / 欺負 / 咱 / 的 / 人 / 天色 / 漸漸 / 光 / 遮有 / 一陣 / 人 / 為 / 了 / 守護 / 咱 / 的 / 夢 / 成 / 做 / 更加 / 勇敢的人 / 天色 / 漸漸 / 光 / 已經 / 不再 / 驚惶 / 現在 / 就是 / 彼一工 / 換阮 / 做 / 守護 / 恁 / 的 / 人 / 已經 / 袂 / 記 / 是 / 第幾 / 工 / 請 / 毋通 / 煩惱 / 我 / 因為 / 阮 / 知道 / 無行過 / 寒冬 / 袂 / 有 / 花開 / 的 / 一工 / 天色 / 漸漸 / 光 / 天色 / 漸漸 / 光 / 已經 / 是 / 更加 / 勇敢的人 / 天色 / 漸漸 / 光 / 咱 / 就 / 大聲 / 來 / 唱 / 著歌 / 一直 / 到 / 希望 / 的 / 光線 / 照光 / 島嶼 / 每 / 一個 / 人 / 天色 / 漸漸 / 光 / 咱 / 就 / 大聲 / 來 / 唱 / 著歌 / 日頭 / 一爬 / 上山 / 就 / 會 / 使 / 轉去 / 啦 / 現在 / 是 / 彼 / 一工 / 勇敢 / 的 / 台灣 / 人\n</code></pre>\n\n<p>原本猜想結果應該會蠻差的，畢竟詞庫中沒有台語的用詞，但是因為 HMM 的關係猜出了一些新詞，讓我們還是得到不錯的結果，<strong>「袂當」</strong>斷成了<strong>「袂」「當」</strong>，<strong>「袂記」</strong>斷成了<strong>「袂」「記」</strong>，<strong>「袂有」</strong>斷成了<strong>「袂」「有」</strong>等等，我們要如何改善這些結果呢？</p>\n\n<p>jieba 提供了一個功能讓使用者可以增加自定義詞庫，這種無法用 HMM 判斷出來的新詞就可以得到改善，我們就來試試看吧！</p>\n\n<h3 id=\"\">台語歌詞斷詞，使用繁體詞庫加自定義詞庫</h3>\n\n<p>首先我們新增一個純文字檔建立自定義詞庫，格式如下：</p>\n\n<p>userdict.txt</p>\n\n<pre><code>行袂開跤 2 v\n袂當 4 d\n袂記 4 v\n袂有 4 d\n唱著 4 v\n每一個 4 m\n會使 70 d\n</code></pre>\n\n<p>其中每一行代表一筆語料資料，首先填上自定義詞如：<strong>「袂當」</strong>、<strong>「袂記」</strong>，然後填上權重，權重值可以依照斷詞結果做自己想做的調整，最後填上詞性，但詞性非必要填寫，詞性列表可以參考 <a href=\"https://github.com/ansjsun/ansj_seg/wiki/%E8%AF%8D%E6%80%A7%E5%AF%B9%E7%85%A7%E8%AF%B4%E6%98%8E.%E4%B8%AD%E7%A7%91%E9%99%A2%E7%89%88%E6%9C%AC\">词性对照说明.中科院版本</a>。</p>\n\n<p>Sample Code：</p>\n\n<p>jieba_cut_lyric_zh_tw_custom.py</p>\n\n<pre><code>#encoding=utf-8\nimport jieba\n\njieba.set_dictionary('dict.txt.big')\njieba.load_userdict(\"userdict.txt\")\n\ncontent = open('lyric_tw.txt', 'rb').read()\n\nprint \"Input：\", content\n\nwords = jieba.cut(content, cut_all=False)\n\nprint \"Output 精確模式 Full Mode：\"\nfor word in words:\n    print word\n</code></pre>\n\n<p>我們在程式中多加一行 <code>jieba.load_userdict(\"userdict.txt\")</code>，這樣就可以將自定義詞庫加進來了，超級簡單的。</p>\n\n<p>得到的斷詞結果會是：</p>\n\n<pre><code>親愛 / 的 / 媽媽 / 請 / 你 / 毋通 / 煩惱 / 我 / 原諒 / 我 / 行袂開跤 / 我 / 欲 / 去 / 對抗 / 袂當 / 原諒 / 的 / 人 / 歹勢 / 啦 / 愛人 / 啊 / 袂當 / 陪你去 / 看 / 電影 / 原諒 / 我 / 行袂開跤 / 我 / 欲 / 去 / 對抗 / 欺負 / 咱 / 的 / 人 / 天色 / 漸漸 / 光 / 遮有 / 一陣 / 人 / 為 / 了 / 守護 / 咱 / 的 / 夢 / 成 / 做 / 更加 / 勇敢的人 / 天色 / 漸漸 / 光 / 已經 / 不再 / 驚惶 / 現在 / 就是 / 彼一工 / 換阮 / 做 / 守護 / 恁 / 的 / 人 / 已經 / 袂記 / 是 / 第幾 / 工 / 請 / 毋通 / 煩惱 / 我 / 因為 / 阮 / 知道 / 無行過 / 寒冬 / 袂有 / 花開 / 的 / 一工 / 天色 / 漸漸 / 光 / 天色 / 漸漸 / 光 / 已經 / 是 / 更加 / 勇敢的人 / 天色 / 漸漸 / 光 / 咱 / 就 / 大聲 / 來 / 唱著 / 歌 / 一直 / 到 / 希望 / 的 / 光線 / 照光 / 島嶼 / 每 / 一個 / 人 / 天色 / 漸漸 / 光 / 咱 / 就 / 大聲 / 來 / 唱著 / 歌 / 日頭 / 一爬 / 上山 / 就 / 會使 / 轉去 / 啦 / 現在 / 是 / 彼 / 一工 / 勇敢 / 的 / 台灣 / 人\n</code></pre>\n\n<p>完美！</p>\n\n<h3 id=\"\">取出斷詞詞性</h3>\n\n<p>大部份的斷詞系統都可以列出斷詞的詞性，jieba 也有這個功能，但結果可能不是那麼好，這其實是跟所使用的語料庫有關係，不過既然是 Open Source，希望未來能有語言學家可以加入，讓 jieba 可以得到更好的效果。</p>\n\n<p>Sample Code：</p>\n\n<p>jieba_cut_lyric_zh_flag.py</p>\n\n<pre><code>#encoding=utf-8\nimport jieba\nimport jieba.posseg as pseg\n\njieba.set_dictionary('dict.txt.big')\n\ncontent = open('lyric.txt', 'rb').read()\n\nprint \"Input：\", content\n\nwords = pseg.cut(content)\n\nprint \"Output 精確模式 Full Mode：\"\nfor word in words:\n    print word.word, word.flag\n</code></pre>\n\n<p>得到的結果會是：</p>\n\n<pre><code>我 r\n沒有 x\n心 n\n\n我 r\n沒有 x\n真實 x\n的 uj\n自我 r\n\n...\n</code></pre>\n\n<h3 id=\"\">取出斷詞位置</h3>\n\n<p>有時我們會需要得到斷詞在文章中的位置：</p>\n\n<p>Sample Code：</p>\n\n<p>jieba_cut_lyric_zh_tokenize.py</p>\n\n<pre><code>#encoding=utf-8\nimport jieba\n\njieba.set_dictionary('dict.txt.big')\n\ncontent = open('lyric.txt', 'rb').read()\n\nprint \"Input：\", content\n\nwords = jieba.tokenize(unicode(content, 'utf-8'))\n\nprint \"Output 精確模式 Full Mode：\"\nfor tk in words:\n    print \"word %s\\t\\t start: %d \\t\\t end:%d\" % (tk[0],tk[1],tk[2])\n</code></pre>\n\n<p>得到的結果會是：</p>\n\n<pre><code>word 我 start: 0 end:1\nword 沒有 start: 1 end:3\nword 心 start: 3 end:4\nword start: 4 end:5\nword 我 start: 5 end:6\nword 沒有 start: 6 end:8\nword 真實 start: 8 end:10\nword 的 start: 10 end:11\nword 自我 start: 11 end:13\n\n...\n</code></pre>\n\n<h3 id=\"\">取出文章中的關鍵詞</h3>\n\n<p>jieba 使用了 tf-idf 方法來實作萃取出文章中關鍵詞的功能：</p>\n\n<p>Sample Code：</p>\n\n<p>jieba_cut_lyric_zh_keyword.py</p>\n\n<pre><code>#encoding=utf-8\nimport jieba\nimport jieba.analyse\n\njieba.set_dictionary('dict.txt.big')\n\ncontent = open('lyric.txt', 'rb').read()\n\nprint \"Input：\", content\n\ntags = jieba.analyse.extract_tags(content, 10)\n\nprint \"Output：\"\nprint \",\".join(tags)\n</code></pre>\n\n<p>程式中的 <code>jieba.analyse.extract_tags(content, 10)</code>，就是告訴 jieba 我們要從這個文章中取出前 10 個 tf-idf 值最大的關鍵詞。</p>\n\n<p>得到的結果會是：</p>\n\n<pre><code>沒有,所謂,是否,一般,雖然,退縮,肌迫,矯作,來說,怯懦\n</code></pre>\n\n<p>一開始使用這個功能的時候，會不知道 jieba 的 idf 值是從哪裡來的，看了一下 souce code 才知道原來 jieba 有提供一個 idf 的語料庫，但在實務上每個人所使用的語料庫可能會不太一樣，有時我們會想要使用自己的idf 語料庫，stop words 的語料庫也可能會想換成自己的，比如目前的結果中，最重要的「座右銘」並沒有出現在關鍵詞裡，我就會想要將「座右銘」加到 idf 語料庫，並讓 idf 值高一點，而「沒有」這個關鍵詞對我來說是沒有用的，我就會想把它加到 stop words 語料庫，這樣「沒有」就不會出現在關鍵詞裡。</p>\n\n<p>可惜目前 pip 安裝的 jieba 版本並不能切換 idf 及 stop words 語料庫，所以我才會修改了一下 jieba，讓它可以支援 idf 及 stop words 語料庫的切換，目前在 github 上的版本已經可以支援 idf 及 stop words 切換的功能了！</p>\n\n<h3 id=\"\">結語</h3>\n\n<p>使用了 jieba 之後，其實有蠻深的感嘆，其實中研院的斷詞核心必非不好，想要收費也不是問題，但是 API 做得這麼差，根本就沒人有信心敢花錢下去使用這樣不可靠的系統，目前又有 jieba 這樣的 open source project，中研院的斷詞系統前途堪慮啊！</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-08-06T11:23:04.000Z","created_by":1,"updated_at":"2014-08-06T14:01:12.000Z","updated_by":1,"published_at":"2014-08-06T13:49:55.000Z","published_by":1},{"id":35,"uuid":"e0fb5139-a625-4795-a5f4-918461cb3043","title":"Laravel 學習筆記 Lesson 2","slug":"laravel-xue-xi-bi-ji-lesson-2","markdown":"### 設定 Laravel App\n\n剛裝好 Laravel，其實 Laravel 就可以正常運作了，但有一些基本設定還是先設定好，這樣可以避免後來遇到一些問題。\n\n首先修改 `app/config/app.php` 檔案：\n\n```language-php\n'timezone' => 'UTC'\n```\n\n改為：\n\t\n```language-php\n'timezone' => 'Asia/Taipei'\n```\n\n另外在開發中如果希望可以看到完整的 Debug 內容，可以將 Debug 打開：\n\n```language-php\n'debug' => false\n```\n\n改為：\n\n```language-php\n'debug' => true\n```\n\n### 設定 Laravel 資料庫\n\n我們需要設定好資料庫的連線資訊，Laravel 預設使用 MySQL 資料庫，所以我們先修改 `app/config/database.php` 檔案中 MySQL 的連線資訊部分：\n\n```language-php\n'mysql' => array(\n\t'driver'    => 'mysql',\n\t'host'      => 'localhost',\n\t'database'  => 'database_name',\n\t'username'  => 'database_username',\n\t'password'  => 'database_password',\n\t'charset'   => 'utf8',\n\t'collation' => 'utf8_unicode_ci',\n\t'prefix'    => '',\n)\n```\n    \n將 `host、database、username、password` 改成所使用的資料庫相關資訊。\n\n\n### 設定 Laravel Routing 初探\n\n使用者如何進入 Laravel App 的設定都定義在 `app/routes.php` 中，一安裝好 Laravel 時，`app/routes.php` 就已經有定義一條規則：\n\n```language-php\nRoute::get('/', function()\n{\n\treturn View::make('hello');\n});\n```\n\n這個意思是，當使用者輸入 `/` 這個網址時，Laravel 就會回應 hello 這個 View 給使用者。\n\n通常我們都會希望除了 `/` 這個網址可以進入首頁之外，`/index.html` 也會希望可以正常進入首頁，所以我們就要自己加入這個規則：\n\n```language-php\nRoute::get('/', function()\n{\n\treturn View::make('hello');\n});\n    \nRoute::get('/index.html', function()\n{\n\treturn View::make('hello');\n});\n```\n    \n### 使用 Laravel Controller\n\n目前我們並沒有使用到 Controller，而是直接將程式內容塞在每個 Routing Rule 裡面，將來網站功能變多，`routes.php` 的內容就會變得很龐大，這樣並不是一個好的架構。\n\n所以我們需要將功能邏輯的部分改寫在 Controller 裡，例如：\n\n```language-php\nRoute::get('/', 'HomeController@getIndex');\n```\n\n如此， `/` 這個網址的程式功能及邏輯就可以分離至 `HomeController` 中的 static method `getIndex` 了。\n\n然後我們在 `app/controller/HomeController.php` 裡加上 `getIndex` 的內容，如下：\n\n```language-php\nclass HomeController extends BaseController {\n\n\tpublic function getIndex()\n\t{\n\t\treturn View::make('index');\n\t}\n\n}\n```\n\n### 範例網頁使用 bootstrap、fontawesome\n\nController 中寫的 `View::make('index')` 意思就是會去將 `app/views/index.blade.php` 這個網頁內容包含進來，所以我們需要新增 `app/views/index.blade.php` 這個網頁，我們會使用 bootstrap 及 fontawesome 來製作我們的範例網頁：\n\n```language-markup\t\n<!DOCTYPE html>\n<html lang=\"en\">\n    <head>\n        <meta charset=\"utf-8\">\n        <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n        <meta name=\"description\" content=\"\">\n        <meta name=\"author\" content=\"\">\n        <link rel=\"icon\" href=\"/image/favicon.ico\">\n        <title>Titel</title>\n        <!-- Bootstrap core CSS -->\n        <link rel=\"stylesheet\" href=\"//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css\">\n        <link href=\"//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css\" rel=\"stylesheet\">\n        <!-- Custom styles for this template -->\n        <link href=\"/stylesheet/pinhere.css\" rel=\"stylesheet\">\n        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->\n        <!--[if lt IE 9]>\n        <script src=\"https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js\"></script>\n        <script src=\"https://oss.maxcdn.com/respond/1.4.2/respond.min.js\"></script>\n        <![endif]-->\n    </head>\n    <body>\n    \t<div class=\"navbar navbar-inverse navbar-fixed-top\" role=\"navigation\">\n        \t<div class=\"container\">\n            \t<div class=\"navbar-header\">\n                \t<button type=\"button\" class=\"navbar-toggle\" data-toggle=\"collapse\" data-target=\".navbar-collapse\">\n                    \t<span class=\"sr-only\">Toggle navigation</span>\n                    \t<span class=\"icon-bar\"></span>\n                    \t<span class=\"icon-bar\"></span>\n                    \t<span class=\"icon-bar\"></span>\n                \t</button>\n                \t<a class=\"navbar-brand\" href=\"#\">Project name</a>\n            \t</div>\n            \t<div class=\"collapse navbar-collapse\">\n                \t<ul class=\"nav navbar-nav\">\n                    \t<li class=\"active\"><a href=\"#\">Home</a></li>\n                    \t<li><a href=\"#about\">About</a></li>\n                    \t<li><a href=\"#contact\">Contact</a></li>\n                \t</ul>\n            \t</div><!--/.nav-collapse -->\n        \t</div>\n    \t</div>\n    \t<div class=\"container\">\n        \t<div class=\"starter-template\">\n        \t\t<h1>Bootstrap starter template</h1>\n        \t\t<p class=\"lead\">Use this document as a way to quickly start any new project.<br> All you get is this text and a mostly barebones HTML document.</p>\n    \t\t</div>\n    \t</div><!-- /.container -->\n    \t<!-- Bootstrap core JavaScript\n    ================================================== -->\n    \t<!-- Placed at the end of the document so the pages load faster -->\n    \t<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js\"></script>\n    \t<!-- Latest compiled and minified JavaScript -->\n    \t<script src=\"//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js\"></script>\n    </body>\n</html>\n```\n\n### 將 layout 及頁面內容分開來\n\n由於許多頁面我們都會用到相同的排版佈局，比如可能上面都有 header，所以我們可以將這樣的佈局抽離開來，讓每個網頁都可以繼承同樣的排版佈局。首先我們新增 `app/views/layouts/layout.blade.php` 這個檔案，內容如下：\n\n```language-markup\n<!DOCTYPE html>\n<html lang=\"en\">\n    <head>\n        <meta charset=\"utf-8\">\n        <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n        <meta name=\"description\" content=\"\">\n        <meta name=\"author\" content=\"\">\n        <link rel=\"icon\" href=\"/image/favicon.ico\">\n        <title>Title</title>\n        <!-- Bootstrap core CSS -->\n        <link rel=\"stylesheet\" href=\"//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css\">\n        <link href=\"//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css\" rel=\"stylesheet\">\n        <!-- Custom styles for this template -->\n        <link href=\"/stylesheet/pinhere.css\" rel=\"stylesheet\">\n        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->\n        <!--[if lt IE 9]>\n        <script src=\"https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js\"></script>\n        <script src=\"https://oss.maxcdn.com/respond/1.4.2/respond.min.js\"></script>\n        <![endif]-->\n    </head>\n    <body>\n    <div class=\"navbar navbar-inverse navbar-fixed-top\" role=\"navigation\">\n        <div class=\"container\">\n            <div class=\"navbar-header\">\n                <button type=\"button\" class=\"navbar-toggle\" data-toggle=\"collapse\" data-target=\".navbar-collapse\">\n                    <span class=\"sr-only\">Toggle navigation</span>\n                    <span class=\"icon-bar\"></span>\n                    <span class=\"icon-bar\"></span>\n                    <span class=\"icon-bar\"></span>\n                </button>\n                <a class=\"navbar-brand\" href=\"#\">Project name</a>\n            </div>\n            <div class=\"collapse navbar-collapse\">\n                <ul class=\"nav navbar-nav\">\n                    <li class=\"active\"><a href=\"#\">Home</a></li>\n                    <li><a href=\"#about\">About</a></li>\n                    <li><a href=\"#contact\">Contact</a></li>\n                </ul>\n            </div><!--/.nav-collapse -->\n        </div>\n    </div>\n    <div class=\"container\">\n        @yield('content')\n    </div><!-- /.container -->\n    <!-- Bootstrap core JavaScript\n    ================================================== -->\n    <!-- Placed at the end of the document so the pages load faster -->\n    <script src=\"https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js\"></script>\n    <!-- Latest compiled and minified JavaScript -->\n    <script src=\"//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js\"></script>\n    </body>\n</html>\n```\n\n我們將每個網頁會改變的部分改成 `@yield('content')`，如此原來的 `app/views/index.blade.php` 內容就可以改成如下：\n\n```language-php\n@extends('layouts.layout')\n\n@section('content')\n    <div class=\"starter-template\">\n        <h1>Bootstrap starter template</h1>\n        <p class=\"lead\">Use this document as a way to quickly start any new project.<br> All you get is this text and a mostly barebones HTML document.</p>\n    </div>\n@stop\n```\n\n我們可以直接繼承 `app/views/layouts/layout.blade.php` 的佈局架構，並把 `@yield('content')` 換成 `@section('content')` 中間的內容，將 layout 及頁面內容分開來我們可以更方便地修改 View 的內容。","html":"<h3 id=\"laravelapp\">設定 Laravel App</h3>\n\n<p>剛裝好 Laravel，其實 Laravel 就可以正常運作了，但有一些基本設定還是先設定好，這樣可以避免後來遇到一些問題。</p>\n\n<p>首先修改 <code>app/config/app.php</code> 檔案：</p>\n\n<pre><code class=\"language-php\">'timezone' =&gt; 'UTC'  \n</code></pre>\n\n<p>改為：</p>\n\n<pre><code class=\"language-php\">'timezone' =&gt; 'Asia/Taipei'  \n</code></pre>\n\n<p>另外在開發中如果希望可以看到完整的 Debug 內容，可以將 Debug 打開：</p>\n\n<pre><code class=\"language-php\">'debug' =&gt; false  \n</code></pre>\n\n<p>改為：</p>\n\n<pre><code class=\"language-php\">'debug' =&gt; true  \n</code></pre>\n\n<h3 id=\"laravel\">設定 Laravel 資料庫</h3>\n\n<p>我們需要設定好資料庫的連線資訊，Laravel 預設使用 MySQL 資料庫，所以我們先修改 <code>app/config/database.php</code> 檔案中 MySQL 的連線資訊部分：</p>\n\n<pre><code class=\"language-php\">'mysql' =&gt; array(  \n    'driver'    =&gt; 'mysql',\n    'host'      =&gt; 'localhost',\n    'database'  =&gt; 'database_name',\n    'username'  =&gt; 'database_username',\n    'password'  =&gt; 'database_password',\n    'charset'   =&gt; 'utf8',\n    'collation' =&gt; 'utf8_unicode_ci',\n    'prefix'    =&gt; '',\n)\n</code></pre>\n\n<p>將 <code>host、database、username、password</code> 改成所使用的資料庫相關資訊。</p>\n\n<h3 id=\"laravelrouting\">設定 Laravel Routing 初探</h3>\n\n<p>使用者如何進入 Laravel App 的設定都定義在 <code>app/routes.php</code> 中，一安裝好 Laravel 時，<code>app/routes.php</code> 就已經有定義一條規則：</p>\n\n<pre><code class=\"language-php\">Route::get('/', function()  \n{\n    return View::make('hello');\n});\n</code></pre>\n\n<p>這個意思是，當使用者輸入 <code>/</code> 這個網址時，Laravel 就會回應 hello 這個 View 給使用者。</p>\n\n<p>通常我們都會希望除了 <code>/</code> 這個網址可以進入首頁之外，<code>/index.html</code> 也會希望可以正常進入首頁，所以我們就要自己加入這個規則：</p>\n\n<pre><code class=\"language-php\">Route::get('/', function()  \n{\n    return View::make('hello');\n});\n\nRoute::get('/index.html', function()  \n{\n    return View::make('hello');\n});\n</code></pre>\n\n<h3 id=\"laravelcontroller\">使用 Laravel Controller</h3>\n\n<p>目前我們並沒有使用到 Controller，而是直接將程式內容塞在每個 Routing Rule 裡面，將來網站功能變多，<code>routes.php</code> 的內容就會變得很龐大，這樣並不是一個好的架構。</p>\n\n<p>所以我們需要將功能邏輯的部分改寫在 Controller 裡，例如：</p>\n\n<pre><code class=\"language-php\">Route::get('/', 'HomeController@getIndex');  \n</code></pre>\n\n<p>如此， <code>/</code> 這個網址的程式功能及邏輯就可以分離至 <code>HomeController</code> 中的 static method <code>getIndex</code> 了。</p>\n\n<p>然後我們在 <code>app/controller/HomeController.php</code> 裡加上 <code>getIndex</code> 的內容，如下：</p>\n\n<pre><code class=\"language-php\">class HomeController extends BaseController {\n\n    public function getIndex()\n    {\n        return View::make('index');\n    }\n\n}\n</code></pre>\n\n<h3 id=\"bootstrapfontawesome\">範例網頁使用 bootstrap、fontawesome</h3>\n\n<p>Controller 中寫的 <code>View::make('index')</code> 意思就是會去將 <code>app/views/index.blade.php</code> 這個網頁內容包含進來，所以我們需要新增 <code>app/views/index.blade.php</code> 這個網頁，我們會使用 bootstrap 及 fontawesome 來製作我們的範例網頁：</p>\n\n<pre><code class=\"language-markup    \">&lt;!DOCTYPE html&gt;  \n&lt;html lang=\"en\"&gt;  \n    &lt;head&gt;\n        &lt;meta charset=\"utf-8\"&gt;\n        &lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"&gt;\n        &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"&gt;\n        &lt;meta name=\"description\" content=\"\"&gt;\n        &lt;meta name=\"author\" content=\"\"&gt;\n        &lt;link rel=\"icon\" href=\"/image/favicon.ico\"&gt;\n        &lt;title&gt;Titel&lt;/title&gt;\n        &lt;!-- Bootstrap core CSS --&gt;\n        &lt;link rel=\"stylesheet\" href=\"//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css\"&gt;\n        &lt;link href=\"//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css\" rel=\"stylesheet\"&gt;\n        &lt;!-- Custom styles for this template --&gt;\n        &lt;link href=\"/stylesheet/pinhere.css\" rel=\"stylesheet\"&gt;\n        &lt;!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries --&gt;\n        &lt;!--[if lt IE 9]&gt;\n        &lt;script src=\"https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js\"&gt;&lt;/script&gt;\n        &lt;script src=\"https://oss.maxcdn.com/respond/1.4.2/respond.min.js\"&gt;&lt;/script&gt;\n        &lt;![endif]--&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;div class=\"navbar navbar-inverse navbar-fixed-top\" role=\"navigation\"&gt;\n            &lt;div class=\"container\"&gt;\n                &lt;div class=\"navbar-header\"&gt;\n                    &lt;button type=\"button\" class=\"navbar-toggle\" data-toggle=\"collapse\" data-target=\".navbar-collapse\"&gt;\n                        &lt;span class=\"sr-only\"&gt;Toggle navigation&lt;/span&gt;\n                        &lt;span class=\"icon-bar\"&gt;&lt;/span&gt;\n                        &lt;span class=\"icon-bar\"&gt;&lt;/span&gt;\n                        &lt;span class=\"icon-bar\"&gt;&lt;/span&gt;\n                    &lt;/button&gt;\n                    &lt;a class=\"navbar-brand\" href=\"#\"&gt;Project name&lt;/a&gt;\n                &lt;/div&gt;\n                &lt;div class=\"collapse navbar-collapse\"&gt;\n                    &lt;ul class=\"nav navbar-nav\"&gt;\n                        &lt;li class=\"active\"&gt;&lt;a href=\"#\"&gt;Home&lt;/a&gt;&lt;/li&gt;\n                        &lt;li&gt;&lt;a href=\"#about\"&gt;About&lt;/a&gt;&lt;/li&gt;\n                        &lt;li&gt;&lt;a href=\"#contact\"&gt;Contact&lt;/a&gt;&lt;/li&gt;\n                    &lt;/ul&gt;\n                &lt;/div&gt;&lt;!--/.nav-collapse --&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n        &lt;div class=\"container\"&gt;\n            &lt;div class=\"starter-template\"&gt;\n                &lt;h1&gt;Bootstrap starter template&lt;/h1&gt;\n                &lt;p class=\"lead\"&gt;Use this document as a way to quickly start any new project.&lt;br&gt; All you get is this text and a mostly barebones HTML document.&lt;/p&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;&lt;!-- /.container --&gt;\n        &lt;!-- Bootstrap core JavaScript\n    ================================================== --&gt;\n        &lt;!-- Placed at the end of the document so the pages load faster --&gt;\n        &lt;script src=\"https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js\"&gt;&lt;/script&gt;\n        &lt;!-- Latest compiled and minified JavaScript --&gt;\n        &lt;script src=\"//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js\"&gt;&lt;/script&gt;\n    &lt;/body&gt;\n&lt;/html&gt;  \n</code></pre>\n\n<h3 id=\"layout\">將 layout 及頁面內容分開來</h3>\n\n<p>由於許多頁面我們都會用到相同的排版佈局，比如可能上面都有 header，所以我們可以將這樣的佈局抽離開來，讓每個網頁都可以繼承同樣的排版佈局。首先我們新增 <code>app/views/layouts/layout.blade.php</code> 這個檔案，內容如下：</p>\n\n<pre><code class=\"language-markup\">&lt;!DOCTYPE html&gt;  \n&lt;html lang=\"en\"&gt;  \n    &lt;head&gt;\n        &lt;meta charset=\"utf-8\"&gt;\n        &lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"&gt;\n        &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"&gt;\n        &lt;meta name=\"description\" content=\"\"&gt;\n        &lt;meta name=\"author\" content=\"\"&gt;\n        &lt;link rel=\"icon\" href=\"/image/favicon.ico\"&gt;\n        &lt;title&gt;Title&lt;/title&gt;\n        &lt;!-- Bootstrap core CSS --&gt;\n        &lt;link rel=\"stylesheet\" href=\"//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css\"&gt;\n        &lt;link href=\"//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css\" rel=\"stylesheet\"&gt;\n        &lt;!-- Custom styles for this template --&gt;\n        &lt;link href=\"/stylesheet/pinhere.css\" rel=\"stylesheet\"&gt;\n        &lt;!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries --&gt;\n        &lt;!--[if lt IE 9]&gt;\n        &lt;script src=\"https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js\"&gt;&lt;/script&gt;\n        &lt;script src=\"https://oss.maxcdn.com/respond/1.4.2/respond.min.js\"&gt;&lt;/script&gt;\n        &lt;![endif]--&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n    &lt;div class=\"navbar navbar-inverse navbar-fixed-top\" role=\"navigation\"&gt;\n        &lt;div class=\"container\"&gt;\n            &lt;div class=\"navbar-header\"&gt;\n                &lt;button type=\"button\" class=\"navbar-toggle\" data-toggle=\"collapse\" data-target=\".navbar-collapse\"&gt;\n                    &lt;span class=\"sr-only\"&gt;Toggle navigation&lt;/span&gt;\n                    &lt;span class=\"icon-bar\"&gt;&lt;/span&gt;\n                    &lt;span class=\"icon-bar\"&gt;&lt;/span&gt;\n                    &lt;span class=\"icon-bar\"&gt;&lt;/span&gt;\n                &lt;/button&gt;\n                &lt;a class=\"navbar-brand\" href=\"#\"&gt;Project name&lt;/a&gt;\n            &lt;/div&gt;\n            &lt;div class=\"collapse navbar-collapse\"&gt;\n                &lt;ul class=\"nav navbar-nav\"&gt;\n                    &lt;li class=\"active\"&gt;&lt;a href=\"#\"&gt;Home&lt;/a&gt;&lt;/li&gt;\n                    &lt;li&gt;&lt;a href=\"#about\"&gt;About&lt;/a&gt;&lt;/li&gt;\n                    &lt;li&gt;&lt;a href=\"#contact\"&gt;Contact&lt;/a&gt;&lt;/li&gt;\n                &lt;/ul&gt;\n            &lt;/div&gt;&lt;!--/.nav-collapse --&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n    &lt;div class=\"container\"&gt;\n        @yield('content')\n    &lt;/div&gt;&lt;!-- /.container --&gt;\n    &lt;!-- Bootstrap core JavaScript\n    ================================================== --&gt;\n    &lt;!-- Placed at the end of the document so the pages load faster --&gt;\n    &lt;script src=\"https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js\"&gt;&lt;/script&gt;\n    &lt;!-- Latest compiled and minified JavaScript --&gt;\n    &lt;script src=\"//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js\"&gt;&lt;/script&gt;\n    &lt;/body&gt;\n&lt;/html&gt;  \n</code></pre>\n\n<p>我們將每個網頁會改變的部分改成 <code>@yield('content')</code>，如此原來的 <code>app/views/index.blade.php</code> 內容就可以改成如下：</p>\n\n<pre><code class=\"language-php\">@extends('layouts.layout')\n\n@section('content')\n    &lt;div class=\"starter-template\"&gt;\n        &lt;h1&gt;Bootstrap starter template&lt;/h1&gt;\n        &lt;p class=\"lead\"&gt;Use this document as a way to quickly start any new project.&lt;br&gt; All you get is this text and a mostly barebones HTML document.&lt;/p&gt;\n    &lt;/div&gt;\n@stop\n</code></pre>\n\n<p>我們可以直接繼承 <code>app/views/layouts/layout.blade.php</code> 的佈局架構，並把 <code>@yield('content')</code> 換成 <code>@section('content')</code> 中間的內容，將 layout 及頁面內容分開來我們可以更方便地修改 View 的內容。</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-08-13T20:17:32.000Z","created_by":1,"updated_at":"2014-08-20T19:30:15.000Z","updated_by":1,"published_at":"2014-08-19T11:19:28.000Z","published_by":1},{"id":36,"uuid":"fa7bfe7c-cd9b-4501-936b-cf6e7e914af9","title":"Laravel 學習筆記 Lesson 3","slug":"laravel-xue-xi-bi-ji-lesson-3","markdown":"### 如何新增資料表及 Model - 以 Users 為例\n\n我們使用 Laravel 的遷移(migration)系統來建立資料表以保存我們的資料。遷移記錄著資料庫的改變歷程，這讓團隊成員間的資訊分享更為簡單。\n\n接下來，我們來創建遷移檔，我們使用 Artisan CLI。在專案的根目錄下，在終端裡執行下列指令：\n\n```language-bash\nphp artisan migrate:make create_users_table\n```\n\n然後，在 `app/database/migrations` 目錄下找到產生的遷移檔。檔案中有一個包含了兩個方法 `up` 和 `down` 的類別。在 `up` 方法中，你必須表明你要對你的資料表做哪些更動，而在 `down` 的方法裡，你只要回復這些更動。\n\n我們定義一個遷移檔如下：\n\n```language-php\n<?php\n\nuse Illuminate\\Database\\Schema\\Blueprint;\nuse Illuminate\\Database\\Migrations\\Migration;\n\nclass CreateUsersTable extends Migration {\n\n\t/**\n\t * Run the migrations.\n\t *\n\t * @return void\n\t */\n\tpublic function up()\n\t{\n\t\tSchema::create('users', function($table)\n\t    {\n\t        $table->increments('id');\n\t        $table->string('email')->unique();\n\t        $table->string('name');\n\t        $table->string('password', 60);\n\t        $table->string('remember_token', 100);\n\t        $table->timestamps();\n\t    });\n\t}\n\n\t/**\n\t * Reverse the migrations.\n\t *\n\t * @return void\n\t */\n\tpublic function down()\n\t{\n\t\tSchema::drop('users');\n\t}\n\n}\n```\n\n然後我們從終端裡透過 migrate 指令來執行遷移動作。在專案的根目錄裡執行下列指令：\n\n```language-bash\nphp artisan migrate --force\n```\n\n這樣我們就可以建好 Users 資料表了，開始放些資料進去吧。\n\n### 新增 User Model Class\n\n當建立好資料表，我們需要在 `app/models` 底下新增一個資料表所對應的 Model Class，通常 users 資料表，我們會新增一個命名為 User 的 Model Class，檔名為 `app/models/User.php`，資料表用小寫複數命名，Model 用單數首字母駝峰式命名，Model 內容如下：\n\n```language-php\n<?php\n\nuse Illuminate\\Auth\\UserTrait;\nuse Illuminate\\Auth\\UserInterface;\nuse Illuminate\\Auth\\Reminders\\RemindableTrait;\nuse Illuminate\\Auth\\Reminders\\RemindableInterface;\n\nclass User extends Eloquent implements UserInterface, RemindableInterface {\n\n\tuse UserTrait, RemindableTrait;\n\n\t/**\n\t * The database table used by the model.\n\t *\n\t * @var string\n\t */\n\tprotected $table = 'users';\n\n\t/**\n\t * The attributes excluded from the model's JSON form.\n\t *\n\t * @var array\n\t */\n\tprotected $hidden = array('password', 'remember_token');\n\n}\n\n```\n\n### 製作表單\n\n新增一個新的 Routing 規則：\n\n```language-php\nRoute::get('/user/create-form', 'UserController@getCreateForm');\n```\n\n實作 `UserController` 中的 `getCreateForm` 方法：\n\n```language-php\npublic function getCreateForm()\n{\n\n\treturn View::make('user.user-create-form');\n\n};\n```\n\n然後新增 `app/views/user/user-create-form.blade.php`，內容如下：\n\n```language-php\n@extends('layouts.layout')\n\n@section('content')\n<?php\n\n$old_name = '';\n$old_email = '';\n$old_password = '';\n\nif (!empty(Input::old())) {\n    $old_name = Input::old('input_name');\n    $old_email = Input::old('input_email');\n    $old_password = Input::old('input_password');\n}\n\n?>\n<div class=\"starter-template\">\n    @if ($errors->has())\n        <div class=\"alert alert-danger\" role=\"alert\">\n        @foreach ($errors->all() as $error)\n            {{ $error }}<br/>\n        @endforeach\n        </div>\n    @endif\n    <form class=\"form-horizontal\" role=\"form\" method=\"post\" action=\"/user/create\">\n        {{ Form::token() }}\n        <div class=\"form-group @if ($errors->has('name')) has-error @endif\">\n            <label for=\"input_name\" class=\"col-sm-2 control-label\">\n                Name\n            </label>\n            <div class=\"col-sm-10\">\n                <input type=\"text\" class=\"form-control\" id=\"input_name\" name=\"input_name\" placeholder=\"Name\" value=\"{{ $old_name }}\">\n                @if ($errors->has('name')) <p class=\"help-block\">{{ $errors->first('name') }}</p> @endif\n            </div>\n        </div>\n        <div class=\"form-group @if ($errors->has('email')) has-error @endif\">\n            <label for=\"input_email\" class=\"col-sm-2 control-label\">\n                Email\n            </label>\n            <div class=\"col-sm-10\">\n                <input type=\"email\" class=\"form-control\" id=\"input_email\" name=\"input_email\" placeholder=\"Email\" value=\"{{ $old_email }}\">\n                @if ($errors->has('email')) <p class=\"help-block\">{{ $errors->first('email') }}</p> @endif\n            </div>\n        </div>\n        <div class=\"form-group @if ($errors->has('password')) has-error @endif\">\n            <label for=\"input_password\" class=\"col-sm-2 control-label\">\n                Password\n            </label>\n            <div class=\"col-sm-10\">\n                <input type=\"password\" class=\"form-control\" id=\"input_password\" name=\"input_password\" placeholder=\"Password\" value=\"{{ $old_password }}\">\n                @if ($errors->has('password')) <p class=\"help-block\">{{ $errors->first('password') }}</p> @endif\n            </div>\n        </div>\n        <div class=\"form-group\">\n            <div class=\"col-sm-offset-2 col-sm-10\">\n                <button type=\"submit\" class=\"btn btn-default\">\n                    Create\n                </button>\n            </div>\n        </div>\n    </form>\n</div>\n@stop\n```\n\n### 接收表單資料\n\n由於在 `app/views/user/user-create-form.blade.php` 的表單所指定的 action url 是 post 到 `/user/create`，所以我們要新增 Routing 規則：\n\n```language-php\nRoute::post('/user/create', array('before' => 'csrf', 'uses' => 'UserController@create'));\n```\n\n實作 `UserController` 中的 `create` 方法：\n\n```language-php\npublic function create() {\n\n    $input           = Input::all();\n    $input_name      = $input['input_name'];\n    $input_email     = $input['input_email'];\n    $input_password  = $input['input_password'];\n    $hashed_password = Hash::make($input_password);\n\n    $validator = Validator::make(\n        array(\n            'name' => $input_name,\n            'email' => $input_email,\n            'password' => $input_password\n        ),\n        array(\n            'name' => 'required',\n            'email' => 'required|email|unique:users',\n            'password' => 'required|min:8'\n        )\n    );\n\n    if ($validator->fails()) {\n\n        // get the error messages from the validator\n        $messages = $validator->messages();\n\n        // redirect our user back to the form with the errors from the validator\n        return Redirect::to('/user/create-form')\n                ->withInput()\n                ->withErrors($validator);\n\n    } else {\n\n        $user           = new User;\n        $user->name     = $input_name;\n        $user->email    = $input_email;\n        $user->password = $hashed_password;\n        $user->save();\n\n        return Redirect::to('/user/list');\n\n    }\n\n}\n```\n\n### csrf 驗證\n\nCSRF/XSRF 全名 Cross Site Request Forgery 中文翻成「跨網站偽造請求」，而 CSRF 主要的攻擊行為就是利用當使用者合法取得網站使用認證後，透過某些方式偽造網站合法使用者的身份進行非法的存取動作，合法使用者即可能在不自覺的情況下被引導到駭客的攻擊網頁。\n\n在現今的 Web Framework 幾乎都有很完善的 CSRF 驗證解決方案來解決 CSRF 的攻擊，Laravel 也不例外。\n\n首先我們可以在 `app/routes.php` 中的 Routing Rules 中指定是否使用 CSRF 驗證，如：\n\n```language-php\nRoute::post('/user/create', array('before' => 'csrf', 'uses' => 'UserController@create'));\n```\n\n當我們有指定要驗證 CSRF 時，Laravel 在進入 `UserController@create` 前就會先驗證 CSRF Token 是否合法，這個動作 Laravel 實作在 `app/filters.php`，內容如下：\n\n```language-php\nRoute::filter('csrf', function()\n{\n\tif (Session::token() != Input::get('_token'))\n\t{\n\t\tthrow new Illuminate\\Session\\TokenMismatchException;\n\t}\n});\n```\n\n我們可以在這邊修改 CSRF Token 驗證的動作。\n\n由於 Laravel 在進入 `UserController@create` 前就會先驗證 CSRF Token 是否合法，我們需要在表單中傳入合法的 Token，產生合法 Token 的方法如下：\n\n```language-php\n{{ Form::token() }}\n```\n\n這樣 Laravel 就會自動在表單中插入含有合法 Token 的隱藏 input 標籤。\n\n### 後端表單驗證\n\nLaravel 有很方便且易讀的資料驗證方法，例如：\n\n```language-php\n$validator = Validator::make(\n    array(\n        'name' => $input_name,\n        'email' => $input_email,\n        'password' => $input_password\n    ),\n    array(\n        'name' => 'required',\n        'email' => 'required|email|unique:users',\n        'password' => 'required|min:8'\n    )\n);\n```\n\n第一個 array 是放要驗證的資料值，有 name, email, password 需要驗證，第二個 array 則是 name, email, password 分別要符合哪些條件，比如 name 是 required，不能是空值，email 是 required 而且格式要符合 email 且在 users 表單中是唯一值，password 的條件則是 required 且最少要 8 個字元。\n\n如此我們就可以很方便地驗證使用者輸入的資料，並且可讀性也非常好。\n\n如果驗證失敗了，我們也可以將錯誤及提示回傳至頁面讓使用者知道：\n\n```language-php\nif ($validator->fails()) {\n\n    // get the error messages from the validator\n    $messages = $validator->messages();\n\n    // redirect our user back to the form with the errors from the validator\n    return Redirect::to('/user/create-form')\n            ->withInput()\n            ->withErrors($validator);\n\n}\n```\n\n如此我們就可以在 `/user/create-form` 用以下的方式取得錯誤訊息及之前使用者輸入的值：\n\n```language-php\nif (!empty(Input::old())) {\n    $old_name = Input::old('input_name');\n    $old_email = Input::old('input_email');\n    $old_password = Input::old('input_password');\n}\n\n@foreach ($errors->all() as $error)\n    {{ $error }}<br/>\n@endforeach\n```\n\n\n","html":"<h3 id=\"modelusers\">如何新增資料表及 Model - 以 Users 為例</h3>\n\n<p>我們使用 Laravel 的遷移(migration)系統來建立資料表以保存我們的資料。遷移記錄著資料庫的改變歷程，這讓團隊成員間的資訊分享更為簡單。</p>\n\n<p>接下來，我們來創建遷移檔，我們使用 Artisan CLI。在專案的根目錄下，在終端裡執行下列指令：</p>\n\n<pre><code class=\"language-bash\">php artisan migrate:make create_users_table  \n</code></pre>\n\n<p>然後，在 <code>app/database/migrations</code> 目錄下找到產生的遷移檔。檔案中有一個包含了兩個方法 <code>up</code> 和 <code>down</code> 的類別。在 <code>up</code> 方法中，你必須表明你要對你的資料表做哪些更動，而在 <code>down</code> 的方法裡，你只要回復這些更動。</p>\n\n<p>我們定義一個遷移檔如下：</p>\n\n<pre><code class=\"language-php\">&lt;?php\n\nuse Illuminate\\Database\\Schema\\Blueprint;  \nuse Illuminate\\Database\\Migrations\\Migration;\n\nclass CreateUsersTable extends Migration {\n\n    /**\n     * Run the migrations.\n     *\n     * @return void\n     */\n    public function up()\n    {\n        Schema::create('users', function($table)\n        {\n            $table-&gt;increments('id');\n            $table-&gt;string('email')-&gt;unique();\n            $table-&gt;string('name');\n            $table-&gt;string('password', 60);\n            $table-&gt;string('remember_token', 100);\n            $table-&gt;timestamps();\n        });\n    }\n\n    /**\n     * Reverse the migrations.\n     *\n     * @return void\n     */\n    public function down()\n    {\n        Schema::drop('users');\n    }\n\n}\n</code></pre>\n\n<p>然後我們從終端裡透過 migrate 指令來執行遷移動作。在專案的根目錄裡執行下列指令：</p>\n\n<pre><code class=\"language-bash\">php artisan migrate --force  \n</code></pre>\n\n<p>這樣我們就可以建好 Users 資料表了，開始放些資料進去吧。</p>\n\n<h3 id=\"usermodelclass\">新增 User Model Class</h3>\n\n<p>當建立好資料表，我們需要在 <code>app/models</code> 底下新增一個資料表所對應的 Model Class，通常 users 資料表，我們會新增一個命名為 User 的 Model Class，檔名為 <code>app/models/User.php</code>，資料表用小寫複數命名，Model 用單數首字母駝峰式命名，Model 內容如下：</p>\n\n<pre><code class=\"language-php\">&lt;?php\n\nuse Illuminate\\Auth\\UserTrait;  \nuse Illuminate\\Auth\\UserInterface;  \nuse Illuminate\\Auth\\Reminders\\RemindableTrait;  \nuse Illuminate\\Auth\\Reminders\\RemindableInterface;\n\nclass User extends Eloquent implements UserInterface, RemindableInterface {\n\n    use UserTrait, RemindableTrait;\n\n    /**\n     * The database table used by the model.\n     *\n     * @var string\n     */\n    protected $table = 'users';\n\n    /**\n     * The attributes excluded from the model's JSON form.\n     *\n     * @var array\n     */\n    protected $hidden = array('password', 'remember_token');\n\n}\n</code></pre>\n\n<h3 id=\"\">製作表單</h3>\n\n<p>新增一個新的 Routing 規則：</p>\n\n<pre><code class=\"language-php\">Route::get('/user/create-form', 'UserController@getCreateForm');  \n</code></pre>\n\n<p>實作 <code>UserController</code> 中的 <code>getCreateForm</code> 方法：</p>\n\n<pre><code class=\"language-php\">public function getCreateForm()  \n{\n\n    return View::make('user.user-create-form');\n\n};\n</code></pre>\n\n<p>然後新增 <code>app/views/user/user-create-form.blade.php</code>，內容如下：</p>\n\n<pre><code class=\"language-php\">@extends('layouts.layout')\n\n@section('content')\n&lt;?php\n\n$old_name = '';\n$old_email = '';\n$old_password = '';\n\nif (!empty(Input::old())) {  \n    $old_name = Input::old('input_name');\n    $old_email = Input::old('input_email');\n    $old_password = Input::old('input_password');\n}\n\n?&gt;\n&lt;div class=\"starter-template\"&gt;  \n    @if ($errors-&gt;has())\n        &lt;div class=\"alert alert-danger\" role=\"alert\"&gt;\n        @foreach ($errors-&gt;all() as $error)\n            {{ $error }}&lt;br/&gt;\n        @endforeach\n        &lt;/div&gt;\n    @endif\n    &lt;form class=\"form-horizontal\" role=\"form\" method=\"post\" action=\"/user/create\"&gt;\n        {{ Form::token() }}\n        &lt;div class=\"form-group @if ($errors-&gt;has('name')) has-error @endif\"&gt;\n            &lt;label for=\"input_name\" class=\"col-sm-2 control-label\"&gt;\n                Name\n            &lt;/label&gt;\n            &lt;div class=\"col-sm-10\"&gt;\n                &lt;input type=\"text\" class=\"form-control\" id=\"input_name\" name=\"input_name\" placeholder=\"Name\" value=\"{{ $old_name }}\"&gt;\n                @if ($errors-&gt;has('name')) &lt;p class=\"help-block\"&gt;{{ $errors-&gt;first('name') }}&lt;/p&gt; @endif\n            &lt;/div&gt;\n        &lt;/div&gt;\n        &lt;div class=\"form-group @if ($errors-&gt;has('email')) has-error @endif\"&gt;\n            &lt;label for=\"input_email\" class=\"col-sm-2 control-label\"&gt;\n                Email\n            &lt;/label&gt;\n            &lt;div class=\"col-sm-10\"&gt;\n                &lt;input type=\"email\" class=\"form-control\" id=\"input_email\" name=\"input_email\" placeholder=\"Email\" value=\"{{ $old_email }}\"&gt;\n                @if ($errors-&gt;has('email')) &lt;p class=\"help-block\"&gt;{{ $errors-&gt;first('email') }}&lt;/p&gt; @endif\n            &lt;/div&gt;\n        &lt;/div&gt;\n        &lt;div class=\"form-group @if ($errors-&gt;has('password')) has-error @endif\"&gt;\n            &lt;label for=\"input_password\" class=\"col-sm-2 control-label\"&gt;\n                Password\n            &lt;/label&gt;\n            &lt;div class=\"col-sm-10\"&gt;\n                &lt;input type=\"password\" class=\"form-control\" id=\"input_password\" name=\"input_password\" placeholder=\"Password\" value=\"{{ $old_password }}\"&gt;\n                @if ($errors-&gt;has('password')) &lt;p class=\"help-block\"&gt;{{ $errors-&gt;first('password') }}&lt;/p&gt; @endif\n            &lt;/div&gt;\n        &lt;/div&gt;\n        &lt;div class=\"form-group\"&gt;\n            &lt;div class=\"col-sm-offset-2 col-sm-10\"&gt;\n                &lt;button type=\"submit\" class=\"btn btn-default\"&gt;\n                    Create\n                &lt;/button&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n    &lt;/form&gt;\n&lt;/div&gt;  \n@stop\n</code></pre>\n\n<h3 id=\"\">接收表單資料</h3>\n\n<p>由於在 <code>app/views/user/user-create-form.blade.php</code> 的表單所指定的 action url 是 post 到 <code>/user/create</code>，所以我們要新增 Routing 規則：</p>\n\n<pre><code class=\"language-php\">Route::post('/user/create', array('before' =&gt; 'csrf', 'uses' =&gt; 'UserController@create'));  \n</code></pre>\n\n<p>實作 <code>UserController</code> 中的 <code>create</code> 方法：</p>\n\n<pre><code class=\"language-php\">public function create() {\n\n    $input           = Input::all();\n    $input_name      = $input['input_name'];\n    $input_email     = $input['input_email'];\n    $input_password  = $input['input_password'];\n    $hashed_password = Hash::make($input_password);\n\n    $validator = Validator::make(\n        array(\n            'name' =&gt; $input_name,\n            'email' =&gt; $input_email,\n            'password' =&gt; $input_password\n        ),\n        array(\n            'name' =&gt; 'required',\n            'email' =&gt; 'required|email|unique:users',\n            'password' =&gt; 'required|min:8'\n        )\n    );\n\n    if ($validator-&gt;fails()) {\n\n        // get the error messages from the validator\n        $messages = $validator-&gt;messages();\n\n        // redirect our user back to the form with the errors from the validator\n        return Redirect::to('/user/create-form')\n                -&gt;withInput()\n                -&gt;withErrors($validator);\n\n    } else {\n\n        $user           = new User;\n        $user-&gt;name     = $input_name;\n        $user-&gt;email    = $input_email;\n        $user-&gt;password = $hashed_password;\n        $user-&gt;save();\n\n        return Redirect::to('/user/list');\n\n    }\n\n}\n</code></pre>\n\n<h3 id=\"csrf\">csrf 驗證</h3>\n\n<p>CSRF/XSRF 全名 Cross Site Request Forgery 中文翻成「跨網站偽造請求」，而 CSRF 主要的攻擊行為就是利用當使用者合法取得網站使用認證後，透過某些方式偽造網站合法使用者的身份進行非法的存取動作，合法使用者即可能在不自覺的情況下被引導到駭客的攻擊網頁。</p>\n\n<p>在現今的 Web Framework 幾乎都有很完善的 CSRF 驗證解決方案來解決 CSRF 的攻擊，Laravel 也不例外。</p>\n\n<p>首先我們可以在 <code>app/routes.php</code> 中的 Routing Rules 中指定是否使用 CSRF 驗證，如：</p>\n\n<pre><code class=\"language-php\">Route::post('/user/create', array('before' =&gt; 'csrf', 'uses' =&gt; 'UserController@create'));  \n</code></pre>\n\n<p>當我們有指定要驗證 CSRF 時，Laravel 在進入 <code>UserController@create</code> 前就會先驗證 CSRF Token 是否合法，這個動作 Laravel 實作在 <code>app/filters.php</code>，內容如下：</p>\n\n<pre><code class=\"language-php\">Route::filter('csrf', function()  \n{\n    if (Session::token() != Input::get('_token'))\n    {\n        throw new Illuminate\\Session\\TokenMismatchException;\n    }\n});\n</code></pre>\n\n<p>我們可以在這邊修改 CSRF Token 驗證的動作。</p>\n\n<p>由於 Laravel 在進入 <code>UserController@create</code> 前就會先驗證 CSRF Token 是否合法，我們需要在表單中傳入合法的 Token，產生合法 Token 的方法如下：</p>\n\n<pre><code class=\"language-php\">{{ Form::token() }}\n</code></pre>\n\n<p>這樣 Laravel 就會自動在表單中插入含有合法 Token 的隱藏 input 標籤。</p>\n\n<h3 id=\"\">後端表單驗證</h3>\n\n<p>Laravel 有很方便且易讀的資料驗證方法，例如：</p>\n\n<pre><code class=\"language-php\">$validator = Validator::make(\n    array(\n        'name' =&gt; $input_name,\n        'email' =&gt; $input_email,\n        'password' =&gt; $input_password\n    ),\n    array(\n        'name' =&gt; 'required',\n        'email' =&gt; 'required|email|unique:users',\n        'password' =&gt; 'required|min:8'\n    )\n);\n</code></pre>\n\n<p>第一個 array 是放要驗證的資料值，有 name, email, password 需要驗證，第二個 array 則是 name, email, password 分別要符合哪些條件，比如 name 是 required，不能是空值，email 是 required 而且格式要符合 email 且在 users 表單中是唯一值，password 的條件則是 required 且最少要 8 個字元。</p>\n\n<p>如此我們就可以很方便地驗證使用者輸入的資料，並且可讀性也非常好。</p>\n\n<p>如果驗證失敗了，我們也可以將錯誤及提示回傳至頁面讓使用者知道：</p>\n\n<pre><code class=\"language-php\">if ($validator-&gt;fails()) {\n\n    // get the error messages from the validator\n    $messages = $validator-&gt;messages();\n\n    // redirect our user back to the form with the errors from the validator\n    return Redirect::to('/user/create-form')\n            -&gt;withInput()\n            -&gt;withErrors($validator);\n\n}\n</code></pre>\n\n<p>如此我們就可以在 <code>/user/create-form</code> 用以下的方式取得錯誤訊息及之前使用者輸入的值：</p>\n\n<pre><code class=\"language-php\">if (!empty(Input::old())) {  \n    $old_name = Input::old('input_name');\n    $old_email = Input::old('input_email');\n    $old_password = Input::old('input_password');\n}\n\n@foreach ($errors-&gt;all() as $error)\n    {{ $error }}&lt;br/&gt;\n@endforeach\n</code></pre>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-08-19T11:43:47.000Z","created_by":1,"updated_at":"2014-08-20T19:30:51.000Z","updated_by":1,"published_at":"2014-08-20T19:28:34.000Z","published_by":1},{"id":37,"uuid":"e41ad1bc-2ade-48c0-add1-998561bac974","title":"Laravel 學習筆記 Lesson 4","slug":"laravel-xue-xi-bi-ji-lesson-4","markdown":"### 如何登入\n\n要讓使用者登入，你可以使用 `Auth::attempt` 方法：\n\n```language-php\nif (Auth::attempt(array('email' => $email, 'password' => $password)))\n{\n    return Redirect::intended('dashboard');\n}\n```\n\n不過要使用這種登入使用者的語法需要讓 User Model 實作一些 Auth 介面，所以 User Model 看起來像這樣：\n\n```language-php\n<?php\n\nuse Illuminate\\Auth\\UserTrait;\nuse Illuminate\\Auth\\UserInterface;\nuse Illuminate\\Auth\\Reminders\\RemindableTrait;\nuse Illuminate\\Auth\\Reminders\\RemindableInterface;\n\nclass User extends Eloquent implements UserInterface, RemindableInterface {\n\n\tuse UserTrait, RemindableTrait;\n\n\t/**\n\t * The database table used by the model.\n\t *\n\t * @var string\n\t */\n\tprotected $table = 'users';\n\n\t/**\n\t * The attributes excluded from the model's JSON form.\n\t *\n\t * @var array\n\t */\n\tprotected $hidden = array('password', 'remember_token');\n\n}\n```\n\n### 判斷使用者是否已登入\n\n```language-php\nif (Auth::check())\n{\n    // The user is logged in...\n}\n```\n\n### 如何登出\n\n```language-php\nAuth::logout();\n```\n\n### 另外一種登入方法\n\n有時我們不是透過帳號密碼讓使用者登入，比如使用 facebook 得到 token 之後，應該透過程式讓使用者自動登入，方法如下：\n\n```language-php\n$user = User::find(1);\nAuth::login($user);\n```","html":"<h3 id=\"\">如何登入</h3>\n\n<p>要讓使用者登入，你可以使用 <code>Auth::attempt</code> 方法：</p>\n\n<pre><code class=\"language-php\">if (Auth::attempt(array('email' =&gt; $email, 'password' =&gt; $password)))  \n{\n    return Redirect::intended('dashboard');\n}\n</code></pre>\n\n<p>不過要使用這種登入使用者的語法需要讓 User Model 實作一些 Auth 介面，所以 User Model 看起來像這樣：</p>\n\n<pre><code class=\"language-php\">&lt;?php\n\nuse Illuminate\\Auth\\UserTrait;  \nuse Illuminate\\Auth\\UserInterface;  \nuse Illuminate\\Auth\\Reminders\\RemindableTrait;  \nuse Illuminate\\Auth\\Reminders\\RemindableInterface;\n\nclass User extends Eloquent implements UserInterface, RemindableInterface {\n\n    use UserTrait, RemindableTrait;\n\n    /**\n     * The database table used by the model.\n     *\n     * @var string\n     */\n    protected $table = 'users';\n\n    /**\n     * The attributes excluded from the model's JSON form.\n     *\n     * @var array\n     */\n    protected $hidden = array('password', 'remember_token');\n\n}\n</code></pre>\n\n<h3 id=\"\">判斷使用者是否已登入</h3>\n\n<pre><code class=\"language-php\">if (Auth::check())  \n{\n    // The user is logged in...\n}\n</code></pre>\n\n<h3 id=\"\">如何登出</h3>\n\n<pre><code class=\"language-php\">Auth::logout();  \n</code></pre>\n\n<h3 id=\"\">另外一種登入方法</h3>\n\n<p>有時我們不是透過帳號密碼讓使用者登入，比如使用 facebook 得到 token 之後，應該透過程式讓使用者自動登入，方法如下：</p>\n\n<pre><code class=\"language-php\">$user = User::find(1);\nAuth::login($user);  \n</code></pre>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-08-26T19:17:55.000Z","created_by":1,"updated_at":"2014-09-09T13:30:07.000Z","updated_by":1,"published_at":"2014-09-03T12:21:43.000Z","published_by":1},{"id":38,"uuid":"69824f5f-15da-45cb-bcd6-7c79451a3eec","title":"Laravel 學習筆記 Lesson 5","slug":"laravel-xue-xi-bi-ji-lesson-5","markdown":"## 在 Laravel 中使用 Oauth\n\n目前網站使用其他社群網站帳號授權登入的功能非常常見，其實這樣的登入機制是使用 <a href=\"http://zh.wikipedia.org/wiki/OAuth\" target=\"_blank\">Oauth</a> 這種開放授權標準，只要實作 Oauth 這個標準，我們就可以很方便又相對安全地分享使用者的資料。\n\n在 Laravel 中也可以透過 <a href=\"https://github.com/artdarek/oauth-4-laravel\" target=\"_blank\">artdarek/oauth-4-laravel</a> 這個套件來使用 Oauth。\n\n### 安裝 artdarek/oauth-4-laravel\n\n我們可以使用 composer 的方式來安裝 artdarek/oauth-4-laravel，請在 `composer.josn` 中的 `require` 加入以下內容：\n\n```language-json\n\"require\": {\n  \"artdarek/oauth-4-laravel\": \"dev-master\"\n}\n```\n\n然後執行以下指令使用 composer 將套件安裝好：\n\n```language-bash\n$ composer update\n```\n\n### 註冊使用 artdarek/oauth-4-laravel 套件\n\n安裝完後還需要做一些設定，請打開 `app/config/app.php` 檔案，分別在 `providers` 及 `aliases` 中註冊要使用 `artdarek/oauth-4-laravel` 套件：\n\n```language-php\n'providers' => array(\n    // ...\n\n    'Artdarek\\OAuth\\OAuthServiceProvider'\n)\n```\n\n```language-php\n'aliases' => array(\n    // ...\n\n    'OAuth' => 'Artdarek\\OAuth\\Facade\\OAuth',\n)\n```\n\n### 產生並修改設定檔\n\nOauth 的機制中需要先向資料提供網站註冊並取得 API Key 及 Token，所以勢必要有一個地方記錄這些 API Key 及 Token。\n\n首先我們先使用以下指令告訴 `artdarek/oauth-4-laravel` 產生設定檔：\n\n```language-bash\n$ php artisan config:publish artdarek/oauth-4-laravel\n```\n\n設定檔產生的位置在 `app/config/packages/artdarek/oauth-4-laravel/config.php`，打開它，將 facebook 給的 `app id` 及 `app secret` 填入：\n\n```language-php\n<?php\nreturn array( \n\n    /**\n     * Storage\n     */\n    'storage' => 'Session', \n\n    /**\n     * Consumers\n     */\n    'consumers' => array(\n\n        /**\n         * Facebook\n         */\n        'Facebook' => array(\n            'client_id'     => '',\n            'client_secret' => '',\n            'scope'         => array(),\n        ),      \n\n    )\n\n);\n```\n\n### 以 Facebook 登入為例\n\n接下來我們就可以試作看看 Facebook 登入的功能，首先增加一條 Route 規則：\n\n```language-php\nRoute::get('/fb-login', 'AuthController@fbLogin');\n```\n\n然後在對應的 Controller 增加對應的 method 處理功能邏輯，範例程式碼如下：\n\n```language-php\n    public function fbLogin() {\n\n        // get data from input\n        $code = Input::get('code');\n        // get fb service\n        $fb = OAuth::consumer('Facebook');\n\n        // check if code is valid\n\n        // if code is provided get user data and sign in\n        if (!empty($code)) {\n\n            // This was a callback request from facebook, get the token\n            $token = $fb->requestAccessToken($code);\n\n            // Send a request with it\n            $result = json_decode($fb->request('/me'), true);\n\n            $facebook_user_id   = $result['id'];\n            $user_name          = $result['name'];\n            $user_email         = $result['email'];\n            $user_avatar_url    = \"http://graph.facebook.com/\".$facebook_user_id.\"/picture?type=large\";\n\n            $user_obj = User::where('email', '=', $user_email)->first();\n\n            if (empty($user_obj)) {\n\n                // create user\n                $user_obj = new User;\n                $user_obj->name = $user_name;\n                $user_obj->email = $user_email;\n                $user_obj->password = Hash::make(str_random(8));\n                $user_obj->save();\n\n                $user_id = $user_obj->id;\n\n            }\n\n            Auth::login($user_obj);\n\n            return Redirect::to('/');\n\n\n        } else {\n\n            // get fb authorization\n            $url = $fb->getAuthorizationUri();\n\n            // return to facebook login url\n            return Redirect::to((string)$url);\n\n        }\n\n    }\n```\n\n就這樣就可以完成 Laravel 上使用 Facebook Oauth 登入的功能了！真簡單！\n","html":"<h2 id=\"laraveloauth\">在 Laravel 中使用 Oauth</h2>\n\n<p>目前網站使用其他社群網站帳號授權登入的功能非常常見，其實這樣的登入機制是使用 <a href=\"http://zh.wikipedia.org/wiki/OAuth\" target=\"_blank\">Oauth</a> 這種開放授權標準，只要實作 Oauth 這個標準，我們就可以很方便又相對安全地分享使用者的資料。</p>\n\n<p>在 Laravel 中也可以透過 <a href=\"https://github.com/artdarek/oauth-4-laravel\" target=\"_blank\">artdarek/oauth-4-laravel</a> 這個套件來使用 Oauth。</p>\n\n<h3 id=\"artdarekoauth4laravel\">安裝 artdarek/oauth-4-laravel</h3>\n\n<p>我們可以使用 composer 的方式來安裝 artdarek/oauth-4-laravel，請在 <code>composer.josn</code> 中的 <code>require</code> 加入以下內容：</p>\n\n<pre><code class=\"language-json\">\"require\": {  \n  \"artdarek/oauth-4-laravel\": \"dev-master\"\n}\n</code></pre>\n\n<p>然後執行以下指令使用 composer 將套件安裝好：</p>\n\n<pre><code class=\"language-bash\">$ composer update\n</code></pre>\n\n<h3 id=\"artdarekoauth4laravel\">註冊使用 artdarek/oauth-4-laravel 套件</h3>\n\n<p>安裝完後還需要做一些設定，請打開 <code>app/config/app.php</code> 檔案，分別在 <code>providers</code> 及 <code>aliases</code> 中註冊要使用 <code>artdarek/oauth-4-laravel</code> 套件：</p>\n\n<pre><code class=\"language-php\">'providers' =&gt; array(  \n    // ...\n\n    'Artdarek\\OAuth\\OAuthServiceProvider'\n)\n</code></pre>\n\n<pre><code class=\"language-php\">'aliases' =&gt; array(  \n    // ...\n\n    'OAuth' =&gt; 'Artdarek\\OAuth\\Facade\\OAuth',\n)\n</code></pre>\n\n<h3 id=\"\">產生並修改設定檔</h3>\n\n<p>Oauth 的機制中需要先向資料提供網站註冊並取得 API Key 及 Token，所以勢必要有一個地方記錄這些 API Key 及 Token。</p>\n\n<p>首先我們先使用以下指令告訴 <code>artdarek/oauth-4-laravel</code> 產生設定檔：</p>\n\n<pre><code class=\"language-bash\">$ php artisan config:publish artdarek/oauth-4-laravel\n</code></pre>\n\n<p>設定檔產生的位置在 <code>app/config/packages/artdarek/oauth-4-laravel/config.php</code>，打開它，將 facebook 給的 <code>app id</code> 及 <code>app secret</code> 填入：</p>\n\n<pre><code class=\"language-php\">&lt;?php  \nreturn array( \n\n    /**\n     * Storage\n     */\n    'storage' =&gt; 'Session', \n\n    /**\n     * Consumers\n     */\n    'consumers' =&gt; array(\n\n        /**\n         * Facebook\n         */\n        'Facebook' =&gt; array(\n            'client_id'     =&gt; '',\n            'client_secret' =&gt; '',\n            'scope'         =&gt; array(),\n        ),      \n\n    )\n\n);\n</code></pre>\n\n<h3 id=\"facebook\">以 Facebook 登入為例</h3>\n\n<p>接下來我們就可以試作看看 Facebook 登入的功能，首先增加一條 Route 規則：</p>\n\n<pre><code class=\"language-php\">Route::get('/fb-login', 'AuthController@fbLogin');  \n</code></pre>\n\n<p>然後在對應的 Controller 增加對應的 method 處理功能邏輯，範例程式碼如下：</p>\n\n<pre><code class=\"language-php\">    public function fbLogin() {\n\n        // get data from input\n        $code = Input::get('code');\n        // get fb service\n        $fb = OAuth::consumer('Facebook');\n\n        // check if code is valid\n\n        // if code is provided get user data and sign in\n        if (!empty($code)) {\n\n            // This was a callback request from facebook, get the token\n            $token = $fb-&gt;requestAccessToken($code);\n\n            // Send a request with it\n            $result = json_decode($fb-&gt;request('/me'), true);\n\n            $facebook_user_id   = $result['id'];\n            $user_name          = $result['name'];\n            $user_email         = $result['email'];\n            $user_avatar_url    = \"http://graph.facebook.com/\".$facebook_user_id.\"/picture?type=large\";\n\n            $user_obj = User::where('email', '=', $user_email)-&gt;first();\n\n            if (empty($user_obj)) {\n\n                // create user\n                $user_obj = new User;\n                $user_obj-&gt;name = $user_name;\n                $user_obj-&gt;email = $user_email;\n                $user_obj-&gt;password = Hash::make(str_random(8));\n                $user_obj-&gt;save();\n\n                $user_id = $user_obj-&gt;id;\n\n            }\n\n            Auth::login($user_obj);\n\n            return Redirect::to('/');\n\n\n        } else {\n\n            // get fb authorization\n            $url = $fb-&gt;getAuthorizationUri();\n\n            // return to facebook login url\n            return Redirect::to((string)$url);\n\n        }\n\n    }\n</code></pre>\n\n<p>就這樣就可以完成 Laravel 上使用 Facebook Oauth 登入的功能了！真簡單！</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-09-09T12:58:07.000Z","created_by":1,"updated_at":"2014-09-16T07:03:05.000Z","updated_by":1,"published_at":"2014-09-16T07:03:05.000Z","published_by":1},{"id":39,"uuid":"68689420-4b36-480d-996b-56c43af2f107","title":"Laravel 學習筆記 Lesson 6","slug":"laravel-xue-xi-bi-ji-lesson-6","markdown":"### 設定 Table 之間 One to One 的關係\n\nLaravel 中的 ORM 可以設定 model 之間的關係，這是關聯式資料庫非常重要的部分，在這邊將簡易說明如何設定 ORM 之間 One to One 的關係。\n\n目前我們已經有 user 這個 table，之前也介紹過如何使用 facebook 登入，這邊我們想將 user id 與 facebook user id 之間的關係記到資料庫以方便未來使用。\n\n首先我們使用 migration 新增一個 facebook_user_mappings 這個 table：\n\n```language-bash\nphp artisan migrate:make create_facebook_user_mapping_table\n```\n\n遷移檔的內容如下：\n\n```language-php\n<?php\n\nuse Illuminate\\Database\\Schema\\Blueprint;\nuse Illuminate\\Database\\Migrations\\Migration;\n\nclass CreateFacebookUserMappingTable extends Migration {\n\n\t/**\n\t * Run the migrations.\n\t *\n\t * @return void\n\t */\n\tpublic function up()\n\t{\n\t\tSchema::create('facebook_user_mappings', function($table)\n\t    {\n\t        $table->increments('id');\n\t        $table->integer('facebook_user_id')->unique();\n\t        $table->integer('user_id');\n\t        $table->timestamps();\n\t    });\n\t}\n\n\t/**\n\t * Reverse the migrations.\n\t *\n\t * @return void\n\t */\n\tpublic function down()\n\t{\n\t\tSchema::drop('facebook_user_mappings');\n\t}\n\n}\n```\n\n然後執行 migration：\n\n```language-bash\nphp artisan migrate --force\n```\n\n這樣就會建出 facebook_user_mappings 這個 table，接下來建立對應到 table 的 ORM Model：\n\n```language-php\n<?php\n\nclass FacebookUserMapping extends Eloquent {\n\n    /**\n     * The database table used by the model.\n     *\n     * @var string\n     */\n    protected $table = 'facebook_user_mappings';\n\n}\n```\n\n這樣我們就可以在使用者使用 facebook 登入的時候將 user id 與 facebook user id 之間的對應關係記下來，範例程式碼如下：\n\n```language-php\n$facebook_user_mapping_obj = FacebookUserMapping::where('facebook_user_id', '=', $fb_user_id)->first();\n\nif (!empty($user_id) &&  empty($facebook_user_mapping_obj)) {\n\n    $facebook_user_mapping_obj = new FacebookUserMapping;\n    $facebook_user_mapping_obj->facebook_user_id = $fb_user_id;\n    $facebook_user_mapping_obj->user_id = $user_id;\n    $facebook_user_mapping_obj->save();\n\n}\n```\n\n由於一個使用者只能連結一個 facebook 帳號，所以 user table 與 facebook_user_mappings table 就是一種 One to One 的關係，如何在 Laravel 中的 ORM 設定 One to One 的關係呢？\n\n首先我們在 User Model 增加一個 facebookUserMapping Method：\n\n```language-php\npublic function facebookUserMapping() {\n    return $this->hasOne('FacebookUserMapping', 'user_id', 'id');\n    //return $this->hasOne('OtherModel', 'foreign_key', 'local_key');\n}\n```\n\n裡面設定了 User Model 會 hasOne FacebookUserMapping，後面兩個參數分別代表 foreign_key 及 local_key，在這個例子中，foreign_key 就是 facebook_user_mappings 這個 table 中的 user_id 欄位，而 local_key 就是 user table 中的 id 這個欄位。\n\n接下來在 FacebookUserMapping Model 增加 user 這個 Method：\n\npublic function user() {\n    return $this->belongsTo('User', 'user_id', 'id');\n    //$this->belongsTo('User', 'local_key', 'parent_key');\n}\n\n裡面設定了 FacebookUserMapping Model 會 belongsTo User Model，後面兩個參數分別代表 local_key 及 parent_key，在這個例子中，local_key 就是 facebook_user_mappings 這個 table 中的 user_id 欄位，而 parent_key 就是 user table 中的 id 這個欄位。\n\n如此我們就設定好 User Model 與 FacebookUserMapping Model 之間的 One to One 對應關係了～\n\n### 設定 Table 之間 One to One 的關係可以做什麼呢？\n\n設定好 table 之間的 One to One 關係可以讓我們更方便地使用資料庫的資料，若果沒有設定 table 之間 One to One 的關係，我要取出 users 的 facebook_user_mappings 資料就要使用 Query Builder 來 Join 取出資料，比如：\n\n```language-php\n$results = DB::table('users')\n\t\t\t->join('facebook_user_mappings', 'users.id', '=', 'facebook_user_mappings.user_id')\n            ->get();\nprint_r($results);\n```\n\n印出來的資料會像這樣：\n\n```language-php\nArray\n(\n    [0] => stdClass Object\n        (\n            [id] => 9\n            [email] => fukuball@gmail.com\n            [name] => Fukuball Lin\n            [password] => $2y$10$rGCzZJVTuPM6kZmqj6XPIOGQu/1b1g1ewcxyQIcthtxotGUjWU25m\n            [remember_token] => 4N7zVwiGp7QagMxwofyKGIx2VoxNcNtMYYTea19uUssAjn59q78uNIyOTQOP\n            [created_at] => 2014-09-30 16:32:55\n            [updated_at] => 2014-09-30 16:32:55\n            [deleted_at] => \n            [facebook_user_id] => 1111111111\n            [user_id] => 8\n        )\n\n)\n```\n\n只是單純地將資料從資料庫中取出來而已，如此就無法使用 ORM 的方式來操作資料。但如果建立了 One to One 關係，我們就可以使用 ORM 的方式來取出資料，比如：\n\n```language-php\n$users = User::all();\nforeach($users as $user) {\n    print_r($user->facebookUserMapping);\n}\n```\n\n印出來的資料會像這樣：\n\n```language-php\nFacebookUserMapping Object\n(\n    [table:protected] => facebook_user_mappings\n    [connection:protected] => \n    [primaryKey:protected] => id\n    [perPage:protected] => 15\n    [incrementing] => 1\n    [timestamps] => 1\n    [attributes:protected] => Array\n        (\n            [id] => 9\n            [facebook_user_id] => 1111111111\n            [user_id] => 8\n            [created_at] => 2014-09-30 16:32:55\n            [updated_at] => 2014-09-30 16:32:55\n        )\n\n    [original:protected] => Array\n        (\n            [id] => 9\n            [facebook_user_id] => 1111111111\n            [user_id] => 8\n            [created_at] => 2014-09-30 16:32:55\n            [updated_at] => 2014-09-30 16:32:55\n        )\n\n    [relations:protected] => Array\n        (\n        )\n\n    [hidden:protected] => Array\n        (\n        )\n\n    [visible:protected] => Array\n        (\n        )\n\n    [appends:protected] => Array\n        (\n        )\n\n    [fillable:protected] => Array\n        (\n        )\n\n    [guarded:protected] => Array\n        (\n            [0] => *\n        )\n\n    [dates:protected] => Array\n        (\n        )\n\n    [touches:protected] => Array\n        (\n        )\n\n    [observables:protected] => Array\n        (\n        )\n\n    [with:protected] => Array\n        (\n        )\n\n    [morphClass:protected] => \n    [exists] => 1\n)\n```\n\n如此取出來的資料會是一個完整的 FacebookUserMapping Model Object，而不只是單純的資料，這樣就可以直接使用寫在 FacebookUserMapping Model 裡的方便 method。\n\n總結來說，如果可以設定 Model 之間的關係，未來在使用資料時會變得很方便，否則就會寫很多 Query 在程式碼中，程式會變得難以維護。但是如果是複雜的 Query 還是可以斟酌使用 Query Builder 來查詢資料。","html":"<h3 id=\"tableonetoone\">設定 Table 之間 One to One 的關係</h3>\n\n<p>Laravel 中的 ORM 可以設定 model 之間的關係，這是關聯式資料庫非常重要的部分，在這邊將簡易說明如何設定 ORM 之間 One to One 的關係。</p>\n\n<p>目前我們已經有 user 這個 table，之前也介紹過如何使用 facebook 登入，這邊我們想將 user id 與 facebook user id 之間的關係記到資料庫以方便未來使用。</p>\n\n<p>首先我們使用 migration 新增一個 facebook<em>user</em>mappings 這個 table：</p>\n\n<pre><code class=\"language-bash\">php artisan migrate:make create_facebook_user_mapping_table  \n</code></pre>\n\n<p>遷移檔的內容如下：</p>\n\n<pre><code class=\"language-php\">&lt;?php\n\nuse Illuminate\\Database\\Schema\\Blueprint;  \nuse Illuminate\\Database\\Migrations\\Migration;\n\nclass CreateFacebookUserMappingTable extends Migration {\n\n    /**\n     * Run the migrations.\n     *\n     * @return void\n     */\n    public function up()\n    {\n        Schema::create('facebook_user_mappings', function($table)\n        {\n            $table-&gt;increments('id');\n            $table-&gt;integer('facebook_user_id')-&gt;unique();\n            $table-&gt;integer('user_id');\n            $table-&gt;timestamps();\n        });\n    }\n\n    /**\n     * Reverse the migrations.\n     *\n     * @return void\n     */\n    public function down()\n    {\n        Schema::drop('facebook_user_mappings');\n    }\n\n}\n</code></pre>\n\n<p>然後執行 migration：</p>\n\n<pre><code class=\"language-bash\">php artisan migrate --force  \n</code></pre>\n\n<p>這樣就會建出 facebook<em>user</em>mappings 這個 table，接下來建立對應到 table 的 ORM Model：</p>\n\n<pre><code class=\"language-php\">&lt;?php\n\nclass FacebookUserMapping extends Eloquent {\n\n    /**\n     * The database table used by the model.\n     *\n     * @var string\n     */\n    protected $table = 'facebook_user_mappings';\n\n}\n</code></pre>\n\n<p>這樣我們就可以在使用者使用 facebook 登入的時候將 user id 與 facebook user id 之間的對應關係記下來，範例程式碼如下：</p>\n\n<pre><code class=\"language-php\">$facebook_user_mapping_obj = FacebookUserMapping::where('facebook_user_id', '=', $fb_user_id)-&gt;first();\n\nif (!empty($user_id) &amp;&amp;  empty($facebook_user_mapping_obj)) {\n\n    $facebook_user_mapping_obj = new FacebookUserMapping;\n    $facebook_user_mapping_obj-&gt;facebook_user_id = $fb_user_id;\n    $facebook_user_mapping_obj-&gt;user_id = $user_id;\n    $facebook_user_mapping_obj-&gt;save();\n\n}\n</code></pre>\n\n<p>由於一個使用者只能連結一個 facebook 帳號，所以 user table 與 facebook<em>user</em>mappings table 就是一種 One to One 的關係，如何在 Laravel 中的 ORM 設定 One to One 的關係呢？</p>\n\n<p>首先我們在 User Model 增加一個 facebookUserMapping Method：</p>\n\n<pre><code class=\"language-php\">public function facebookUserMapping() {  \n    return $this-&gt;hasOne('FacebookUserMapping', 'user_id', 'id');\n    //return $this-&gt;hasOne('OtherModel', 'foreign_key', 'local_key');\n}\n</code></pre>\n\n<p>裡面設定了 User Model 會 hasOne FacebookUserMapping，後面兩個參數分別代表 foreign<em>key 及 local</em>key，在這個例子中，foreign<em>key 就是 facebook</em>user<em>mappings 這個 table 中的 user</em>id 欄位，而 local_key 就是 user table 中的 id 這個欄位。</p>\n\n<p>接下來在 FacebookUserMapping Model 增加 user 這個 Method：</p>\n\n<p>public function user() { <br />\n    return $this->belongsTo('User', 'user<em>id', 'id');\n    //$this->belongsTo('User', 'local</em>key', 'parent_key');\n}</p>\n\n<p>裡面設定了 FacebookUserMapping Model 會 belongsTo User Model，後面兩個參數分別代表 local<em>key 及 parent</em>key，在這個例子中，local<em>key 就是 facebook</em>user<em>mappings 這個 table 中的 user</em>id 欄位，而 parent_key 就是 user table 中的 id 這個欄位。</p>\n\n<p>如此我們就設定好 User Model 與 FacebookUserMapping Model 之間的 One to One 對應關係了～</p>\n\n<h3 id=\"tableonetoone\">設定 Table 之間 One to One 的關係可以做什麼呢？</h3>\n\n<p>設定好 table 之間的 One to One 關係可以讓我們更方便地使用資料庫的資料，若果沒有設定 table 之間 One to One 的關係，我要取出 users 的 facebook<em>user</em>mappings 資料就要使用 Query Builder 來 Join 取出資料，比如：</p>\n\n<pre><code class=\"language-php\">$results = DB::table('users')\n            -&gt;join('facebook_user_mappings', 'users.id', '=', 'facebook_user_mappings.user_id')\n            -&gt;get();\nprint_r($results);  \n</code></pre>\n\n<p>印出來的資料會像這樣：</p>\n\n<pre><code class=\"language-php\">Array  \n(\n    [0] =&gt; stdClass Object\n        (\n            [id] =&gt; 9\n            [email] =&gt; fukuball@gmail.com\n            [name] =&gt; Fukuball Lin\n            [password] =&gt; $2y$10$rGCzZJVTuPM6kZmqj6XPIOGQu/1b1g1ewcxyQIcthtxotGUjWU25m\n            [remember_token] =&gt; 4N7zVwiGp7QagMxwofyKGIx2VoxNcNtMYYTea19uUssAjn59q78uNIyOTQOP\n            [created_at] =&gt; 2014-09-30 16:32:55\n            [updated_at] =&gt; 2014-09-30 16:32:55\n            [deleted_at] =&gt; \n            [facebook_user_id] =&gt; 1111111111\n            [user_id] =&gt; 8\n        )\n\n)\n</code></pre>\n\n<p>只是單純地將資料從資料庫中取出來而已，如此就無法使用 ORM 的方式來操作資料。但如果建立了 One to One 關係，我們就可以使用 ORM 的方式來取出資料，比如：</p>\n\n<pre><code class=\"language-php\">$users = User::all();\nforeach($users as $user) {  \n    print_r($user-&gt;facebookUserMapping);\n}\n</code></pre>\n\n<p>印出來的資料會像這樣：</p>\n\n<pre><code class=\"language-php\">FacebookUserMapping Object  \n(\n    [table:protected] =&gt; facebook_user_mappings\n    [connection:protected] =&gt; \n    [primaryKey:protected] =&gt; id\n    [perPage:protected] =&gt; 15\n    [incrementing] =&gt; 1\n    [timestamps] =&gt; 1\n    [attributes:protected] =&gt; Array\n        (\n            [id] =&gt; 9\n            [facebook_user_id] =&gt; 1111111111\n            [user_id] =&gt; 8\n            [created_at] =&gt; 2014-09-30 16:32:55\n            [updated_at] =&gt; 2014-09-30 16:32:55\n        )\n\n    [original:protected] =&gt; Array\n        (\n            [id] =&gt; 9\n            [facebook_user_id] =&gt; 1111111111\n            [user_id] =&gt; 8\n            [created_at] =&gt; 2014-09-30 16:32:55\n            [updated_at] =&gt; 2014-09-30 16:32:55\n        )\n\n    [relations:protected] =&gt; Array\n        (\n        )\n\n    [hidden:protected] =&gt; Array\n        (\n        )\n\n    [visible:protected] =&gt; Array\n        (\n        )\n\n    [appends:protected] =&gt; Array\n        (\n        )\n\n    [fillable:protected] =&gt; Array\n        (\n        )\n\n    [guarded:protected] =&gt; Array\n        (\n            [0] =&gt; *\n        )\n\n    [dates:protected] =&gt; Array\n        (\n        )\n\n    [touches:protected] =&gt; Array\n        (\n        )\n\n    [observables:protected] =&gt; Array\n        (\n        )\n\n    [with:protected] =&gt; Array\n        (\n        )\n\n    [morphClass:protected] =&gt; \n    [exists] =&gt; 1\n)\n</code></pre>\n\n<p>如此取出來的資料會是一個完整的 FacebookUserMapping Model Object，而不只是單純的資料，這樣就可以直接使用寫在 FacebookUserMapping Model 裡的方便 method。</p>\n\n<p>總結來說，如果可以設定 Model 之間的關係，未來在使用資料時會變得很方便，否則就會寫很多 Query 在程式碼中，程式會變得難以維護。但是如果是複雜的 Query 還是可以斟酌使用 Query Builder 來查詢資料。</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-09-16T07:09:07.000Z","created_by":1,"updated_at":"2014-09-30T10:58:46.000Z","updated_by":1,"published_at":"2014-09-30T10:57:42.000Z","published_by":1},{"id":40,"uuid":"8c235f10-a4f1-4118-bbb7-56cb9696debc","title":"Laravel 學習筆記 Lesson 7","slug":"laravel-xue-xi-bi-ji-lesson-7","markdown":"### 刪除及軟刪除\n\n寫網頁應用程式免不了需要做到一些刪除資料的功能，由於 Laravel 使用 ORM，要刪除資料非常容易，不用再特地寫 SQL Query，程式可讀性也非常好，例如：\n\n```language-php\n$user_obj = User::find(1);\n$user_obj->delete();\n```\n\n如此就可以從資料庫中刪除編號 1 這筆 user 的資料。 \n\n但有時我們會有這樣的需求，我們想要刪除資料，但不想要真的將資料從資料庫中刪除掉，這樣要怎麼做呢？\n\n直覺的做法就是，自己新增一個欄位來標示資料是否被刪除，這樣基本上就可以做到這樣的功能，只是取資料時總是要多寫一個條件來篩選出沒有被標示成已刪除的資料，自己在程式中處理久了，會覺得有些麻煩，Laravel 有沒有一個比較好的解決方案呢？\n\n這樣的功能在 Laravel 就叫做軟刪除，而且透過軟刪除後的資料在取資料時不會出現，不必再特地去 Query 中增加條件，Laravel 都幫你處理好了！\n\n要怎麼在 Laravel 中使用軟刪除呢？\n\n首先，我們先增加一個 migration 檔案：\n\n```language-bash\nphp artisan migrate:make update_user_soft_delete\n```\n\n然後在 migration 檔案中使用 `softDeletes()` 幫 user table 增加軟刪除所需要的資料欄位，而實際上這個資料欄位的名稱是 `deleted_at`，請參考底下遷移檔內容：\n\n```language-php\n<?php\n\nuse Illuminate\\Database\\Schema\\Blueprint;\nuse Illuminate\\Database\\Migrations\\Migration;\n\nclass UpdateUserSoftDelete extends Migration {\n\n\t/**\n\t * Run the migrations.\n\t *\n\t * @return void\n\t */\n\tpublic function up()\n\t{\n\t\tSchema::table('users', function($table)\n\t\t{\n\n\t\t\t$table->softDeletes();\n\n\t\t});\n\t}\n\n\t/**\n\t * Reverse the migrations.\n\t *\n\t * @return void\n\t */\n\tpublic function down()\n\t{\n\t\tSchema::table('users', function($table)\n\t\t{\n\t\t    $table->dropColumn('deleted_at');\n\t\t});\n\t}\n\n}\n```\n然後我們執行 migration：\n\n```language-bash\nphp artisan migrate --force\n```\n\n如此我們就準備好軟刪除所需要的資料欄位了。\n\n接下來我們要將 User Model 增加軟刪除的功能，我們需要在 User Model 上方增加這一行 `use Illuminate\\Database\\Eloquent\\SoftDeletingTrait;`，告訴 Laravel 我們要在這個 Model 使用軟刪除。\n\n然後在 User Model 的內容中增加這兩行：`use SoftDeletingTrait;` 及 `protected $dates = ['deleted_at'];`，如此 User Model 的刪除方法就會變成軟刪除，範例如下：\n\n```language-php\nuse Illuminate\\Database\\Eloquent\\SoftDeletingTrait;\n\nclass User extends Eloquent {\n\n    use SoftDeletingTrait;\n\n    protected $dates = ['deleted_at'];\n\n}\n```\n\n我們還是像以前一樣刪除資料：\n\n```language-php\n$user = User::find(1);\n$user->delete();\n```\n\n資料會呈現刪除狀態，但卻不會從資料庫中消失，取資料時也不會取出這些被軟刪除的資料：\n\n```language-php\n$users = User::all();\n// 軟刪除的資料不會出現\n```\n\n若真的想要一併取回列出軟刪除的資料也可以做到：\n\n```language-php\n$users = User::withTrashed()->get();\n// 軟刪除的資料會出現\n```\n\n由於軟刪除的資料其實還存在資料庫中，我們可以將資料回復回來：\n\n```language-php\n$user = User::find(1);\n$user->restore();\n```\n\nLaravel 的軟刪除功能真的很好用！\n\n\n\n\n\n","html":"<h3 id=\"\">刪除及軟刪除</h3>\n\n<p>寫網頁應用程式免不了需要做到一些刪除資料的功能，由於 Laravel 使用 ORM，要刪除資料非常容易，不用再特地寫 SQL Query，程式可讀性也非常好，例如：</p>\n\n<pre><code class=\"language-php\">$user_obj = User::find(1);\n$user_obj-&gt;delete();\n</code></pre>\n\n<p>如此就可以從資料庫中刪除編號 1 這筆 user 的資料。 </p>\n\n<p>但有時我們會有這樣的需求，我們想要刪除資料，但不想要真的將資料從資料庫中刪除掉，這樣要怎麼做呢？</p>\n\n<p>直覺的做法就是，自己新增一個欄位來標示資料是否被刪除，這樣基本上就可以做到這樣的功能，只是取資料時總是要多寫一個條件來篩選出沒有被標示成已刪除的資料，自己在程式中處理久了，會覺得有些麻煩，Laravel 有沒有一個比較好的解決方案呢？</p>\n\n<p>這樣的功能在 Laravel 就叫做軟刪除，而且透過軟刪除後的資料在取資料時不會出現，不必再特地去 Query 中增加條件，Laravel 都幫你處理好了！</p>\n\n<p>要怎麼在 Laravel 中使用軟刪除呢？</p>\n\n<p>首先，我們先增加一個 migration 檔案：</p>\n\n<pre><code class=\"language-bash\">php artisan migrate:make update_user_soft_delete  \n</code></pre>\n\n<p>然後在 migration 檔案中使用 <code>softDeletes()</code> 幫 user table 增加軟刪除所需要的資料欄位，而實際上這個資料欄位的名稱是 <code>deleted_at</code>，請參考底下遷移檔內容：</p>\n\n<pre><code class=\"language-php\">&lt;?php\n\nuse Illuminate\\Database\\Schema\\Blueprint;  \nuse Illuminate\\Database\\Migrations\\Migration;\n\nclass UpdateUserSoftDelete extends Migration {\n\n    /**\n     * Run the migrations.\n     *\n     * @return void\n     */\n    public function up()\n    {\n        Schema::table('users', function($table)\n        {\n\n            $table-&gt;softDeletes();\n\n        });\n    }\n\n    /**\n     * Reverse the migrations.\n     *\n     * @return void\n     */\n    public function down()\n    {\n        Schema::table('users', function($table)\n        {\n            $table-&gt;dropColumn('deleted_at');\n        });\n    }\n\n}\n</code></pre>\n\n<p>然後我們執行 migration：</p>\n\n<pre><code class=\"language-bash\">php artisan migrate --force  \n</code></pre>\n\n<p>如此我們就準備好軟刪除所需要的資料欄位了。</p>\n\n<p>接下來我們要將 User Model 增加軟刪除的功能，我們需要在 User Model 上方增加這一行 <code>use Illuminate\\Database\\Eloquent\\SoftDeletingTrait;</code>，告訴 Laravel 我們要在這個 Model 使用軟刪除。</p>\n\n<p>然後在 User Model 的內容中增加這兩行：<code>use SoftDeletingTrait;</code> 及 <code>protected $dates = ['deleted_at'];</code>，如此 User Model 的刪除方法就會變成軟刪除，範例如下：</p>\n\n<pre><code class=\"language-php\">use Illuminate\\Database\\Eloquent\\SoftDeletingTrait;\n\nclass User extends Eloquent {\n\n    use SoftDeletingTrait;\n\n    protected $dates = ['deleted_at'];\n\n}\n</code></pre>\n\n<p>我們還是像以前一樣刪除資料：</p>\n\n<pre><code class=\"language-php\">$user = User::find(1);\n$user-&gt;delete();\n</code></pre>\n\n<p>資料會呈現刪除狀態，但卻不會從資料庫中消失，取資料時也不會取出這些被軟刪除的資料：</p>\n\n<pre><code class=\"language-php\">$users = User::all();\n// 軟刪除的資料不會出現\n</code></pre>\n\n<p>若真的想要一併取回列出軟刪除的資料也可以做到：</p>\n\n<pre><code class=\"language-php\">$users = User::withTrashed()-&gt;get();\n// 軟刪除的資料會出現\n</code></pre>\n\n<p>由於軟刪除的資料其實還存在資料庫中，我們可以將資料回復回來：</p>\n\n<pre><code class=\"language-php\">$user = User::find(1);\n$user-&gt;restore();\n</code></pre>\n\n<p>Laravel 的軟刪除功能真的很好用！</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-09-22T18:53:42.000Z","created_by":1,"updated_at":"2014-10-15T07:57:07.000Z","updated_by":1,"published_at":"2014-10-15T07:56:32.000Z","published_by":1},{"id":41,"uuid":"1dbec5cd-3219-47ec-9e8f-061b53f8460c","title":"Laravel 學習筆記 Lesson 8","slug":"laravel-xue-xi-bi-ji-lesson-8","markdown":"### 上傳檔案到本機端\n\n網站應用程式經常需要有上傳檔案的功能，在這邊介紹如何在 Laravel Framework 中上傳檔案到本機端。\n\n其實現在大部份的網站應用程式都不會單純將上傳的檔案存到本機端，而是直接存到雲端的硬碟裡，例如存到 AWS，以避免應用程式的伺服器硬碟爆滿而造成整台網頁伺服器掛掉，不過如何上傳檔案到雲端硬碟和上傳檔案到本機端其實大同小異，日後我們會再介紹如何上傳至雲端硬碟。\n\n### 上傳檔案表單\n\n首先我們需要先製作一個上傳檔案的表單，上傳檔案表單與一般表單不同的地方在於上傳檔案表單需要多一個 property `enctype=\"multipart/form-data\"`，如此才能夠順利上傳檔案，範例表單如下：\n\n```\n<form class=\"form-horizontal\" method=\"post\" action=\"/user/icon-upload\" accept-charset=\"UTF-8\" enctype=\"multipart/form-data\">\n\t<input type=\"file\" class=\"form-control\" id=\"user_icon_file\" name=\"user_icon_file\" placeholder=\"上傳圖片\" value=\"\">\n</form>\n```\n\n### 處理上傳檔案\n\n首先我們新增一個表單所對應的請求 Routing Rule：\n\n```\nRoute::post('/user/icon-upload','UserController@iconUpload');\n```\n\n然後在 Routing Rule 指定的 Controller 裡新增對應的方法，在這邊的例子為 iconUpload，範例如下：\n\n```\npublic function iconUpload()\n{\n\t$file = Input::file('user_icon_file');\n\t$extension = $file->getClientOriginalExtension();\n    $file_name = strval(time()).str_random(5).'.'.$extension;\n\t\n    $destination_path = public_path().'/user-upload/';\n\n    if (Input::hasFile('user_icon_file')) {\n    \t$upload_success = $file->move($destination_path, $file_name);\n        echo \"img upload success!\";\n    } else {\n        echo \"img upload failed!\";\n    }\n            \n    $user_obj = Auth::user();\n    $user_obj->user_icon = $file_name;\n    $user_obj->save();\n    \n}\n```\n\n首先我們用 `$file = Input::file('user_icon_file')` 取得上傳檔案的相關資料，如果檔案相關的資料都會存在 `$file` 這個變數裡面，然後使用 `$file->getClientOriginalExtension()` 我們可以取得檔案的副檔名，為了避免上傳的檔案因為檔名相同而造成上傳檔案相互覆蓋，我們通常會重新產生新的檔案名稱，如 `$file_name = strval(time()).str_random(5).'.'.$extension`，記得副檔名也要附加在檔案名稱中。\n\n接下來我們需要告訴 Laravel 我們要將上傳的檔案存在伺服器的哪的資料夾，這個例子是使用 `$destination_path = public_path().'/user-upload/'`，所以必須先在 app 中下這兩個指令新增放上傳檔案的資料夾：\n\n```\n$ mkdir public/user-upload\n$ chmod -R 777 public/user-upload\n```\n\n使用 `Input::hasFile('user_icon_file')` 這個方法檢查檔案是否確實上傳後，`$upload_success = $file->move($destination_path, $file_name)` 這個方法就可以將檔案上傳至 `$destination_path` 並將檔案以 `$file_name` 重新命名。\n\n記得最後要將 `$file_name` 這個資訊存下來，這樣未來才能將上傳的檔案顯示在網頁，如此就完成上傳檔案的功能了。\n\n\n### 在網頁顯示上傳的圖片\n\n由於我們已將上傳的圖片檔案上傳至 `public/user-upload` 這個資料夾，而資料庫裡又有記下圖片檔名的資訊，如此我們可以很容易地得到圖片的公開網址，讓網頁可以順利顯示圖片。\n\n範例如下：\n\n```\n<img src=\"/user-upload/{{ $user_obj->user_icon }}\" >\n```\n\n由於大部份的網頁伺服器都會將 Laravel 的根目錄指向 public 這個資料夾，所以圖片網址就不需要再有 public 了。\n\n如果之前有寫過 PHP，會發現 Laravel 的上傳檔案跟 PHP 的寫法很不一樣，主要是 Laravel 將上傳檔案的相關功能另外再包裝了起來，如此才不會發生一些非預期的意外，不過基本上概念是很累似的。\n\n\n","html":"<h3 id=\"\">上傳檔案到本機端</h3>\n\n<p>網站應用程式經常需要有上傳檔案的功能，在這邊介紹如何在 Laravel Framework 中上傳檔案到本機端。</p>\n\n<p>其實現在大部份的網站應用程式都不會單純將上傳的檔案存到本機端，而是直接存到雲端的硬碟裡，例如存到 AWS，以避免應用程式的伺服器硬碟爆滿而造成整台網頁伺服器掛掉，不過如何上傳檔案到雲端硬碟和上傳檔案到本機端其實大同小異，日後我們會再介紹如何上傳至雲端硬碟。</p>\n\n<h3 id=\"\">上傳檔案表單</h3>\n\n<p>首先我們需要先製作一個上傳檔案的表單，上傳檔案表單與一般表單不同的地方在於上傳檔案表單需要多一個 property <code>enctype=\"multipart/form-data\"</code>，如此才能夠順利上傳檔案，範例表單如下：</p>\n\n<pre><code>&lt;form class=\"form-horizontal\" method=\"post\" action=\"/user/icon-upload\" accept-charset=\"UTF-8\" enctype=\"multipart/form-data\"&gt;  \n    &lt;input type=\"file\" class=\"form-control\" id=\"user_icon_file\" name=\"user_icon_file\" placeholder=\"上傳圖片\" value=\"\"&gt;\n&lt;/form&gt;  \n</code></pre>\n\n<h3 id=\"\">處理上傳檔案</h3>\n\n<p>首先我們新增一個表單所對應的請求 Routing Rule：</p>\n\n<pre><code>Route::post('/user/icon-upload','UserController@iconUpload');  \n</code></pre>\n\n<p>然後在 Routing Rule 指定的 Controller 裡新增對應的方法，在這邊的例子為 iconUpload，範例如下：</p>\n\n<pre><code>public function iconUpload()  \n{\n    $file = Input::file('user_icon_file');\n    $extension = $file-&gt;getClientOriginalExtension();\n    $file_name = strval(time()).str_random(5).'.'.$extension;\n\n    $destination_path = public_path().'/user-upload/';\n\n    if (Input::hasFile('user_icon_file')) {\n        $upload_success = $file-&gt;move($destination_path, $file_name);\n        echo \"img upload success!\";\n    } else {\n        echo \"img upload failed!\";\n    }\n\n    $user_obj = Auth::user();\n    $user_obj-&gt;user_icon = $file_name;\n    $user_obj-&gt;save();\n\n}\n</code></pre>\n\n<p>首先我們用 <code>$file = Input::file('user_icon_file')</code> 取得上傳檔案的相關資料，如果檔案相關的資料都會存在 <code>$file</code> 這個變數裡面，然後使用 <code>$file-&gt;getClientOriginalExtension()</code> 我們可以取得檔案的副檔名，為了避免上傳的檔案因為檔名相同而造成上傳檔案相互覆蓋，我們通常會重新產生新的檔案名稱，如 <code>$file_name = strval(time()).str_random(5).'.'.$extension</code>，記得副檔名也要附加在檔案名稱中。</p>\n\n<p>接下來我們需要告訴 Laravel 我們要將上傳的檔案存在伺服器的哪的資料夾，這個例子是使用 <code>$destination_path = public_path().'/user-upload/'</code>，所以必須先在 app 中下這兩個指令新增放上傳檔案的資料夾：</p>\n\n<pre><code>$ mkdir public/user-upload\n$ chmod -R 777 public/user-upload\n</code></pre>\n\n<p>使用 <code>Input::hasFile('user_icon_file')</code> 這個方法檢查檔案是否確實上傳後，<code>$upload_success = $file-&gt;move($destination_path, $file_name)</code> 這個方法就可以將檔案上傳至 <code>$destination_path</code> 並將檔案以 <code>$file_name</code> 重新命名。</p>\n\n<p>記得最後要將 <code>$file_name</code> 這個資訊存下來，這樣未來才能將上傳的檔案顯示在網頁，如此就完成上傳檔案的功能了。</p>\n\n<h3 id=\"\">在網頁顯示上傳的圖片</h3>\n\n<p>由於我們已將上傳的圖片檔案上傳至 <code>public/user-upload</code> 這個資料夾，而資料庫裡又有記下圖片檔名的資訊，如此我們可以很容易地得到圖片的公開網址，讓網頁可以順利顯示圖片。</p>\n\n<p>範例如下：</p>\n\n<pre><code>&lt;img src=\"/user-upload/{{ $user_obj-&gt;user_icon }}\" &gt;  \n</code></pre>\n\n<p>由於大部份的網頁伺服器都會將 Laravel 的根目錄指向 public 這個資料夾，所以圖片網址就不需要再有 public 了。</p>\n\n<p>如果之前有寫過 PHP，會發現 Laravel 的上傳檔案跟 PHP 的寫法很不一樣，主要是 Laravel 將上傳檔案的相關功能另外再包裝了起來，如此才不會發生一些非預期的意外，不過基本上概念是很累似的。</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-09-30T10:06:02.000Z","created_by":1,"updated_at":"2014-11-12T12:55:30.000Z","updated_by":1,"published_at":"2014-11-12T12:55:20.000Z","published_by":1},{"id":42,"uuid":"6c72e238-61e9-47f2-8cf8-d699a41f4848","title":"Laravel 學習筆記 Lesson 9-1","slug":"laravel-xue-xi-bi-ji-lesson-9","markdown":"### 設定 Table 之間 One to Many 的關係\n\n我們在 [Laravel 學習筆記 Lesson 6](http://blog.fukuball.com/laravel-xue-xi-bi-ji-lesson-6/) 曾經說明過如何設定 Laravel 中的 Model One to One 的關係，在這邊將簡易說明如何設定 ORM 之間 One to Many 的關係。\n\n假設現在我們的資料有文章（Post）及文章評論（Comment），我們會先建立好文章的 Model 及 Comment 的 Model，通常一篇文章會有很多個評論，所以文章及評論就是一種一對多的關係。\n\n要在 Laravel 中的 Model 建立一對多的關係非常簡易，首先我們在 Model Post Class 中設定好一對多的關係，範例程式碼如下：\n\n```\nclass Post extends Eloquent {\n\n    public function comments()\n    {\n        return $this->hasMany('Comment', 'post_id', 'id');\n        //return $this->hasMany('model_name', 'foreign_key', 'local_key');\n    }\n\n}\n```\n\n裡面設定了 Post Model 會 hasMany Comment，後面兩個參數分別代表 foreign key 及 local key，在這個例子中，foreign key 就是 comment 這個 table 中的 post_id 欄位，而 local key 就是 post table 中的 id 這個欄位。\n\n接下來在 Comment Model 增加 post 這個 Method：\n\n```\nclass Comment extends Eloquent {\n\n    public function post()\n    {\n        return $this->belongsTo('Post', 'post_id', 'id');\n        //return $this->belongsTo('model_name', 'local_key', 'parent_key');\n    }\n\n}\n```\n\n裡面設定了 Comment Model 會 belongsTo Post Model，後面兩個參數分別代表 local key 及 parent key，在這個例子中，local key 就是 comment 這個 table 中的 post_id 欄位，而 parent key 就是 post 中的 id 這個欄位。\n\n如此我們就設定好 Post Model 與 Comment Model 之間的 One to Many 對應關係了～\n\n設定好 table 之間的 One to Many 關係可以讓我們更方便地使用資料庫的資料，若果沒有設定 table 之間 One to Many 的關係，我要取出 Post 的 comment 資料就要使用 Query Builder 來 Join 取出資料，但如果建立了 One to Many 關係，我們就可以使用 ORM 的方式來取出資料，比如：\n\n```\n$comments = Post::find(1)->comments;\n```\n\n總結來說，如果可以設定 Model 之間的關係，未來在使用資料時會變得很方便，否則就會寫很多 Query 在程式碼中，程式會變得難以維護。但是如果是複雜的 Query 還是可以斟酌使用 Query Builder 來查詢資料。","html":"<h3 id=\"tableonetomany\">設定 Table 之間 One to Many 的關係</h3>\n\n<p>我們在 <a href=\"http://blog.fukuball.com/laravel-xue-xi-bi-ji-lesson-6/\">Laravel 學習筆記 Lesson 6</a> 曾經說明過如何設定 Laravel 中的 Model One to One 的關係，在這邊將簡易說明如何設定 ORM 之間 One to Many 的關係。</p>\n\n<p>假設現在我們的資料有文章（Post）及文章評論（Comment），我們會先建立好文章的 Model 及 Comment 的 Model，通常一篇文章會有很多個評論，所以文章及評論就是一種一對多的關係。</p>\n\n<p>要在 Laravel 中的 Model 建立一對多的關係非常簡易，首先我們在 Model Post Class 中設定好一對多的關係，範例程式碼如下：</p>\n\n<pre><code>class Post extends Eloquent {\n\n    public function comments()\n    {\n        return $this-&gt;hasMany('Comment', 'post_id', 'id');\n        //return $this-&gt;hasMany('model_name', 'foreign_key', 'local_key');\n    }\n\n}\n</code></pre>\n\n<p>裡面設定了 Post Model 會 hasMany Comment，後面兩個參數分別代表 foreign key 及 local key，在這個例子中，foreign key 就是 comment 這個 table 中的 post_id 欄位，而 local key 就是 post table 中的 id 這個欄位。</p>\n\n<p>接下來在 Comment Model 增加 post 這個 Method：</p>\n\n<pre><code>class Comment extends Eloquent {\n\n    public function post()\n    {\n        return $this-&gt;belongsTo('Post', 'post_id', 'id');\n        //return $this-&gt;belongsTo('model_name', 'local_key', 'parent_key');\n    }\n\n}\n</code></pre>\n\n<p>裡面設定了 Comment Model 會 belongsTo Post Model，後面兩個參數分別代表 local key 及 parent key，在這個例子中，local key 就是 comment 這個 table 中的 post_id 欄位，而 parent key 就是 post 中的 id 這個欄位。</p>\n\n<p>如此我們就設定好 Post Model 與 Comment Model 之間的 One to Many 對應關係了～</p>\n\n<p>設定好 table 之間的 One to Many 關係可以讓我們更方便地使用資料庫的資料，若果沒有設定 table 之間 One to Many 的關係，我要取出 Post 的 comment 資料就要使用 Query Builder 來 Join 取出資料，但如果建立了 One to Many 關係，我們就可以使用 ORM 的方式來取出資料，比如：</p>\n\n<pre><code>$comments = Post::find(1)-&gt;comments;\n</code></pre>\n\n<p>總結來說，如果可以設定 Model 之間的關係，未來在使用資料時會變得很方便，否則就會寫很多 Query 在程式碼中，程式會變得難以維護。但是如果是複雜的 Query 還是可以斟酌使用 Query Builder 來查詢資料。</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-10-06T08:05:32.000Z","created_by":1,"updated_at":"2015-04-01T14:17:18.000Z","updated_by":1,"published_at":"2015-04-01T14:17:09.000Z","published_by":1},{"id":43,"uuid":"a9251c98-cdb2-44ee-bd86-d193e3b073df","title":"Laravel 學習筆記 Lesson 10","slug":"laravel-xue-xi-bi-ji-lesson-10","markdown":"### 使用 flysystem 串接 aws s3\n\nhttps://github.com/GrahamCampbell/Laravel-Flysystem\n\n \"graham-campbell/flysystem\": \"~1.0\"\n \"league/event\": \"~1.0\"\n \"aws/aws-sdk-php\": \"~2.4\"\n \n composer update\n\napp/config/app.php and add the following to the providers key.\n\n\n'GrahamCampbell\\Flysystem\\FlysystemServiceProvider'\n\nYou can register the Flysystem facade in the aliases key of your app/config/app.php file if you like.\n\n'Flysystem' => 'GrahamCampbell\\Flysystem\\Facades\\Flysystem'\n\nphp artisan config:publish graham-campbell/flysystem\n\n'default' => 'local',\n\n$temp_file_path = Input::file('pin_img_file')->getRealPath();\n                $file_contents = file_get_contents($temp_file_path);\n                Flysystem::put($filename, $file_contents);\n                unset($temp_file_path);\n                \n'awss3' => array(\n            'driver'    => 'awss3',\n            'key'       => 'your-key',\n            'secret'    => 'your-secret',\n            'bucket'    => 'your-bucket',\n            'region'    => 'your-region',\n            // 'base_url'  => 'your-url',\n            // 'prefix'    => 'your-prefix',\n            // 'eventable' => true,\n            // 'cache'     => 'foo'\n        ),\n        \nreturn Flysystem::getAdapter()->getClient()->getObjectUrl('pin-here.com', $this->pin_img_file, '+5 minutes');","html":"<h3 id=\"flysystemawss3\">使用 flysystem 串接 aws s3</h3>\n\n<p><a href='https://github.com/GrahamCampbell/Laravel-Flysystem'>https://github.com/GrahamCampbell/Laravel-Flysystem</a></p>\n\n<p>\"graham-campbell/flysystem\": \"~1.0\"\n \"league/event\": \"~1.0\"\n \"aws/aws-sdk-php\": \"~2.4\"</p>\n\n<p>composer update</p>\n\n<p>app/config/app.php and add the following to the providers key.</p>\n\n<p>'GrahamCampbell\\Flysystem\\FlysystemServiceProvider'</p>\n\n<p>You can register the Flysystem facade in the aliases key of your app/config/app.php file if you like.</p>\n\n<p>'Flysystem' => 'GrahamCampbell\\Flysystem\\Facades\\Flysystem'</p>\n\n<p>php artisan config:publish graham-campbell/flysystem</p>\n\n<p>'default' => 'local',</p>\n\n<p>$temp<em>file</em>path = Input::file('pin<em>img</em>file')->getRealPath();\n                $file<em>contents = file</em>get<em>contents($temp</em>file<em>path);\n                Flysystem::put($filename, $file</em>contents);\n                unset($temp<em>file</em>path);</p>\n\n<p>'awss3' => array( <br />\n            'driver'    => 'awss3',\n            'key'       => 'your-key',\n            'secret'    => 'your-secret',\n            'bucket'    => 'your-bucket',\n            'region'    => 'your-region',\n            // 'base_url'  => 'your-url',\n            // 'prefix'    => 'your-prefix',\n            // 'eventable' => true,\n            // 'cache'     => 'foo'\n        ),</p>\n\n<p>return Flysystem::getAdapter()->getClient()->getObjectUrl('pin-here.com', $this->pin<em>img</em>file, '+5 minutes');</p>","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-10-13T07:44:22.000Z","created_by":1,"updated_at":"2014-10-13T11:55:38.000Z","updated_by":1,"published_at":null,"published_by":null},{"id":44,"uuid":"05283ac4-10f7-4f96-8578-11fc85b48952","title":"Laravel 學習筆記 Lesson 11","slug":"laravel-xue-xi-bi-ji-lesson-11","markdown":"### 使用 laravel mail 串接 mailgun\n\n\"guzzlehttp/guzzle\": \"~4.0\"\n\n先在 app/config/mail.php 設定檔中將 driver 設定為 mailgun\n\napp/config/services.php\n\n'mailgun' => array(\n    'domain' => 'your-mailgun-domain',\n    'secret' => 'your-mailgun-key',\n),\n\nRoute::get('/test-mail/{demo}', 'HomeController@getTestMail');\n\npublic function getTestMail($demo)\n\t{\n\n\t\t$to = 'fukuball@gmail.com';\n\t\t$to_name = 'Fukuball Lin';\n\t\t$subject = 'Welcome!';\n\n\t\tswitch ($demo) {\n\n\t\tdefault:\n\t\tcase '1':\n\n\t\t\t$data = array('data1'=>'this is data 1', 'data2'=>'this is data 2');\n\n\t\t\tMail::send('emails.welcome-html', $data, function($message) use ($to, $to_name, $subject)\n\t\t\t{\n    \t\t\t$message->to($to, $to_name)->subject($subject);\n\t\t\t});\n\n\t\t\techo 'demo 1 mail send';\n\n\t\t\tbreak;\n\n\t\t}\n\n\t}\n    \n    \ncase '2':\n\n\t\t\t$data = array('data1'=>'this is data 1', 'data2'=>'this is data 2');\n\n\t\t\tMail::send(array('emails.welcome-html', 'emails.welcome-text'), $data, function($message) use ($to, $to_name, $subject)\n\t\t\t{\n    \t\t\t$message->to($to, $to_name)->subject($subject);\n\t\t\t});\n\n\t\t\techo 'demo 2 mail send';\n\n\t\t\tbreak;\n            \n            \n  case '3':\n\n\t\t\t$data = array('data1'=>'this is data 1', 'data2'=>'this is data 2');\n\n\t\t\tMail::send(array('emails.welcome-html', 'emails.welcome-text'), $data, function($message) use ($to, $to_name, $subject, $from, $from_name)\n\t\t\t{\n\t\t\t\t$message->from($from, $from_name);\n    \t\t\t$message->to($to, $to_name)->cc('fukuball@indievox.com')->subject($subject);\n    \t\t\t$message->attach(public_path().'/robots.txt', array('as' => '檔案名稱'));\n\t\t\t});\n\n\t\t\techo 'demo 3 mail send';\n\n\t\t\tbreak;","html":"<h3 id=\"laravelmailmailgun\">使用 laravel mail 串接 mailgun</h3>\n\n<p>\"guzzlehttp/guzzle\": \"~4.0\"</p>\n\n<p>先在 app/config/mail.php 設定檔中將 driver 設定為 mailgun</p>\n\n<p>app/config/services.php</p>\n\n<p>'mailgun' => array( <br />\n    'domain' => 'your-mailgun-domain',\n    'secret' => 'your-mailgun-key',\n),</p>\n\n<p>Route::get('/test-mail/{demo}', 'HomeController@getTestMail');</p>\n\n<p>public function getTestMail($demo) <br />\n    {</p>\n\n<pre><code>    $to = 'fukuball@gmail.com';\n    $to_name = 'Fukuball Lin';\n    $subject = 'Welcome!';\n\n    switch ($demo) {\n\n    default:\n    case '1':\n\n        $data = array('data1'=&gt;'this is data 1', 'data2'=&gt;'this is data 2');\n\n        Mail::send('emails.welcome-html', $data, function($message) use ($to, $to_name, $subject)\n        {\n            $message-&gt;to($to, $to_name)-&gt;subject($subject);\n        });\n\n        echo 'demo 1 mail send';\n\n        break;\n\n    }\n\n}\n</code></pre>\n\n<p>case '2':</p>\n\n<pre><code>        $data = array('data1'=&gt;'this is data 1', 'data2'=&gt;'this is data 2');\n\n        Mail::send(array('emails.welcome-html', 'emails.welcome-text'), $data, function($message) use ($to, $to_name, $subject)\n        {\n            $message-&gt;to($to, $to_name)-&gt;subject($subject);\n        });\n\n        echo 'demo 2 mail send';\n\n        break;\n</code></pre>\n\n<p>case '3':</p>\n\n<pre><code>        $data = array('data1'=&gt;'this is data 1', 'data2'=&gt;'this is data 2');\n\n        Mail::send(array('emails.welcome-html', 'emails.welcome-text'), $data, function($message) use ($to, $to_name, $subject, $from, $from_name)\n        {\n            $message-&gt;from($from, $from_name);\n            $message-&gt;to($to, $to_name)-&gt;cc('fukuball@indievox.com')-&gt;subject($subject);\n            $message-&gt;attach(public_path().'/robots.txt', array('as' =&gt; '檔案名稱'));\n        });\n\n        echo 'demo 3 mail send';\n\n        break;\n</code></pre>","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-10-13T08:32:11.000Z","created_by":1,"updated_at":"2014-10-13T09:49:53.000Z","updated_by":1,"published_at":null,"published_by":null},{"id":45,"uuid":"f6ca712d-0366-489f-95e2-63d9b1f7f47e","title":"Laravel 學習筆記 Lesson 11","slug":"laravel-xue-xi-bi-ji-lesson-11-2","markdown":"### 使用 Cache\n\napp/config/cache.php\n\nCache::put('key', 'value', $minutes);\n\nif (Cache::has('key'))\n{\n    //\n}\n\n$value = Cache::get('key');\n\nif (Cache::has('users-all')) {\n            $users = Cache::get('users-all');\n            echo \"From Cache\";\n        } else {\n            $users = User::all();\n            Cache::put('users-all', $users, 3);\n            echo \"From Database\";\n        }\n\n$value = Cache::remember('users', $minutes, function()\n{\n    return DB::table('users')->get();\n});","html":"<h3 id=\"cache\">使用 Cache</h3>\n\n<p>app/config/cache.php</p>\n\n<p>Cache::put('key', 'value', $minutes);</p>\n\n<p>if (Cache::has('key')) <br />\n{\n    //\n}</p>\n\n<p>$value = Cache::get('key');</p>\n\n<p>if (Cache::has('users-all')) { <br />\n            $users = Cache::get('users-all');\n            echo \"From Cache\";\n        } else {\n            $users = User::all();\n            Cache::put('users-all', $users, 3);\n            echo \"From Database\";\n        }</p>\n\n<p>$value = Cache::remember('users', $minutes, function()\n{\n    return DB::table('users')->get();\n});</p>","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-10-21T09:58:52.000Z","created_by":1,"updated_at":"2014-10-21T09:59:13.000Z","updated_by":1,"published_at":null,"published_by":null},{"id":46,"uuid":"bb65959c-9304-4523-aa2a-2a00f5ac94cd","title":"Laravel 學習筆記 Lesson 12","slug":"laravel-xue-xi-bi-ji-lesson-12","markdown":"Crawler\n\n\"repositories\": [\n    {\n      \"url\": \"https://github.com/everlaat/laravel4-goutte.git\",\n      \"type\": \"git\"\n    }\n  ],\n  \n  {\n  \"require\": {\n    \"laravel/framework\": \"4.0.*\",\n    ...\n    \"elvedia/goutte\": \"1.0.*\"\n  }\n  ...\n}\n\ncomposer update\n\n'providers' => array(\n    ...\n    'Elvedia\\Goutte\\Providers\\GoutteServiceProvider',\n),\n\n'aliases' => array(\n    ...\n  'Goutte' => 'Elvedia\\Goutte\\Facades\\Laravel\\GoutteFacade',\n),\n\n\nuse Symfony\\Component\\DomCrawler\\Crawler;\n\n\n$crawler = new Crawler();\n\t\t\t$crawler->addContent('<html><body><p>Hello World!</p></body></html>');\n\t\t\techo $crawler->filter('body > p')->text();\n            \nhttps://github.com/everlaat/laravel4-goutte\nhttps://github.com/fabpot/goutte\nhttps://github.com/symfony/DomCrawler\nhttp://symfony.com/doc/current/components/dom_crawler.html\n\nuse Goutte\\Client;\n\n$client = new Client();\n\t\t\t$crawler = $client->request('GET', 'http://zh.wikipedia.org/wiki/%E4%B9%94%E7%BA%B3%E6%96%AF%C2%B7%E7%B4%A2%E5%B0%94%E5%85%8B');\n\t\t\t$crawler->filter('a')->each(function ($node) {\n\t\t\t\techo $node->text().\"<br/>\";\n\t\t\t\techo $node->attr('href').\"<br/>\";\n\t\t\t});\n            \n            \n$client = new Client();\n\t\t\t$crawler = $client->request('GET', 'http://www.indievox.com/song/1');\n\n\t\t\t$title_node = $crawler->filter('title');\n\t\t\techo \"title<br/>\";\n\t\t\techo $title_node->text().\"<br/>\";\n\n\t\t\t$crawler->filter('meta[property=\"og:title\"]')->each(function ($node) {\n\t\t\t\techo \"og:title<br/>\";\n\t\t\t\techo $node->attr('content').\"<br/>\";\n\t\t\t});\n\n\t\t\t$crawler->filter('meta[property=\"og:url\"]')->each(function ($node) {\n\t\t\t\techo \"og:url<br/>\";\n\t\t\t\techo $node->attr('content').\"<br/>\";\n\t\t\t});\n\n\t\t\t$crawler->filter('meta[property=\"og:image\"]')->each(function ($node) {\n\t\t\t\techo \"og:image<br/>\";\n\t\t\t\techo $node->attr('content').\"<br/>\";\n\t\t\t});\n\n\t\t\t$crawler->filter('meta[property=\"og:description\"]')->each(function ($node) {\n\t\t\t\techo \"og:description<br/>\";\n\t\t\t\techo $node->attr('content').\"<br/>\";\n\t\t\t});","html":"<p>Crawler</p>\n\n<p>\"repositories\": [ <br />\n    {\n      \"url\": \"<a href='https://github.com/everlaat/laravel4-goutte.git'>https://github.com/everlaat/laravel4-goutte.git</a>\",\n      \"type\": \"git\"\n    }\n  ],</p>\n\n<p>{\n  \"require\": {\n    \"laravel/framework\": \"4.0.<em>\",\n    ...\n    \"elvedia/goutte\": \"1.0.</em>\"\n  }\n  ...\n}</p>\n\n<p>composer update</p>\n\n<p>'providers' => array( <br />\n    ...\n    'Elvedia\\Goutte\\Providers\\GoutteServiceProvider',\n),</p>\n\n<p>'aliases' => array( <br />\n    ...\n  'Goutte' => 'Elvedia\\Goutte\\Facades\\Laravel\\GoutteFacade',\n),</p>\n\n<p>use Symfony\\Component\\DomCrawler\\Crawler;</p>\n\n<p>$crawler = new Crawler();\n            $crawler->addContent('<html><body><p>Hello World!</p></body></html>');\n            echo $crawler->filter('body > p')->text();</p>\n\n<p><a href='https://github.com/everlaat/laravel4-goutte'>https://github.com/everlaat/laravel4-goutte</a> <br />\n<a href='https://github.com/fabpot/goutte'>https://github.com/fabpot/goutte</a> <br />\n<a href='https://github.com/symfony/DomCrawler'>https://github.com/symfony/DomCrawler</a> <br />\n<a href='http://symfony.com/doc/current/components/dom_crawler.html'>http://symfony.com/doc/current/components/dom_crawler.html</a></p>\n\n<p>use Goutte\\Client;</p>\n\n<p>$client = new Client();\n            $crawler = $client->request('GET', '<a href='http://zh.wikipedia.org/wiki/%E4%B9%94%E7%BA%B3%E6%96%AF%C2%B7%E7%B4%A2%E5%B0%94%E5%85%8B'>http://zh.wikipedia.org/wiki/%E4%B9%94%E7%BA%B3%E6%96%AF%C2%B7%E7%B4%A2%E5%B0%94%E5%85%8B</a>');\n            $crawler->filter('a')->each(function ($node) {\n                echo $node->text().\"<br/>\";\n                echo $node->attr('href').\"<br/>\";\n            });</p>\n\n<p>$client = new Client();\n            $crawler = $client->request('GET', '<a href='http://www.indievox.com/song/1'>http://www.indievox.com/song/1</a>');</p>\n\n<pre><code>        $title_node = $crawler-&gt;filter('title');\n        echo \"title&lt;br/&gt;\";\n        echo $title_node-&gt;text().\"&lt;br/&gt;\";\n\n        $crawler-&gt;filter('meta[property=\"og:title\"]')-&gt;each(function ($node) {\n            echo \"og:title&lt;br/&gt;\";\n            echo $node-&gt;attr('content').\"&lt;br/&gt;\";\n        });\n\n        $crawler-&gt;filter('meta[property=\"og:url\"]')-&gt;each(function ($node) {\n            echo \"og:url&lt;br/&gt;\";\n            echo $node-&gt;attr('content').\"&lt;br/&gt;\";\n        });\n\n        $crawler-&gt;filter('meta[property=\"og:image\"]')-&gt;each(function ($node) {\n            echo \"og:image&lt;br/&gt;\";\n            echo $node-&gt;attr('content').\"&lt;br/&gt;\";\n        });\n\n        $crawler-&gt;filter('meta[property=\"og:description\"]')-&gt;each(function ($node) {\n            echo \"og:description&lt;br/&gt;\";\n            echo $node-&gt;attr('content').\"&lt;br/&gt;\";\n        });\n</code></pre>","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-10-28T10:59:45.000Z","created_by":1,"updated_at":"2014-10-28T11:44:01.000Z","updated_by":1,"published_at":null,"published_by":null},{"id":47,"uuid":"da91483b-2da3-4c30-bdfe-00bc16f368bb","title":"Laravel 學習筆記 Lesson 13","slug":"laravel-xue-xi-bi-ji-lesson-13","markdown":"command\n\ncea\n\nprotected $name = 'crawler:fetch-data';\n\nprotected $description = 'Use this command to get data from web page.';\n\npublic function fire()\n\t{\n\t\t$arguments = $this->argument();\n\t\t$file_path = $this->argument('file_path');\n\t\techo \"Hello World! Fetch $file_path \\n\";\n\t}\n\napp/start/artisan.php\n\nArtisan::add(new CrawlerCommand);\n\nphp artisan crawler:fetch_data /var/www/list.txt\nHello World! Fetch /var/www/list.txt\n\nphp artisan command:make FooCommand\n\nprotected $name = 'foo:bar';\n\npublic function fire()\n\t{\n\t\t$arguments = $this->argument();\n\t\tprint_r($arguments);\n\t\t$options = $this->option();\n\t\tprint_r($options);\n\t\t$this->info('Display this on the screen');\n\t\tif ($this->confirm('Do you wish to continue? [yes|no]'))\n\t\t{\n    \t\t$name = $this->ask('What is your name?');\n    \t\t$this->info('Your name: '.$name);\n\t\t}\n\t}\n    \n    \n return array(\n\t\t\tarray('a1', InputArgument::REQUIRED, 'An example argument.'),\n\t\t\tarray('a2', InputArgument::REQUIRED, 'An example argument.'),\n\t\t);\n        \n return array(\n\t\t\tarray('o1', null, InputOption::VALUE_OPTIONAL, 'An example option.', null),\n\t\t\tarray('o2', null, InputOption::VALUE_OPTIONAL, 'An example option.', null),\n\t\t);","html":"<p>command</p>\n\n<p>cea</p>\n\n<p>protected $name = 'crawler:fetch-data';</p>\n\n<p>protected $description = 'Use this command to get data from web page.';</p>\n\n<p>public function fire() <br />\n    {\n        $arguments = $this->argument();\n        $file<em>path = $this->argument('file</em>path');\n        echo \"Hello World! Fetch $file_path \\n\";\n    }</p>\n\n<p>app/start/artisan.php</p>\n\n<p>Artisan::add(new CrawlerCommand);</p>\n\n<p>php artisan crawler:fetch_data /var/www/list.txt <br />\nHello World! Fetch /var/www/list.txt</p>\n\n<p>php artisan command:make FooCommand</p>\n\n<p>protected $name = 'foo:bar';</p>\n\n<p>public function fire() <br />\n    {\n        $arguments = $this->argument();\n        print<em>r($arguments);\n        $options = $this->option();\n        print</em>r($options);\n        $this->info('Display this on the screen');\n        if ($this->confirm('Do you wish to continue? [yes|no]'))\n        {\n            $name = $this->ask('What is your name?');\n            $this->info('Your name: '.$name);\n        }\n    }</p>\n\n<p>return array(\n            array('a1', InputArgument::REQUIRED, 'An example argument.'),\n            array('a2', InputArgument::REQUIRED, 'An example argument.'),\n        );</p>\n\n<p>return array(\n            array('o1', null, InputOption::VALUE<em>OPTIONAL, 'An example option.', null),\n            array('o2', null, InputOption::VALUE</em>OPTIONAL, 'An example option.', null),\n        );</p>","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-10-28T11:55:48.000Z","created_by":1,"updated_at":"2014-10-29T05:40:21.000Z","updated_by":1,"published_at":null,"published_by":null},{"id":48,"uuid":"d0964320-55a7-4bbe-a2aa-5aa127b67444","title":"Laravel 學習筆記 Lesson 14","slug":"laravel-xue-xi-bi-ji-lesson-14","markdown":"### 忘記密碼\n\nuse Illuminate\\Auth\\Reminders\\RemindableTrait;\nuse Illuminate\\Auth\\Reminders\\RemindableInterface;\n\nclass User extends Eloquent implements RemindableInterface {\n\n    use RemindableTrait;\n\n}\n\nphp artisan auth:reminders-table\n\nphp artisan migrate\n\nphp artisan auth:reminders-controller\n\nRoute::get('/remind', 'RemindersController@getRemind');\n\npublic function getRemind()\n\t{\n\t\treturn View::make('password.remind');\n\t}\n    \n<form action=\"/remind\" method=\"POST\">\n    <input type=\"email\" name=\"email\">\n    <input type=\"submit\" value=\"Send Reminder\">\n</form>\n\nRoute::post('/remind', 'RemindersController@postRemind');\n\nPassword::remind(Input::only('email'), function($message)\n{\n    $message->subject('Password Reminder');\n});\n\nRoute::get('/password/reset/{token}', 'RemindersController@getReset');\n\n<form class=\"form\" role=\"form\" action=\"/password/reset\" method=\"POST\">\n    <input class=\"form-control\" type=\"hidden\" name=\"token\" value=\"{{ $token }}\">\n    <label class=\"control-label\">Email</label>\n    <input class=\"form-control\" type=\"email\" name=\"email\">\n    <label class=\"control-label\">新密碼</label>\n    <input class=\"form-control\" type=\"password\" name=\"password\">\n    <label class=\"control-label\">確認新密碼</label>\n    <input class=\"form-control\" type=\"password\" name=\"password_confirmation\">\n    <button class=\"btn btn-primary\" type=\"submit\">Reset Password</button>\n</form>\n\nRoute::post('/password/reset', 'RemindersController@postReset');","html":"<h3 id=\"\">忘記密碼</h3>\n\n<p>use Illuminate\\Auth\\Reminders\\RemindableTrait; <br />\nuse Illuminate\\Auth\\Reminders\\RemindableInterface;</p>\n\n<p>class User extends Eloquent implements RemindableInterface {</p>\n\n<pre><code>use RemindableTrait;\n</code></pre>\n\n<p>}</p>\n\n<p>php artisan auth:reminders-table</p>\n\n<p>php artisan migrate</p>\n\n<p>php artisan auth:reminders-controller</p>\n\n<p>Route::get('/remind', 'RemindersController@getRemind');</p>\n\n<p>public function getRemind() <br />\n    {\n        return View::make('password.remind');\n    }</p>\n\n<form action=\"/remind\" method=\"POST\">  \n    <input type=\"email\" name=\"email\">\n    <input type=\"submit\" value=\"Send Reminder\">\n</form>\n\n<p>Route::post('/remind', 'RemindersController@postRemind');</p>\n\n<p>Password::remind(Input::only('email'), function($message) <br />\n{\n    $message->subject('Password Reminder');\n});</p>\n\n<p>Route::get('/password/reset/{token}', 'RemindersController@getReset');</p>\n\n<form class=\"form\" role=\"form\" action=\"/password/reset\" method=\"POST\">  \n    <input class=\"form-control\" type=\"hidden\" name=\"token\" value=\"{{ $token }}\">\n    <label class=\"control-label\">Email</label>\n    <input class=\"form-control\" type=\"email\" name=\"email\">\n    <label class=\"control-label\">新密碼</label>\n    <input class=\"form-control\" type=\"password\" name=\"password\">\n    <label class=\"control-label\">確認新密碼</label>\n    <input class=\"form-control\" type=\"password\" name=\"password_confirmation\">\n    <button class=\"btn btn-primary\" type=\"submit\">Reset Password</button>\n</form>\n\n<p>Route::post('/password/reset', 'RemindersController@postReset');</p>","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-11-04T08:11:01.000Z","created_by":1,"updated_at":"2014-11-04T11:59:42.000Z","updated_by":1,"published_at":null,"published_by":null},{"id":49,"uuid":"67006e58-4d58-406d-bbb9-0450192c1e86","title":"練歌單","slug":"lian-ge-dan","markdown":"Coldplay - The Scientist","html":"<p>Coldplay - The Scientist</p>","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-11-17T07:45:28.000Z","created_by":1,"updated_at":"2014-11-17T07:45:28.000Z","updated_by":1,"published_at":null,"published_by":null},{"id":50,"uuid":"52ae2cd7-45f5-41e5-b1be-e73241bf0e29","title":"Laravel 學習筆記 Lesson 15","slug":"laravel-xue-xi-bi-ji-lesson-15","markdown":"### 在地化\n\nLaravel 的 Lang 類別提供方便的方法來取得多種語言的字串\n\n語言字串儲存在 app/lang 資料夾的檔案裡\n\n新增語言檔 message.php\n\n切換\n\nApp::setLocale('en');\n\necho Lang::get('messages.welcome');\n\necho trans('messages.welcome');\n\n使用變數\n\n'hi' => '嗨， :name'\n\necho trans('messages.hi', array('name' => 'Fukuball'));\n\n複數\n\n'apples' => '{0} There are none|[1,19] There are some|[20,Inf] There are many',\n\necho Lang::choice('messages.apples', 10);\n\n'dogs' => '{1} There is :num dog|[2,Inf] There are :num dogs',\n\necho Lang::choice('messages.dogs', 1, array('num' => 1));\n\n","html":"<h3 id=\"\">在地化</h3>\n\n<p>Laravel 的 Lang 類別提供方便的方法來取得多種語言的字串</p>\n\n<p>語言字串儲存在 app/lang 資料夾的檔案裡</p>\n\n<p>新增語言檔 message.php</p>\n\n<p>切換</p>\n\n<p>App::setLocale('en');</p>\n\n<p>echo Lang::get('messages.welcome');</p>\n\n<p>echo trans('messages.welcome');</p>\n\n<p>使用變數</p>\n\n<p>'hi' => '嗨， :name'</p>\n\n<p>echo trans('messages.hi', array('name' => 'Fukuball'));</p>\n\n<p>複數</p>\n\n<p>'apples' => '{0} There are none|[1,19] There are some|[20,Inf] There are many',</p>\n\n<p>echo Lang::choice('messages.apples', 10);</p>\n\n<p>'dogs' => '{1} There is :num dog|[2,Inf] There are :num dogs',</p>\n\n<p>echo Lang::choice('messages.dogs', 1, array('num' => 1));</p>","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-11-17T12:48:34.000Z","created_by":1,"updated_at":"2014-11-17T13:12:24.000Z","updated_by":1,"published_at":null,"published_by":null},{"id":51,"uuid":"0a0795d2-4e11-4622-ba5b-b3d9bf33c4f9","title":"Laravel 學習筆記 Lesson 16","slug":"laravel-xue-xi-bi-ji-lesson-16","markdown":"### 分頁\n\n$allUsers = User::paginate(15);\n\n<?php echo $allUsers->links(); ?>\n\n$allUsers = User::simplePaginate(15);\n\n<?php echo $allUsers->links('pagination::simple'); ?>\n\n<?php\n    $presenter = new Illuminate\\Pagination\\BootstrapPresenter($paginator);\n\n    $trans = $environment->getTranslator();\n?>\n\n<?php if ($paginator->getLastPage() > 1): ?>\n    <ul class=\"pager custom\">\n        <?php\n            echo $presenter->getPrevious($trans->trans('pagination.previous'));\n\n            echo $presenter->getNext($trans->trans('pagination.next'));\n        ?>\n    </ul>\n<?php endif; ?>\n\n\n<?php echo $allUsers->links('pagination.my-links'); ?>","html":"<h3 id=\"\">分頁</h3>\n\n<p>$allUsers = User::paginate(15);</p>\n\n<?php echo $allUsers->links(); ?>\n\n<p>$allUsers = User::simplePaginate(15);</p>\n\n<?php echo $allUsers->links('pagination::simple'); ?>\n\n<?php  \n    $presenter = new Illuminate\\Pagination\\BootstrapPresenter($paginator);\n\n    $trans = $environment->getTranslator();\n?>\n\n<?php if ($paginator->getLastPage() > 1): ?>  \n\n<pre><code>&lt;ul class=\"pager custom\"&gt;\n    &lt;?php\n        echo $presenter-&gt;getPrevious($trans-&gt;trans('pagination.previous'));\n\n        echo $presenter-&gt;getNext($trans-&gt;trans('pagination.next'));\n    ?&gt;\n&lt;/ul&gt;\n</code></pre>\n\n<?php endif; ?>\n\n<?php echo $allUsers->links('pagination.my-links'); ?>","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-11-17T13:14:09.000Z","created_by":1,"updated_at":"2014-11-17T13:37:38.000Z","updated_by":1,"published_at":null,"published_by":null},{"id":52,"uuid":"6f8e90e4-9fa0-4f5a-a33d-b0ba9581a4fc","title":"Laravel 學習筆記 Lesson 17","slug":"laravel-xue-xi-bi-ji-lesson-17","markdown":"### 單元測試\n\n\"phpunit/phpunit\": \"4.0.*\",\n\ncomposer update\n\nvendor/bin/phpunit \n\n<?php\n\nclass ControllerTest extends TestCase {\n\n    /**\n     * A basic functional test example.\n     *\n     * @return void\n     */\n    public function testHomeControllerGetIndex()\n    {\n\n        $this->call('GET', '/');\n        $this->assertResponseOk();\n\n    }\n\n}\n\n登入狀態如何測試\n\npublic function testHomeControllerGetIndex()\n    {\n\n        $crawler = $this->client->request('GET', '/index.html');\n        $this->assertTrue($this->client->getResponse()->isOk());\n        $this->assertCount(0, $crawler->filter('a:contains(\"Fukuball Lin\")'));\n\n        $user = User::find(8);\n        $this->be($user);\n        $crawler = $this->client->request('GET', '/index.html');\n        $this->assertTrue($this->client->getResponse()->isOk());\n        $this->assertCount(1, $crawler->filter('a:contains(\"Fukuball Lin\")'));\n\n    }","html":"<h3 id=\"\">單元測試</h3>\n\n<p>\"phpunit/phpunit\": \"4.0.*\",</p>\n\n<p>composer update</p>\n\n<p>vendor/bin/phpunit </p>\n\n<p><?php</p>\n\n<p>class ControllerTest extends TestCase {</p>\n\n<pre><code>/**\n * A basic functional test example.\n *\n * @return void\n */\npublic function testHomeControllerGetIndex()\n{\n\n    $this-&gt;call('GET', '/');\n    $this-&gt;assertResponseOk();\n\n}\n</code></pre>\n\n<p>}</p>\n\n<p>登入狀態如何測試</p>\n\n<p>public function testHomeControllerGetIndex() <br />\n    {</p>\n\n<pre><code>    $crawler = $this-&gt;client-&gt;request('GET', '/index.html');\n    $this-&gt;assertTrue($this-&gt;client-&gt;getResponse()-&gt;isOk());\n    $this-&gt;assertCount(0, $crawler-&gt;filter('a:contains(\"Fukuball Lin\")'));\n\n    $user = User::find(8);\n    $this-&gt;be($user);\n    $crawler = $this-&gt;client-&gt;request('GET', '/index.html');\n    $this-&gt;assertTrue($this-&gt;client-&gt;getResponse()-&gt;isOk());\n    $this-&gt;assertCount(1, $crawler-&gt;filter('a:contains(\"Fukuball Lin\")'));\n\n}\n</code></pre>","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-11-17T13:42:33.000Z","created_by":1,"updated_at":"2014-12-09T07:37:40.000Z","updated_by":1,"published_at":null,"published_by":null},{"id":53,"uuid":"7c74e5e6-a973-4e7e-9f59-2fe4b2db8400","title":"如何找出占用 Port 的 Process 並將之關閉","slug":"ru-he-zhao-chu-zhan-yong-port-de-process-bing-jiang-zhi-guan-bi","markdown":"開服務時有時難免會遇到 Port 被占用的情況，這時就需要找出哪些 Process 占用了這個 Port，並強制關閉 Process，如此才能夠再度使用這個 Port。\n\n假設現在 1337 Port 被占用了，我們可以使用以下指令找出占用 Port 的 Process：\n\n```\n$ lsof -i tcp:1337\n\n// 輸出結果\nCOMMAND   PID USER   FD   TYPE  DEVICE SIZE/OFF NODE NAME\nphp     26267 root    5u  IPv4 4542269      0t0  TCP *:1337 (LISTEN)\nphp     26267 root    7u  IPv4 4542295      0t0  TCP \nphp     26267 root    8u  IPv4 4542296      0t0  TCP\n```\n\n然後我們就可以使用 PID 來關閉 Process：\n\n```\n$ kill -9 26267\n```","html":"<p>開服務時有時難免會遇到 Port 被占用的情況，這時就需要找出哪些 Process 占用了這個 Port，並強制關閉 Process，如此才能夠再度使用這個 Port。</p>\n\n<p>假設現在 1337 Port 被占用了，我們可以使用以下指令找出占用 Port 的 Process：</p>\n\n<pre><code>$ lsof -i tcp:1337\n\n// 輸出結果\nCOMMAND   PID USER   FD   TYPE  DEVICE SIZE/OFF NODE NAME  \nphp     26267 root    5u  IPv4 4542269      0t0  TCP *:1337 (LISTEN)  \nphp     26267 root    7u  IPv4 4542295      0t0  TCP  \nphp     26267 root    8u  IPv4 4542296      0t0  TCP  \n</code></pre>\n\n<p>然後我們就可以使用 PID 來關閉 Process：</p>\n\n<pre><code>$ kill -9 26267\n</code></pre>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-11-25T11:13:25.000Z","created_by":1,"updated_at":"2014-11-25T11:14:32.000Z","updated_by":1,"published_at":"2014-11-25T11:14:14.000Z","published_by":1},{"id":54,"uuid":"d124bd39-5898-402b-861d-7e0a12b38cd8","title":"在 Linux 系統背後執行一個程序","slug":"zai-linux-xi-tong-bei-hou-zhi-xing-ge-cheng-xu","markdown":"當有一個程序需要不斷的偵測請求，我們通常需要背景執行一個程序，且不能讓程序關閉，比較正規的做法當然就是直接開一個伺服器服務來執行這個程序，但有時我們會不希望為了一個小程序而去開服務，這時可以用以下這個指令來讓系統背後執行一個程序：\n\n```\n$ nohup node server.js &\n```\n\n以上例子就會讓系統背後執行 node.js 的預設 server，即使關閉 terminal 也會持續執行，若想關閉，就需要用 kill 的方式來將程序關閉。","html":"<p>當有一個程序需要不斷的偵測請求，我們通常需要背景執行一個程序，且不能讓程序關閉，比較正規的做法當然就是直接開一個伺服器服務來執行這個程序，但有時我們會不希望為了一個小程序而去開服務，這時可以用以下這個指令來讓系統背後執行一個程序：</p>\n\n<pre><code>$ nohup node server.js &amp;\n</code></pre>\n\n<p>以上例子就會讓系統背後執行 node.js 的預設 server，即使關閉 terminal 也會持續執行，若想關閉，就需要用 kill 的方式來將程序關閉。</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-11-25T11:24:52.000Z","created_by":1,"updated_at":"2014-11-25T11:25:06.000Z","updated_by":1,"published_at":"2014-11-25T11:24:59.000Z","published_by":1},{"id":55,"uuid":"077034e0-8fea-49bc-986a-897da1da13e2","title":"Laravel 學習筆記 Lesson 18","slug":"laravel-xue-xi-bi-ji-lesson-18","markdown":"即時訊息 WAMP Server\n\nhttps://github.com/sidneywidmer/Latchet\n\nnohup php artisan latchet:listen &","html":"<p>即時訊息 WAMP Server</p>\n\n<p><a href='https://github.com/sidneywidmer/Latchet'>https://github.com/sidneywidmer/Latchet</a></p>\n\n<p>nohup php artisan latchet:listen &amp;</p>","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-12-02T08:21:56.000Z","created_by":1,"updated_at":"2014-12-09T08:10:37.000Z","updated_by":1,"published_at":null,"published_by":null},{"id":56,"uuid":"a52d53e9-5c1b-46a7-98d7-4c81376ef87c","title":"Laravel 學習筆記 Lesson 19","slug":"laravel-xue-xi-bi-ji-lesson-19","markdown":"WAMP Client\n\nhttps://github.com/bazo/wamp-client","html":"<p>WAMP Client</p>\n\n<p><a href='https://github.com/bazo/wamp-client'>https://github.com/bazo/wamp-client</a></p>","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-12-02T08:27:43.000Z","created_by":1,"updated_at":"2014-12-02T08:27:43.000Z","updated_by":1,"published_at":null,"published_by":null},{"id":57,"uuid":"39f3a3ce-8ac8-419b-962f-41d66af75d91","title":"Laravel 學習筆記 Lesson 20","slug":"laravel-xue-xi-bi-ji-lesson-20","markdown":"增加 Helper Class\n\nhttp://www.mackhankins.com/blog/defining-your-own-helper-classes-in-laravel-4\n\ncomposer dump-autoload","html":"<p>增加 Helper Class</p>\n\n<p><a href='http://www.mackhankins.com/blog/defining-your-own-helper-classes-in-laravel-4'>http://www.mackhankins.com/blog/defining-your-own-helper-classes-in-laravel-4</a></p>\n\n<p>composer dump-autoload</p>","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2014-12-02T08:28:36.000Z","created_by":1,"updated_at":"2014-12-02T08:31:50.000Z","updated_by":1,"published_at":null,"published_by":null},{"id":58,"uuid":"96b50e9a-64ef-49e7-be23-a9bf18018a8a","title":"Laravel 學習筆記 Lesson 9-2","slug":"laravel-xue-xi-bi-ji-lesson-9-2","markdown":"### 設定 Table 之間 Many to Many 的關係\n\n我們在 [Laravel 學習筆記 Lesson 6](http://blog.fukuball.com/laravel-xue-xi-bi-ji-lesson-6/) 曾經說明過如何設定 Laravel 中的 Model One to One 的關係，也在 [Laravel 學習筆記 Lesson 9-1](http://blog.fukuball.com/laravel-xue-xi-bi-ji-lesson-9/) 說明過如何設定 Laravel 中的 Model One to Many 的關係，在這邊我們繼續說明如何設定 ORM 之間 Many to Many 的關係。\n\n假設現在我們的資料有使用者（User）及文章（Post），我們會先建立好 User 的 Model 及 Post 的 Model，通常一個使用者會有很多篇文章，而文章如果可以由多位使用者共同編輯，使用者及文章就是一種多對多的關係。\n\n要在 Laravel 中的 Model 建立多對多的關係需要有一個中介的 table，這樣的資料庫結構設計也才會符合正規化，所有的多對多關係都可以拆解成一對多的關係，所以在這邊我們還會有一個中介 table，user_post_mappings，這個 tabel 至少會有兩個欄位 user_id 及 post_id，記錄 user 與 post 多對多的資料，如此我們就可以在 User Model 及 Post Model 建立多對多的關係。\n\n在 User Model 建立與 Post 的多對多關係的範例程式碼如下：\n\n```php\nclass User extends Eloquent {\n\n    public function posts()\n    {\n        return $this->belongsToMany('Post', 'user_post_mappings', 'user_id', 'post_id');\n        //return $this->belongsToMany('model_name', 'mapping_table_name', 'foreign_key', 'other_foreign_key');\n    }\n\n}\n```\n\n使用 belongsToMany 來設定，這個意思就是 User Model 屬於多個 Post，且透過 user_post_mappings 來找到對應關係，後面兩個參數一個是 user_id，一個是 post_id，皆是 user_post_mappings 的 foreign_key，分別對應到 User Model 及 Post Model 的 id。\n\n同樣的，在 Post Model 建立與 User 的多對多關係的範例程式碼如下：\n\n```php\nclass Post extends Eloquent {\n\n    public function users()\n    {\n        return $this->belongsToMany('User', 'user_post_mappings', 'post_id', 'user_id');\n        //return $this->belongsToMany('model_name', 'mapping_table_name', 'foreign_key', 'other_foreign_key');\n    }\n\n}\n```\n\n使用 belongsToMany 來設定，這個意思就是 Post Model 屬於多個 User，且透過 user_post_mappings 來找到對應關係，後面兩個參數與 User Model 中的設定反過來，一個是 post_id，一個是 user_id，皆是 user_post_mappings 的 foreign_key，分別對應到 Post Model 及 User Model 的 id。\n\n如此我們就設定好 User Model 與 Post Model 之間的 Many to Many 對應關係了～\n\n設定好 table 之間的 Many to Many 關係可以讓我們更方便地使用資料庫的資料，若果沒有設定 table 之間 Many to Many 的關係，我要取出 User 的 post 資料或 Post 的 User 資料就要使用 Query Builder 來 Join 取出資料，但如果建立了 Many to Many 關係，我們就可以使用 ORM 的方式來取出資料，比如：\n\n```\n$posts = User::find(1)->posts;\n$users = Post::find(10)->users;\n```\n\n總結來說，如果可以設定 Model 之間的關係，未來在使用資料時會變得很方便，否則就會寫很多 Query 在程式碼中，程式會變得難以維護。但是如果是複雜的 Query 還是可以斟酌使用 Query Builder 來查詢資料。","html":"<h3 id=\"tablemanytomany\">設定 Table 之間 Many to Many 的關係</h3>\n\n<p>我們在 <a href=\"http://blog.fukuball.com/laravel-xue-xi-bi-ji-lesson-6/\">Laravel 學習筆記 Lesson 6</a> 曾經說明過如何設定 Laravel 中的 Model One to One 的關係，也在 <a href=\"http://blog.fukuball.com/laravel-xue-xi-bi-ji-lesson-9/\">Laravel 學習筆記 Lesson 9-1</a> 說明過如何設定 Laravel 中的 Model One to Many 的關係，在這邊我們繼續說明如何設定 ORM 之間 Many to Many 的關係。</p>\n\n<p>假設現在我們的資料有使用者（User）及文章（Post），我們會先建立好 User 的 Model 及 Post 的 Model，通常一個使用者會有很多篇文章，而文章如果可以由多位使用者共同編輯，使用者及文章就是一種多對多的關係。</p>\n\n<p>要在 Laravel 中的 Model 建立多對多的關係需要有一個中介的 table，這樣的資料庫結構設計也才會符合正規化，所有的多對多關係都可以拆解成一對多的關係，所以在這邊我們還會有一個中介 table，user<em>post</em>mappings，這個 tabel 至少會有兩個欄位 user<em>id 及 post</em>id，記錄 user 與 post 多對多的資料，如此我們就可以在 User Model 及 Post Model 建立多對多的關係。</p>\n\n<p>在 User Model 建立與 Post 的多對多關係的範例程式碼如下：</p>\n\n<pre><code class=\"php\">class User extends Eloquent {\n\n    public function posts()\n    {\n        return $this-&gt;belongsToMany('Post', 'user_post_mappings', 'user_id', 'post_id');\n        //return $this-&gt;belongsToMany('model_name', 'mapping_table_name', 'foreign_key', 'other_foreign_key');\n    }\n\n}\n</code></pre>\n\n<p>使用 belongsToMany 來設定，這個意思就是 User Model 屬於多個 Post，且透過 user<em>post</em>mappings 來找到對應關係，後面兩個參數一個是 user<em>id，一個是 post</em>id，皆是 user<em>post</em>mappings 的 foreign_key，分別對應到 User Model 及 Post Model 的 id。</p>\n\n<p>同樣的，在 Post Model 建立與 User 的多對多關係的範例程式碼如下：</p>\n\n<pre><code class=\"php\">class Post extends Eloquent {\n\n    public function users()\n    {\n        return $this-&gt;belongsToMany('User', 'user_post_mappings', 'post_id', 'user_id');\n        //return $this-&gt;belongsToMany('model_name', 'mapping_table_name', 'foreign_key', 'other_foreign_key');\n    }\n\n}\n</code></pre>\n\n<p>使用 belongsToMany 來設定，這個意思就是 Post Model 屬於多個 User，且透過 user<em>post</em>mappings 來找到對應關係，後面兩個參數與 User Model 中的設定反過來，一個是 post<em>id，一個是 user</em>id，皆是 user<em>post</em>mappings 的 foreign_key，分別對應到 Post Model 及 User Model 的 id。</p>\n\n<p>如此我們就設定好 User Model 與 Post Model 之間的 Many to Many 對應關係了～</p>\n\n<p>設定好 table 之間的 Many to Many 關係可以讓我們更方便地使用資料庫的資料，若果沒有設定 table 之間 Many to Many 的關係，我要取出 User 的 post 資料或 Post 的 User 資料就要使用 Query Builder 來 Join 取出資料，但如果建立了 Many to Many 關係，我們就可以使用 ORM 的方式來取出資料，比如：</p>\n\n<pre><code>$posts = User::find(1)-&gt;posts;\n$users = Post::find(10)-&gt;users;\n</code></pre>\n\n<p>總結來說，如果可以設定 Model 之間的關係，未來在使用資料時會變得很方便，否則就會寫很多 Query 在程式碼中，程式會變得難以維護。但是如果是複雜的 Query 還是可以斟酌使用 Query Builder 來查詢資料。</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-04-01T14:16:39.000Z","created_by":1,"updated_at":"2015-07-22T08:26:55.000Z","updated_by":1,"published_at":"2015-07-22T08:26:42.000Z","published_by":1},{"id":59,"uuid":"34f197e5-cab3-40e4-a45f-b9567c6299b4","title":"如何使用 CSS3 Transition","slug":"css3-transition","markdown":"### 前言\n\n我們之前曾經介紹過 [如何使用 CSS3 Animation](http://blog.fukuball.com/rru-he-shi-yong-css3-animation/)，也不小心在該篇文章中說要另文跟大家介紹如何使用 CSS3 Transition，拖稿了近一年，今天終於要來實現諾言了～雖然大家可能根本就不在意，但哥就是真性情的人他媽的當真了啊！\n\n在這個浮誇的時代，如果網頁上沒有酷炫的功能或特效，似乎就遜掉了（幹！開場跟如何使用 CSS3 Animation 這篇文章一模一樣，可以再混一點啊），好啦，不說廢話，總之就是網頁有使用到 CSS3 Transition 馬上就會讓你的網站帥十倍啦！很爽吧！（請注意：本人並不會因此帥十倍）\n\n就讓我們一起來看看 CSS3 Transition 大法怎麼練吧！\n\n### CSS3 Transition 第一級\n\n第一級讓我們先由簡單的範例從做中學，現在讓我們想像一個情境，我們希望滑鼠 hover 至某個 div 元素時，讓這個 div 元素改變背景色，若沒有用轉場（Transition）效果，我們就會看到 div 元素硬生生的改變顏色，一點都不溫柔，這樣橫衝直撞會讓 TA 很不舒服，所以我們才會需要 CSS3 Transition 來讓 TA 舒服一點。\n\n```css\ndiv.example-no-transition {\n\twidth: 580px;\n    padding: 9px 15px;\n    background-color: #FF5050;\n    color: white;\n    margin-bottom: 20px;\n    margin-top: 20px;\n    border-radius: 5px;\n}\ndiv.example-no-transition:hover {\n\tbackground-color: #6666FF;\n}\n```\n這段 CSS 語法就是代表當滑鼠 hover 至 div.example-no-transition 元素時，背景顏色會變成 <code>#6666FF</code>，但就是很生硬的變過去，但當我們加入 transition 那就不一樣了。\n\n```css\ndiv.example-with-transition {\n\twidth: 580px;\n    padding: 9px 15px;\n    background-color: #FF5050;\n    color: white;\n    border-radius: 5px;\n    -webkit-transition: background-color 1s;\n    -moz-transition: background-color 1s;\n    -o-transition: background-color 1s;\n    -ms-transition: background-color 1s;\n    transition: background-color 1s;\n}\ndiv.example-with-transition:hover {\n\tbackground-color: #6666FF;\n}\n```\n\n這段 CSS 語法 transition 的部分就是說 div 元素要如何轉場，<code>background-color 1s</code> 的意思就是說 background-color 會在 1 秒的時間變化成 hover 所指定的背景色。\n\n就是這麼簡單！\n\n特別注意有 <code>-webkit-</code> 這個前綴的 CSS 語法是為了支援 webkit 核心的瀏覽器，例如：Chrome、Safari、Opera 等等。\n\n#### <a href=\"http://codepen.io/fukuball/pen/domeOo\" target=\"_blank\">CSS3 Transition 第一級展示</a>\n\n### CSS3 Transition 第二級\n\nCSS3 Transition 第一級使用的只是預設的轉場效果，總感覺缺少了點什麼，通常龜毛的人不會滿足於預設的效果，所以在 CSS3 Transition 第二級我們會學到如何調整 CSS3 Transition 的轉場效果。\n\n```html\n<div id=\"example-transition\">\n  <div class=\"ease\">ease</div>\n  <div class=\"linear\">linear</div>\n  <div class=\"easein\">ease-in</div>\n  <div class=\"easeout\">ease-out</div>\n  <div class=\"easeinout\">ease-in-out</div>\n</div>\n```\n\n```css\n#example-transition {\n  width: 520px;\n}\n#example-transition div {\n  width: 100px;\n  margin: 5px 0;\n  padding: 5px;\n  color: white;\n  background-color: #FF5050;\n  text-align: right;\n  border-radius: 5px;\n}\n#example-transition:hover div {\n  width: 500px;\n}\n#example-transition div.ease {\n  -webkit-transition: 3s ease;\n  -moz-transition: 3s ease;\n  -o-transition: 3s ease;\n  -ms-transition: 3s ease;\n  transition: 3s ease;\n}\n#example-transition div.linear {\n  -webkit-transition: 3s linear;\n  -moz-transition: 3s linear;\n  -o-transition: 3s linear;\n  -ms-transition: 3s linear;\n  transition: 3s linear;\n}\n#example-transition div.easein {\n  -webkit-transition: 3s ease-in;\n  -moz-transition: 3s ease-in;\n  -o-transition: 3s ease-in;\n  -ms-transition: 3s ease-in;\n  transition: 3s ease-in;\n}\n#example-transition div.easeout {\n  -webkit-transition: 3s ease-out;\n  -moz-transition: 3s ease-out;\n  -o-transition: 3s ease-out;\n  -ms-transition: 3s ease-out;\n  transition: 3s ease-out;\n}\n#example-transition div.easeinout {\n  -webkit-transition: 3s ease-in-out;\n  -moz-transition: 3s ease-in-out;\n  -o-transition: 3s ease-in-out;\n  -ms-transition: 3s ease-in-out;\n  transition: 3s ease-in-out;\n}\n```\n\nCSS3 Transition 讓我們可以使用 timing function 這個參數來調整轉場效果，我們在例子中將所以的 timing function <code>ease</code>、<code>linear</code>、<code>ease-in</code>、<code>ease-out</code>、<code>ease-in-out</code> 一字排開，讓大家可以看清楚各種 timing function 有何異同，至於要使用哪個 timing function 就要看各位施主的 sense 了～\n\n其中請特別注意，我們在這個例子中並沒有在 transition 裡說明哪個 CSS property 要做轉場效果，所以會使用預設模式來做轉場效果，預設模式其實就是 all，也就是說所有可以做轉場效果的 property 都會一起做變化，很方便吧！\n\n#### <a href=\"http://codepen.io/fukuball/pen/OVvdrq\" target=\"_blank\">CSS3 Transition 第二級展示</a>\n\n### CSS3 Transition 第三級\n\nCSS3 Transition 還有一個 delay 的參數可以調整，第三級裡面我們將介紹如何使用 CSS3 Transition 的 delay，這樣可以讓我們的轉場效果有更多的變化。\n\n```html\n<div id=\"example-transition\">\n  <div class=\"ease\">ease</div>\n  <div class=\"linear\">linear</div>\n  <div class=\"easein\">ease-in</div>\n  <div class=\"easeout\">ease-out</div>\n  <div class=\"easeinout\">ease-in-out</div>\n</div>\n```\n\n```css\n#example-transition {\n  width: 520px;\n}\n#example-transition div {\n  width: 100px;\n  margin: 5px 0;\n  padding: 5px;\n  color: white;\n  background-color: #FF5050;\n  text-align: right;\n  border-radius: 5px;\n}\n#example-transition:hover div {\n  width: 500px;\n}\n#example-transition div.ease {\n  -webkit-transition: 1s ease;\n  -moz-transition: 1s ease;\n  -o-transition: 1s ease;\n  -ms-transition: 1s ease;\n  transition: 1s ease;\n}\n#example-transition div.linear {\n  -webkit-transition: 1s linear 1s;\n  -moz-transition: 1s linear 1s;\n  -o-transition: 1s linear 1s;\n  -ms-transition: 1s linear 1s;\n  transition: 1s linear 1s;\n}\n#example-transition div.easein {\n  -webkit-transition: 1s ease-in 2s;\n  -moz-transition: 1s ease-in 2s;\n  -o-transition: 1s ease-in 2s;\n  -ms-transition: 1s ease-in 2s;\n  transition: 1s ease-in 2s;\n}\n#example-transition div.easeout {\n  -webkit-transition: 1s ease-out 3s;\n  -moz-transition: 1s ease-out 3s;\n  -o-transition: 1s ease-out 3s;\n  -ms-transition: 1s ease-out 3s;\n  transition: 1s ease-out 3s;\n}\n#example-transition div.easeinout {\n  -webkit-transition: 1s ease-in-out 4s;\n  -moz-transition: 1s ease-in-out 4s;\n  -o-transition: 1s ease-in-out 4s;\n  -ms-transition: 1s ease-in-out 4s;\n  transition: 1s ease-in-out 4s;\n}\n```\n\n其實我們就只是在 timing function 後面加了一個 delay 秒數而已，就可以做到一層一層的轉場變化效果，學會 CSS3 Transition 第三級之後我們就可以隱約看出 CSS3 Transition 的語法：\n\n```css\ntransition: property duration timing-function delay;\n```\n\n第一個值是指定哪個 property 要變化，可以使用 all，第二個值 duration 是說轉場效果會多久完成，第三個值 timing-function 是說要使用什麼轉場效果有 <code>ease</code>、<code>linear</code>、<code>ease-in</code>、<code>ease-out</code>、<code>ease-in-out</code> 五種，第四個值 delay 是說明要延遲多久才開始轉場效果。 \n\n#### <a href=\"http://codepen.io/fukuball/pen/aOYXgL\" target=\"_blank\">CSS3 Transition 第三級展示</a>\n\n### CSS3 Transition 第四級\n\n剛剛有說到 CSS3 Transition 可以指定 all property 進行轉場變化，其實並不是所有的 property 都可以進行轉場變化，可以進行轉場變化的 property 我們列成下表供大家參考。\n\n<table class=\"table table-striped table-bordered\">\n    <tbody>\n      <tr>\n        <th>Property Name</th>\n        <th>Type</th>\n      </tr>\n      <tr>\n        <td><code>background-color</code></td>\n        <td>color</td>\n      </tr>\n      <tr>\n        <td><code>background-image</code></td>\n        <td>only gradients</td>\n      </tr>\n      <tr>\n        <td><code>background-position</code></td>\n        <td>percentage, length</td>\n      </tr>\n      <tr>\n        <td><code>border-bottom-color</code></td>\n        <td>color</td>\n      </tr>\n      <tr>\n        <td><code>border-bottom-width</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>border-color</code></td>\n        <td>color</td>\n      </tr>\n      <tr>\n        <td><code>border-left-color</code></td>\n        <td>color</td>\n      </tr>\n      <tr>\n        <td><code>border-left-width</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>border-right-color</code></td>\n        <td>color</td>\n      </tr>\n      <tr>\n        <td><code>border-right-width</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>border-spacing</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>border-top-color</code></td>\n        <td>color</td>\n      </tr>\n      <tr>\n        <td><code>border-top-width</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>border-width</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>bottom</code></td>\n        <td>length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>color</code></td>\n        <td>color</td>\n      </tr>\n      <tr>\n        <td><code>crop</code></td>\n        <td>rectangle</td>\n      </tr>\n      <tr>\n        <td><code>font-size</code></td>\n        <td>length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>font-weight</code></td>\n        <td>number</td>\n      </tr>\n      <tr>\n        <td><code>grid-*</code></td>\n        <td>various</td>\n      </tr>\n      <tr>\n        <td><code>height</code></td>\n        <td>length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>left</code></td>\n        <td>length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>letter-spacing</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>line-height</code></td>\n        <td>number, length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>margin-bottom</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>margin-left</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>margin-right</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>margin-top</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>max-height</code></td>\n        <td>length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>max-width</code></td>\n        <td>length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>min-height</code></td>\n        <td>length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>min-width</code></td>\n        <td>length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>opacity</code></td>\n        <td>number</td>\n      </tr>\n      <tr>\n        <td><code>outline-color</code></td>\n        <td>color</td>\n      </tr>\n      <tr>\n        <td><code>outline-offset</code></td>\n        <td>integer</td>\n      </tr>\n      <tr>\n        <td><code>outline-width</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>padding-bottom</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>padding-left</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>padding-right</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>padding-top</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>right</code></td>\n        <td>length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>text-indent</code></td>\n        <td>length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>text-shadow</code></td>\n        <td>shadow</td>\n      </tr>\n      <tr>\n        <td><code>top</code></td>\n        <td>length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>vertical-align</code></td>\n        <td>keywords, length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>visibility</code></td>\n        <td>visibility</td>\n      </tr>\n      <tr>\n        <td><code>width</code></td>\n        <td>length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>word-spacing</code></td>\n        <td>length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>z-index</code></td>\n        <td>integer</td>\n      </tr>\n      <tr>\n        <td><code>zoom</code></td>\n        <td>number</td>\n      </tr>\n    </tbody>\n  </table>\n  \n比如我們現在希望 width 跟 backgournd-color 都可以進行轉場變化，就可以這樣寫：\n\n```html\n<div id=\"example-transition\">\n  <div class=\"ease\">ease</div>\n  <div class=\"linear\">linear</div>\n  <div class=\"easein\">ease-in</div>\n  <div class=\"easeout\">ease-out</div>\n  <div class=\"easeinout\">ease-in-out</div>\n</div>\n```\n\n```css\n#example-transition {\n  width: 520px;\n}\n#example-transition div {\n  width: 100px;\n  margin: 5px 0;\n  padding: 5px;\n  color: white;\n  background-color: #FF5050;\n  text-align: right;\n  border-radius: 5px;\n}\n#example-transition:hover div {\n  width: 500px;\n  background-color: #000000;\n}\n#example-transition div.ease {\n  -webkit-transition: 3s ease;\n  -moz-transition: 3s ease;\n  -o-transition: 3s ease;\n  -ms-transition: 3s ease;\n  transition: 3s ease;\n}\n#example-transition div.linear {\n  -webkit-transition: 3s linear;\n  -moz-transition: 3s linear;\n  -o-transition: 3s linear;\n  -ms-transition: 3s linear;\n  transition: 3s linear;\n}\n#example-transition div.easein {\n  -webkit-transition: 3s ease-in;\n  -moz-transition: 3s ease-in;\n  -o-transition: 3s ease-in;\n  -ms-transition: 3s ease-in;\n  transition: 3s ease-in;\n}\n#example-transition div.easeout {\n  -webkit-transition: 3s ease-out;\n  -moz-transition: 3s ease-out;\n  -o-transition: 3s ease-out;\n  -ms-transition: 3s ease-out;\n  transition: 3s ease-out;\n}\n#example-transition div.easeinout {\n  -webkit-transition: 3s ease-in-out;\n  -moz-transition: 3s ease-in-out;\n  -o-transition: 3s ease-in-out;\n  -ms-transition: 3s ease-in-out;\n  transition: 3s ease-in-out;\n}\n```\n\n我們在 hover 時有兩個 property 不一樣，一個是 <code>width: 500px;</code>、一個是 <code>background-color: #000000;</code>，這兩個 property 都是屬於可以進行轉場效果的，因此這樣寫就會一起變化，很簡單吧！\n\n#### <a href=\"http://codepen.io/fukuball/pen/vORPYQ\" target=\"_blank\">CSS3 Transition 第四級展示</a>\n\n### 結語\n\n我們已經練完了 CSS3 Transition 大法的前四級，其實只要學會這些技巧，大概就能做出不錯的特效，只是動畫如何安排才會吸引人就要看個人的 Sense 了，像我就沒有什麼 Sense～\n\n或許可以去請教對動畫最有 Sense 的 <a href=\"http://ricetseng.com/\" target=\"_blank\">Rice Tseng</a> 公主！\n\n結果跟 CSS3 Animation 那篇文章完全一模一樣！真的好混啊！哈哈哈！！！","html":"<h3 id=\"\">前言</h3>\n\n<p>我們之前曾經介紹過 <a href=\"http://blog.fukuball.com/rru-he-shi-yong-css3-animation/\">如何使用 CSS3 Animation</a>，也不小心在該篇文章中說要另文跟大家介紹如何使用 CSS3 Transition，拖稿了近一年，今天終於要來實現諾言了～雖然大家可能根本就不在意，但哥就是真性情的人他媽的當真了啊！</p>\n\n<p>在這個浮誇的時代，如果網頁上沒有酷炫的功能或特效，似乎就遜掉了（幹！開場跟如何使用 CSS3 Animation 這篇文章一模一樣，可以再混一點啊），好啦，不說廢話，總之就是網頁有使用到 CSS3 Transition 馬上就會讓你的網站帥十倍啦！很爽吧！（請注意：本人並不會因此帥十倍）</p>\n\n<p>就讓我們一起來看看 CSS3 Transition 大法怎麼練吧！</p>\n\n<h3 id=\"css3transition\">CSS3 Transition 第一級</h3>\n\n<p>第一級讓我們先由簡單的範例從做中學，現在讓我們想像一個情境，我們希望滑鼠 hover 至某個 div 元素時，讓這個 div 元素改變背景色，若沒有用轉場（Transition）效果，我們就會看到 div 元素硬生生的改變顏色，一點都不溫柔，這樣橫衝直撞會讓 TA 很不舒服，所以我們才會需要 CSS3 Transition 來讓 TA 舒服一點。</p>\n\n<pre><code class=\"css\">div.example-no-transition {  \n    width: 580px;\n    padding: 9px 15px;\n    background-color: #FF5050;\n    color: white;\n    margin-bottom: 20px;\n    margin-top: 20px;\n    border-radius: 5px;\n}\ndiv.example-no-transition:hover {  \n    background-color: #6666FF;\n}\n</code></pre>\n\n<p>這段 CSS 語法就是代表當滑鼠 hover 至 div.example-no-transition 元素時，背景顏色會變成 <code>#6666FF</code>，但就是很生硬的變過去，但當我們加入 transition 那就不一樣了。</p>\n\n<pre><code class=\"css\">div.example-with-transition {  \n    width: 580px;\n    padding: 9px 15px;\n    background-color: #FF5050;\n    color: white;\n    border-radius: 5px;\n    -webkit-transition: background-color 1s;\n    -moz-transition: background-color 1s;\n    -o-transition: background-color 1s;\n    -ms-transition: background-color 1s;\n    transition: background-color 1s;\n}\ndiv.example-with-transition:hover {  \n    background-color: #6666FF;\n}\n</code></pre>\n\n<p>這段 CSS 語法 transition 的部分就是說 div 元素要如何轉場，<code>background-color 1s</code> 的意思就是說 background-color 會在 1 秒的時間變化成 hover 所指定的背景色。</p>\n\n<p>就是這麼簡單！</p>\n\n<p>特別注意有 <code>-webkit-</code> 這個前綴的 CSS 語法是為了支援 webkit 核心的瀏覽器，例如：Chrome、Safari、Opera 等等。</p>\n\n<h4 id=\"ahrefhttpcodepeniofukuballpendomeootarget_blankcss3transitiona\"><a href=\"http://codepen.io/fukuball/pen/domeOo\" target=\"_blank\">CSS3 Transition 第一級展示</a></h4>\n\n<h3 id=\"css3transition\">CSS3 Transition 第二級</h3>\n\n<p>CSS3 Transition 第一級使用的只是預設的轉場效果，總感覺缺少了點什麼，通常龜毛的人不會滿足於預設的效果，所以在 CSS3 Transition 第二級我們會學到如何調整 CSS3 Transition 的轉場效果。</p>\n\n<pre><code class=\"html\">&lt;div id=\"example-transition\"&gt;  \n  &lt;div class=\"ease\"&gt;ease&lt;/div&gt;\n  &lt;div class=\"linear\"&gt;linear&lt;/div&gt;\n  &lt;div class=\"easein\"&gt;ease-in&lt;/div&gt;\n  &lt;div class=\"easeout\"&gt;ease-out&lt;/div&gt;\n  &lt;div class=\"easeinout\"&gt;ease-in-out&lt;/div&gt;\n&lt;/div&gt;  \n</code></pre>\n\n<pre><code class=\"css\">#example-transition {\n  width: 520px;\n}\n#example-transition div {\n  width: 100px;\n  margin: 5px 0;\n  padding: 5px;\n  color: white;\n  background-color: #FF5050;\n  text-align: right;\n  border-radius: 5px;\n}\n#example-transition:hover div {\n  width: 500px;\n}\n#example-transition div.ease {\n  -webkit-transition: 3s ease;\n  -moz-transition: 3s ease;\n  -o-transition: 3s ease;\n  -ms-transition: 3s ease;\n  transition: 3s ease;\n}\n#example-transition div.linear {\n  -webkit-transition: 3s linear;\n  -moz-transition: 3s linear;\n  -o-transition: 3s linear;\n  -ms-transition: 3s linear;\n  transition: 3s linear;\n}\n#example-transition div.easein {\n  -webkit-transition: 3s ease-in;\n  -moz-transition: 3s ease-in;\n  -o-transition: 3s ease-in;\n  -ms-transition: 3s ease-in;\n  transition: 3s ease-in;\n}\n#example-transition div.easeout {\n  -webkit-transition: 3s ease-out;\n  -moz-transition: 3s ease-out;\n  -o-transition: 3s ease-out;\n  -ms-transition: 3s ease-out;\n  transition: 3s ease-out;\n}\n#example-transition div.easeinout {\n  -webkit-transition: 3s ease-in-out;\n  -moz-transition: 3s ease-in-out;\n  -o-transition: 3s ease-in-out;\n  -ms-transition: 3s ease-in-out;\n  transition: 3s ease-in-out;\n}\n</code></pre>\n\n<p>CSS3 Transition 讓我們可以使用 timing function 這個參數來調整轉場效果，我們在例子中將所以的 timing function <code>ease</code>、<code>linear</code>、<code>ease-in</code>、<code>ease-out</code>、<code>ease-in-out</code> 一字排開，讓大家可以看清楚各種 timing function 有何異同，至於要使用哪個 timing function 就要看各位施主的 sense 了～</p>\n\n<p>其中請特別注意，我們在這個例子中並沒有在 transition 裡說明哪個 CSS property 要做轉場效果，所以會使用預設模式來做轉場效果，預設模式其實就是 all，也就是說所有可以做轉場效果的 property 都會一起做變化，很方便吧！</p>\n\n<h4 id=\"ahrefhttpcodepeniofukuballpenovvdrqtarget_blankcss3transitiona\"><a href=\"http://codepen.io/fukuball/pen/OVvdrq\" target=\"_blank\">CSS3 Transition 第二級展示</a></h4>\n\n<h3 id=\"css3transition\">CSS3 Transition 第三級</h3>\n\n<p>CSS3 Transition 還有一個 delay 的參數可以調整，第三級裡面我們將介紹如何使用 CSS3 Transition 的 delay，這樣可以讓我們的轉場效果有更多的變化。</p>\n\n<pre><code class=\"html\">&lt;div id=\"example-transition\"&gt;  \n  &lt;div class=\"ease\"&gt;ease&lt;/div&gt;\n  &lt;div class=\"linear\"&gt;linear&lt;/div&gt;\n  &lt;div class=\"easein\"&gt;ease-in&lt;/div&gt;\n  &lt;div class=\"easeout\"&gt;ease-out&lt;/div&gt;\n  &lt;div class=\"easeinout\"&gt;ease-in-out&lt;/div&gt;\n&lt;/div&gt;  \n</code></pre>\n\n<pre><code class=\"css\">#example-transition {\n  width: 520px;\n}\n#example-transition div {\n  width: 100px;\n  margin: 5px 0;\n  padding: 5px;\n  color: white;\n  background-color: #FF5050;\n  text-align: right;\n  border-radius: 5px;\n}\n#example-transition:hover div {\n  width: 500px;\n}\n#example-transition div.ease {\n  -webkit-transition: 1s ease;\n  -moz-transition: 1s ease;\n  -o-transition: 1s ease;\n  -ms-transition: 1s ease;\n  transition: 1s ease;\n}\n#example-transition div.linear {\n  -webkit-transition: 1s linear 1s;\n  -moz-transition: 1s linear 1s;\n  -o-transition: 1s linear 1s;\n  -ms-transition: 1s linear 1s;\n  transition: 1s linear 1s;\n}\n#example-transition div.easein {\n  -webkit-transition: 1s ease-in 2s;\n  -moz-transition: 1s ease-in 2s;\n  -o-transition: 1s ease-in 2s;\n  -ms-transition: 1s ease-in 2s;\n  transition: 1s ease-in 2s;\n}\n#example-transition div.easeout {\n  -webkit-transition: 1s ease-out 3s;\n  -moz-transition: 1s ease-out 3s;\n  -o-transition: 1s ease-out 3s;\n  -ms-transition: 1s ease-out 3s;\n  transition: 1s ease-out 3s;\n}\n#example-transition div.easeinout {\n  -webkit-transition: 1s ease-in-out 4s;\n  -moz-transition: 1s ease-in-out 4s;\n  -o-transition: 1s ease-in-out 4s;\n  -ms-transition: 1s ease-in-out 4s;\n  transition: 1s ease-in-out 4s;\n}\n</code></pre>\n\n<p>其實我們就只是在 timing function 後面加了一個 delay 秒數而已，就可以做到一層一層的轉場變化效果，學會 CSS3 Transition 第三級之後我們就可以隱約看出 CSS3 Transition 的語法：</p>\n\n<pre><code class=\"css\">transition: property duration timing-function delay;  \n</code></pre>\n\n<p>第一個值是指定哪個 property 要變化，可以使用 all，第二個值 duration 是說轉場效果會多久完成，第三個值 timing-function 是說要使用什麼轉場效果有 <code>ease</code>、<code>linear</code>、<code>ease-in</code>、<code>ease-out</code>、<code>ease-in-out</code> 五種，第四個值 delay 是說明要延遲多久才開始轉場效果。 </p>\n\n<h4 id=\"ahrefhttpcodepeniofukuballpenaoyxgltarget_blankcss3transitiona\"><a href=\"http://codepen.io/fukuball/pen/aOYXgL\" target=\"_blank\">CSS3 Transition 第三級展示</a></h4>\n\n<h3 id=\"css3transition\">CSS3 Transition 第四級</h3>\n\n<p>剛剛有說到 CSS3 Transition 可以指定 all property 進行轉場變化，其實並不是所有的 property 都可以進行轉場變化，可以進行轉場變化的 property 我們列成下表供大家參考。</p>\n\n<table class=\"table table-striped table-bordered\">  \n    <tbody>\n      <tr>\n        <th>Property Name</th>\n        <th>Type</th>\n      </tr>\n      <tr>\n        <td><code>background-color</code></td>\n        <td>color</td>\n      </tr>\n      <tr>\n        <td><code>background-image</code></td>\n        <td>only gradients</td>\n      </tr>\n      <tr>\n        <td><code>background-position</code></td>\n        <td>percentage, length</td>\n      </tr>\n      <tr>\n        <td><code>border-bottom-color</code></td>\n        <td>color</td>\n      </tr>\n      <tr>\n        <td><code>border-bottom-width</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>border-color</code></td>\n        <td>color</td>\n      </tr>\n      <tr>\n        <td><code>border-left-color</code></td>\n        <td>color</td>\n      </tr>\n      <tr>\n        <td><code>border-left-width</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>border-right-color</code></td>\n        <td>color</td>\n      </tr>\n      <tr>\n        <td><code>border-right-width</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>border-spacing</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>border-top-color</code></td>\n        <td>color</td>\n      </tr>\n      <tr>\n        <td><code>border-top-width</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>border-width</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>bottom</code></td>\n        <td>length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>color</code></td>\n        <td>color</td>\n      </tr>\n      <tr>\n        <td><code>crop</code></td>\n        <td>rectangle</td>\n      </tr>\n      <tr>\n        <td><code>font-size</code></td>\n        <td>length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>font-weight</code></td>\n        <td>number</td>\n      </tr>\n      <tr>\n        <td><code>grid-*</code></td>\n        <td>various</td>\n      </tr>\n      <tr>\n        <td><code>height</code></td>\n        <td>length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>left</code></td>\n        <td>length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>letter-spacing</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>line-height</code></td>\n        <td>number, length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>margin-bottom</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>margin-left</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>margin-right</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>margin-top</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>max-height</code></td>\n        <td>length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>max-width</code></td>\n        <td>length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>min-height</code></td>\n        <td>length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>min-width</code></td>\n        <td>length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>opacity</code></td>\n        <td>number</td>\n      </tr>\n      <tr>\n        <td><code>outline-color</code></td>\n        <td>color</td>\n      </tr>\n      <tr>\n        <td><code>outline-offset</code></td>\n        <td>integer</td>\n      </tr>\n      <tr>\n        <td><code>outline-width</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>padding-bottom</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>padding-left</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>padding-right</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>padding-top</code></td>\n        <td>length</td>\n      </tr>\n      <tr>\n        <td><code>right</code></td>\n        <td>length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>text-indent</code></td>\n        <td>length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>text-shadow</code></td>\n        <td>shadow</td>\n      </tr>\n      <tr>\n        <td><code>top</code></td>\n        <td>length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>vertical-align</code></td>\n        <td>keywords, length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>visibility</code></td>\n        <td>visibility</td>\n      </tr>\n      <tr>\n        <td><code>width</code></td>\n        <td>length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>word-spacing</code></td>\n        <td>length, percentage</td>\n      </tr>\n      <tr>\n        <td><code>z-index</code></td>\n        <td>integer</td>\n      </tr>\n      <tr>\n        <td><code>zoom</code></td>\n        <td>number</td>\n      </tr>\n    </tbody>\n  </table>\n\n<p>比如我們現在希望 width 跟 backgournd-color 都可以進行轉場變化，就可以這樣寫：</p>\n\n<pre><code class=\"html\">&lt;div id=\"example-transition\"&gt;  \n  &lt;div class=\"ease\"&gt;ease&lt;/div&gt;\n  &lt;div class=\"linear\"&gt;linear&lt;/div&gt;\n  &lt;div class=\"easein\"&gt;ease-in&lt;/div&gt;\n  &lt;div class=\"easeout\"&gt;ease-out&lt;/div&gt;\n  &lt;div class=\"easeinout\"&gt;ease-in-out&lt;/div&gt;\n&lt;/div&gt;  \n</code></pre>\n\n<pre><code class=\"css\">#example-transition {\n  width: 520px;\n}\n#example-transition div {\n  width: 100px;\n  margin: 5px 0;\n  padding: 5px;\n  color: white;\n  background-color: #FF5050;\n  text-align: right;\n  border-radius: 5px;\n}\n#example-transition:hover div {\n  width: 500px;\n  background-color: #000000;\n}\n#example-transition div.ease {\n  -webkit-transition: 3s ease;\n  -moz-transition: 3s ease;\n  -o-transition: 3s ease;\n  -ms-transition: 3s ease;\n  transition: 3s ease;\n}\n#example-transition div.linear {\n  -webkit-transition: 3s linear;\n  -moz-transition: 3s linear;\n  -o-transition: 3s linear;\n  -ms-transition: 3s linear;\n  transition: 3s linear;\n}\n#example-transition div.easein {\n  -webkit-transition: 3s ease-in;\n  -moz-transition: 3s ease-in;\n  -o-transition: 3s ease-in;\n  -ms-transition: 3s ease-in;\n  transition: 3s ease-in;\n}\n#example-transition div.easeout {\n  -webkit-transition: 3s ease-out;\n  -moz-transition: 3s ease-out;\n  -o-transition: 3s ease-out;\n  -ms-transition: 3s ease-out;\n  transition: 3s ease-out;\n}\n#example-transition div.easeinout {\n  -webkit-transition: 3s ease-in-out;\n  -moz-transition: 3s ease-in-out;\n  -o-transition: 3s ease-in-out;\n  -ms-transition: 3s ease-in-out;\n  transition: 3s ease-in-out;\n}\n</code></pre>\n\n<p>我們在 hover 時有兩個 property 不一樣，一個是 <code>width: 500px;</code>、一個是 <code>background-color: #000000;</code>，這兩個 property 都是屬於可以進行轉場效果的，因此這樣寫就會一起變化，很簡單吧！</p>\n\n<h4 id=\"ahrefhttpcodepeniofukuballpenvorpyqtarget_blankcss3transitiona\"><a href=\"http://codepen.io/fukuball/pen/vORPYQ\" target=\"_blank\">CSS3 Transition 第四級展示</a></h4>\n\n<h3 id=\"\">結語</h3>\n\n<p>我們已經練完了 CSS3 Transition 大法的前四級，其實只要學會這些技巧，大概就能做出不錯的特效，只是動畫如何安排才會吸引人就要看個人的 Sense 了，像我就沒有什麼 Sense～</p>\n\n<p>或許可以去請教對動畫最有 Sense 的 <a href=\"http://ricetseng.com/\" target=\"_blank\">Rice Tseng</a> 公主！</p>\n\n<p>結果跟 CSS3 Animation 那篇文章完全一模一樣！真的好混啊！哈哈哈！！！</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-07-06T06:47:25.000Z","created_by":1,"updated_at":"2015-07-22T08:27:17.000Z","updated_by":1,"published_at":"2015-07-08T08:25:40.000Z","published_by":1},{"id":60,"uuid":"c8d313f5-33bc-4628-af52-72c704ce4b95","title":"PHP Conf","slug":"php-conf","markdown":"我是 PHPConf Taiwan 2015 大會議程組的聖佑，感謝您願意撥冗參與本屆 PHPConf Taiwan 2015 並擔任議程講者，在這邊需要您提供講題相關資訊，另外也整理了關於大會幾個重要的時程表及講者注意事項供您參考。煩請於 7/10 (五) 前回覆，以利大會議程組處理後續議程相關事宜，謝謝您的配合！\n\n講題資料\n\n講者真實姓名 (大會與您聯繫時使用)\n\n林志傑\n\nEmail (大會與您聯繫時使用)\n\nfukuball@gmail.com\n\n講題標題 ( 50 字以內)\n\niNDIEVOX 踏入演唱會售票之後的快速蛻變 - How we build a scalable PHP website for ticketing\n\n講題摘要 (約 150 字介紹)\n\niNDIEVOX 是目前台灣最大的獨立音樂網路商店，主要商業模式是販售高音質數位音樂下載及演唱會售票，除了提供給獨立音樂藝人及音樂廠牌一個完整的數位銷售平台之外，也讓喜歡獨立音樂的大眾可以便利地購買到喜歡的音樂及門票。\n\n我們將在這個議題分享 iNDIEVOX 如何在踏入演唱會售票領域之後開始快速蛻變，慢慢將一個無法 Scale 的 PHP 網站系統改造成如今可以 Scale Up / Scale Out 以承受線上同時上萬人搶票的 PHP 網站系統。\n\n暱稱或是網路 ID\n\nfukuball\n\n自我介紹 (約 150 字介紹)\n\n現職擔任 iNDIEVOX 技術長，iNDIEVOX 是目前台灣最大的獨立音樂網路商店，販售高音質數位音樂下載及演唱會售票。我使用 PHP 及 Python，最近對機器學習感到興趣，所以空閒時會將 Python 有關機器學習的 Github Project 翻譯成 PHP 版本。 / 我也是一個快樂的吉他手～\n\nCTO of iNDIEVOX 獨立音樂網, the largest indie music web site in Taiwan. I use PHP and Python, interested in machine learning recently. I translate some python machine learning project in my free time. / I'm also a happy guitar player.\n\n行動電話 (大會與您聯繫時使用)\n\n0919095468\n\n目前服務單位與職稱 (若不便透露的話，可選填)\n\niNDIEVOX 技術長\n\n講題深度 (依講題難度分為新手、一般、進階)\n\n一般\n\n是否採用主辦單位提供之電腦進行簡報 (為了確保錄影品質，簡報電腦上會先安裝畫面同步錄影軟體)\n\n是\n\n是否需要協助處理住宿？ (若國外或居住在大台北地區以外的講者需要協助請填寫)\n\n否\n\n講題授權方式 (請參考下方講題授權說明)\n\n以 CC BY-NC 3.0 姓名標示-非商業性方式授權\n\n\n\n\n講者注意事項\n\n講題時間為一場 50 分鐘，內容可使用 中文 或 英文\n投稿內容包括但不限於大會主題或任何與 PHP 相關之內容\n講題深度依難度分為新手、一般、進階，供大會參考排定議程順序及場次\n主辦單位保留修改活動辦法及投稿採用與否之權利\n\n重要時程表\n\n7/10/2015：講者回覆大會講題\n8/16/2015：大會提供講者投影片範本\n9/16/2015：請講者提供投影片初稿\n10/9/2015：PHPConf Taiwan 2015\n\n講題授權方式\n\n您的演講將會由大會進行錄影，並於後製完成後，置放於公開的網路影音平台。另外，大會將於議程結束後跟您索取最終版講題投影片，並置放於大會官方網站上。因此，您必須同意大會依以下條件 (擇一) ，授權您的講題：\n授予 PHPConf Taiwan 2015 錄影後製與透過線上平台發佈之權利\n以 CC BY 3.0 姓名標示方式授權\n以 CC BY-SA 3.0 姓名標示-相同方式授權\n以 CC BY-NC 3.0 姓名標示-非商業性方式授權\n\n以上若有任何問題也請不吝與我聯絡\n謝謝！\n\nhttp://www.slideshare.net/eugenewang1/flyingv-laravel-aws\n\nhttp://www.slideshare.net/ktchiu1972/ss-33943559\n\nhttps://medium.com/@hlb/kktix-2015-01-7bf84c47dfdf\n\nhttp://blogs.technet.com/b/azuretw/archive/2015/01/14/flavorus-microsoft-azure-10-150-000.aspx\n\nhttps://speakerdeck.com/hlb/kktix-de-di-nian\n\nhttp://blow.streetvoice.com/6338-%E8%B3%A3%E7%A7%92%E6%AE%BA%E7%A5%A8%E8%B7%9F%E4%BD%A0%E6%83%B3%E5%BE%97%E4%B8%8D%E4%B8%80%E6%A8%A3-%EF%BC%8D-tix-craft-%E9%82%B1%E5%85%89%E5%AE%97%E3%80%81indievox-%E6%9E%97%E5%BF%97%E5%82%91/\n\nhttp://www.ithome.com.tw/article/94529\n\nhttps://speakerdeck.com/tzangms/the-workflow-of-the-new-streetvoice\n\nhttps://speakerdeck.com/tzangms/python\n\nhttps://www.youtube.com/watch?v=FvoLt-YZnb8\n\nhttps://www.youtube.com/watch?v=NiTb_UMLQfc\n\nhttps://tw.pycon.org/2014apac/en/speakers/60/\n\nhttp://www.slideshare.net/Vpon/vpon-coscup-2014\n\nhttp://coscup.org/2014/zh-tw/program/\n\n","html":"<p>我是 PHPConf Taiwan 2015 大會議程組的聖佑，感謝您願意撥冗參與本屆 PHPConf Taiwan 2015 並擔任議程講者，在這邊需要您提供講題相關資訊，另外也整理了關於大會幾個重要的時程表及講者注意事項供您參考。煩請於 7/10 (五) 前回覆，以利大會議程組處理後續議程相關事宜，謝謝您的配合！</p>\n\n<p>講題資料</p>\n\n<p>講者真實姓名 (大會與您聯繫時使用)</p>\n\n<p>林志傑</p>\n\n<p>Email (大會與您聯繫時使用)</p>\n\n<p>fukuball@gmail.com</p>\n\n<p>講題標題 ( 50 字以內)</p>\n\n<p>iNDIEVOX 踏入演唱會售票之後的快速蛻變 - How we build a scalable PHP website for ticketing</p>\n\n<p>講題摘要 (約 150 字介紹)</p>\n\n<p>iNDIEVOX 是目前台灣最大的獨立音樂網路商店，主要商業模式是販售高音質數位音樂下載及演唱會售票，除了提供給獨立音樂藝人及音樂廠牌一個完整的數位銷售平台之外，也讓喜歡獨立音樂的大眾可以便利地購買到喜歡的音樂及門票。</p>\n\n<p>我們將在這個議題分享 iNDIEVOX 如何在踏入演唱會售票領域之後開始快速蛻變，慢慢將一個無法 Scale 的 PHP 網站系統改造成如今可以 Scale Up / Scale Out 以承受線上同時上萬人搶票的 PHP 網站系統。</p>\n\n<p>暱稱或是網路 ID</p>\n\n<p>fukuball</p>\n\n<p>自我介紹 (約 150 字介紹)</p>\n\n<p>現職擔任 iNDIEVOX 技術長，iNDIEVOX 是目前台灣最大的獨立音樂網路商店，販售高音質數位音樂下載及演唱會售票。我使用 PHP 及 Python，最近對機器學習感到興趣，所以空閒時會將 Python 有關機器學習的 Github Project 翻譯成 PHP 版本。 / 我也是一個快樂的吉他手～</p>\n\n<p>CTO of iNDIEVOX 獨立音樂網, the largest indie music web site in Taiwan. I use PHP and Python, interested in machine learning recently. I translate some python machine learning project in my free time. / I'm also a happy guitar player.</p>\n\n<p>行動電話 (大會與您聯繫時使用)</p>\n\n<p>0919095468</p>\n\n<p>目前服務單位與職稱 (若不便透露的話，可選填)</p>\n\n<p>iNDIEVOX 技術長</p>\n\n<p>講題深度 (依講題難度分為新手、一般、進階)</p>\n\n<p>一般</p>\n\n<p>是否採用主辦單位提供之電腦進行簡報 (為了確保錄影品質，簡報電腦上會先安裝畫面同步錄影軟體)</p>\n\n<p>是</p>\n\n<p>是否需要協助處理住宿？ (若國外或居住在大台北地區以外的講者需要協助請填寫)</p>\n\n<p>否</p>\n\n<p>講題授權方式 (請參考下方講題授權說明)</p>\n\n<p>以 CC BY-NC 3.0 姓名標示-非商業性方式授權</p>\n\n<p>講者注意事項</p>\n\n<p>講題時間為一場 50 分鐘，內容可使用 中文 或 英文\n投稿內容包括但不限於大會主題或任何與 PHP 相關之內容\n講題深度依難度分為新手、一般、進階，供大會參考排定議程順序及場次\n主辦單位保留修改活動辦法及投稿採用與否之權利</p>\n\n<p>重要時程表</p>\n\n<p>7/10/2015：講者回覆大會講題 <br />\n8/16/2015：大會提供講者投影片範本 <br />\n9/16/2015：請講者提供投影片初稿 <br />\n10/9/2015：PHPConf Taiwan 2015</p>\n\n<p>講題授權方式</p>\n\n<p>您的演講將會由大會進行錄影，並於後製完成後，置放於公開的網路影音平台。另外，大會將於議程結束後跟您索取最終版講題投影片，並置放於大會官方網站上。因此，您必須同意大會依以下條件 (擇一) ，授權您的講題：\n授予 PHPConf Taiwan 2015 錄影後製與透過線上平台發佈之權利\n以 CC BY 3.0 姓名標示方式授權\n以 CC BY-SA 3.0 姓名標示-相同方式授權\n以 CC BY-NC 3.0 姓名標示-非商業性方式授權</p>\n\n<p>以上若有任何問題也請不吝與我聯絡\n謝謝！</p>\n\n<p><a href='http://www.slideshare.net/eugenewang1/flyingv-laravel-aws'>http://www.slideshare.net/eugenewang1/flyingv-laravel-aws</a></p>\n\n<p><a href='http://www.slideshare.net/ktchiu1972/ss-33943559'>http://www.slideshare.net/ktchiu1972/ss-33943559</a></p>\n\n<p><a href='https://medium.com/@hlb/kktix-2015-01-7bf84c47dfdf'>https://medium.com/@hlb/kktix-2015-01-7bf84c47dfdf</a></p>\n\n<p><a href='http://blogs.technet.com/b/azuretw/archive/2015/01/14/flavorus-microsoft-azure-10-150-000.aspx'>http://blogs.technet.com/b/azuretw/archive/2015/01/14/flavorus-microsoft-azure-10-150-000.aspx</a></p>\n\n<p><a href='https://speakerdeck.com/hlb/kktix-de-di-nian'>https://speakerdeck.com/hlb/kktix-de-di-nian</a></p>\n\n<p><a href='http://blow.streetvoice.com/6338-%E8%B3%A3%E7%A7%92%E6%AE%BA%E7%A5%A8%E8%B7%9F%E4%BD%A0%E6%83%B3%E5%BE%97%E4%B8%8D%E4%B8%80%E6%A8%A3-%EF%BC%8D-tix-craft-%E9%82%B1%E5%85%89%E5%AE%97%E3%80%81indievox-%E6%9E%97%E5%BF%97%E5%82%91/'>http://blow.streetvoice.com/6338-%E8%B3%A3%E7%A7%92%E6%AE%BA%E7%A5%A8%E8%B7%9F%E4%BD%A0%E6%83%B3%E5%BE%97%E4%B8%8D%E4%B8%80%E6%A8%A3-%EF%BC%8D-tix-craft-%E9%82%B1%E5%85%89%E5%AE%97%E3%80%81indievox-%E6%9E%97%E5%BF%97%E5%82%91/</a></p>\n\n<p><a href='http://www.ithome.com.tw/article/94529'>http://www.ithome.com.tw/article/94529</a></p>\n\n<p><a href='https://speakerdeck.com/tzangms/the-workflow-of-the-new-streetvoice'>https://speakerdeck.com/tzangms/the-workflow-of-the-new-streetvoice</a></p>\n\n<p><a href='https://speakerdeck.com/tzangms/python'>https://speakerdeck.com/tzangms/python</a></p>\n\n<p><a href='https://www.youtube.com/watch?v=FvoLt-YZnb8'>https://www.youtube.com/watch?v=FvoLt-YZnb8</a></p>\n\n<p><a href='https://www.youtube.com/watch?v=NiTb_UMLQfc'>https://www.youtube.com/watch?v=NiTb_UMLQfc</a></p>\n\n<p><a href='https://tw.pycon.org/2014apac/en/speakers/60/'>https://tw.pycon.org/2014apac/en/speakers/60/</a></p>\n\n<p><a href='http://www.slideshare.net/Vpon/vpon-coscup-2014'>http://www.slideshare.net/Vpon/vpon-coscup-2014</a></p>\n\n<p><a href='http://coscup.org/2014/zh-tw/program/'>http://coscup.org/2014/zh-tw/program/</a></p>","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-07-06T06:52:47.000Z","created_by":1,"updated_at":"2015-07-06T09:00:27.000Z","updated_by":1,"published_at":null,"published_by":null},{"id":61,"uuid":"edfc9c1f-f359-4043-83af-cc0f2f031fd0","title":"林軒田教授機器學習基石 Machine Learning Foundations 第一講學習筆記","slug":"machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji","markdown":"### 前言\n\n機器學習（Machine Learning）是一門很深的課程，要直接跳進來學習其實並不容易，因此系統性由淺而深的學習過程還是必須的。這一系列部落格文章我將分享我在 Coursera 上臺灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明，希望對有心學習 Machine Learning 的碼農們有些幫助。\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 如何有效學習機器學習\n\n從基礎來由淺入深，包含理論及實作技術用說故事的方式包裝，比如何時可以使用機器學習、為何機器可以學習、機器怎麼學習、如何讓機器學得更好，讓我們可以記得並加以應用。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-1.png?1\">\n</p>\n\n### 從人的學習轉換到機器學習\n\n人學習是為了習得一種技能，比如學習辨認男生或女生，而我們可以從觀察中累積經驗而學會辨認男生或女生，這就是人學習的過程，觀察 -> 累積經驗、學習 -> 習得技能；而機器怎麼學習呢？其實有點相似，機器為了學習一種技能，比如一樣是學習辨認男生或女生，電腦可以從**觀察資料**及**計算**累積**模型**而學會**辨認**男生或女生，這就是機器學習的過程，資料 -> 計算、學習出模型 -> 習得技能。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-2.png\">\n</p>\n\n### 再定義一下什麼是技能\n\n「師爺，翻譯翻譯什麼是他媽的技能」「技能不就是技能嗎」在機器學習上，技能就是透過計算所搜集到的資料來提升一些可量測的性能，比如預測得更準確，實例上像是我們可以搜集股票的交易資料，然後透過機器學習的計算及預測後，是否可以得到更多的投資報酬。如果可以增加預測的準確度，那麼我們就可以說電腦透過機器學習得到了預測股票買賣的技能了。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-3.png\">\n</p>\n\n### 舉個例子\n\n各位勞苦功高的碼農們，現在老闆心血來潮要你寫一個可以辨識樹的圖片的程式，你會怎麼寫呢？你可能寫一個程式檢查圖片中有沒有綠綠的或是有沒有像葉子的形狀的部份等等，然後寫了幾百條規則來完成辨識樹的圖片的功能，現在功能上線了，好死不死現在來了一張樹的圖片上面剛好都沒有葉子，你寫的幾百條規則都沒用了，辨識樹的圖片的功能只能以失敗收場。機器學習可以解決這樣的問題，透過觀察資料的方式來讓電腦自己辨識樹的圖片，可能會比寫幾百條判斷規則更有效。這有點像是教電腦學會釣魚（透過觀察資料學習），而不是直接給電腦魚吃（直接寫規則給電腦）。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-4.png\">\n</p>\n\n### 那麼什麼時候可以使用機器學習呢\n\n從上個例子我們可以大概了解使用機器學習的使用時機，大致上如果觀察到現在你想要解決的問題有以下三個現象，應該就是機器學習上場的時刻了：\n\n1. 存在某種潛在規則\n2. 但沒有很辦法很簡單地用程式直接定義來作邏輯判斷（if else 就可以做到，就不用機器學習）\n3. 這些潛在規則有很多資料可以作為觀察、學習的來源\n    \n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-5.png\">\n</p>\n\n### 舉個實際的機器學習例子 1\n\nNetflix 現在出了一個問題，如果你能讓使用者對電影喜好程度星級預測準確率提升 10%，那就可以獲得 100 萬美金，馬上讓你從碼農無產階級晉升到天龍人資產階級，而這個問題是這樣的：他們給了你大量使用者對一些電影的星級評分資料，你必須要讓電腦學到一個技能，這個技能可以預測到使用者對他還沒看過的電影評分會是多少星級，如果電腦能準確預測的話，那某種程度它就有了可以知道使用者會不會喜歡這些電影的技能，進而可以推薦使用者他們會喜歡的電影，讓他們從口袋裡拿錢過來～\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-6.png\">\n</p>\n\n### 舉個實際的機器學習例子 2\n\n這邊偷偷告訴大家一個很常見的機器學習方法的模型，我們再來整理一下，其實這個問題可以轉化成這樣，使用者有很多個會喜歡這部電影的因素，比如電影中有沒有爆破場景、有沒有養眼畫面、有沒有外星人等等，這個我們就稱之為使用者的特徵值（feature），而電影本身也有很多因素，比如電影中有出現炸彈、是很有魅力的史嘉蕾·喬韓森所主演、片名是 ET 第二集等等，這個我們就稱之為電影的特徵值，我們把這兩個特徵值表示成向量（vector），如此如果使用者與電影特徵值有對應的特徵越多，那就代表使用者很有可能喜歡這部電影，而這可以很快地用向量內積的方式計算出來。也就是說，機器學習在這個問題上，只要能學習出這些會影響使用者喜好的**因素**也就是機器學習所說的**特徵值**會是什麼，那這樣當一部新的電影出來，我們只要叫電腦對一下這部新電影與使用者的特徵值的對應起來的向量內積值高不高就可以知道使用者會不會喜歡這部電影了～\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-7.png\">\n</p>\n\n### 將剛剛的問題用數學式來描述\n\n我們在用銀行核發信用卡的例子來描述機器學習，我們可以把信用卡申請者的資料想成是 x，而 y 是銀行是否核發信用卡。所以這就是一個函式，它有一個潛在規則，可以讓 x 對應到 y，機器學習就是要算出這個 f 函式是什麼。現在我們有大量的信用卡對申請者核發信用卡的資料，就是 D，我們可以從資料觀察中得到一些假設，然後讓電腦去學習這些假設是對的還是錯的，慢慢習得技能，最後電腦可能會算出一個 g 函式，雖然不是完全跟 f 一樣，但跟 f 很像，所以能夠做出還蠻精確的預測。\n    \n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-8.png\">\n</p>\n\n### 機器學習流程\n\n所以機器學習銀行是否核發信用卡的流程就像這樣，我們想要找到 target function，可以完整預測銀行對申請者是否要核發信用卡才會賺錢，這時我們會餵給電腦大量的資料，然後透過學習演算法找出重要的特徵值，這些重要的特徵值就可以組成函式 g，雖然跟 f 不一樣，但很接近。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-9.png\">\n</p>\n\n### 機器學習的 Model\n\n從上面的學習流程，我們可以知道最後電腦會學習出 g 可以辨認資料中較重要的特徵值，這些特徵值可能是我們一開始觀察資料所整理出來的假設，所以我們餵資料給電腦做學習演算法做計算時，也會餵一些假設進去，以銀行核發信用卡的例子就是這個申請者年薪是否有 80 萬、負債是否有 10 萬、工作是否小於兩年等等假設，這些假設就是 H，學習演算法再去計算實際資料與假設是否吻合，這個演算法就是 A，最後演算法會挑出最好的假設集合是哪些。 H 與 A 我們就稱為是機器學習 Model\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-10.png\">\n</p>\n\n### 機器學習的基本定義\n\n機器學習的基本定義可以用這個圖來概括，用一句話來說的話就是「use data to compute hypothesis g that approximates target f」，你如果問我為何要用英文寫下這句話，其實只是因為這樣看起來比較像是一個偉人大學者寫的啊！拿來唬人用的！\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-11.png\">\n</p>","html":"<h3 id=\"\">前言</h3>\n\n<p>機器學習（Machine Learning）是一門很深的課程，要直接跳進來學習其實並不容易，因此系統性由淺而深的學習過程還是必須的。這一系列部落格文章我將分享我在 Coursera 上臺灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明，希望對有心學習 Machine Learning 的碼農們有些幫助。</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">如何有效學習機器學習</h3>\n\n<p>從基礎來由淺入深，包含理論及實作技術用說故事的方式包裝，比如何時可以使用機器學習、為何機器可以學習、機器怎麼學習、如何讓機器學得更好，讓我們可以記得並加以應用。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-1.png?1\">\n</p>\n\n<h3 id=\"\">從人的學習轉換到機器學習</h3>\n\n<p>人學習是為了習得一種技能，比如學習辨認男生或女生，而我們可以從觀察中累積經驗而學會辨認男生或女生，這就是人學習的過程，觀察 -> 累積經驗、學習 -> 習得技能；而機器怎麼學習呢？其實有點相似，機器為了學習一種技能，比如一樣是學習辨認男生或女生，電腦可以從<strong>觀察資料</strong>及<strong>計算</strong>累積<strong>模型</strong>而學會<strong>辨認</strong>男生或女生，這就是機器學習的過程，資料 -> 計算、學習出模型 -> 習得技能。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-2.png\">\n</p>\n\n<h3 id=\"\">再定義一下什麼是技能</h3>\n\n<p>「師爺，翻譯翻譯什麼是他媽的技能」「技能不就是技能嗎」在機器學習上，技能就是透過計算所搜集到的資料來提升一些可量測的性能，比如預測得更準確，實例上像是我們可以搜集股票的交易資料，然後透過機器學習的計算及預測後，是否可以得到更多的投資報酬。如果可以增加預測的準確度，那麼我們就可以說電腦透過機器學習得到了預測股票買賣的技能了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-3.png\">\n</p>\n\n<h3 id=\"\">舉個例子</h3>\n\n<p>各位勞苦功高的碼農們，現在老闆心血來潮要你寫一個可以辨識樹的圖片的程式，你會怎麼寫呢？你可能寫一個程式檢查圖片中有沒有綠綠的或是有沒有像葉子的形狀的部份等等，然後寫了幾百條規則來完成辨識樹的圖片的功能，現在功能上線了，好死不死現在來了一張樹的圖片上面剛好都沒有葉子，你寫的幾百條規則都沒用了，辨識樹的圖片的功能只能以失敗收場。機器學習可以解決這樣的問題，透過觀察資料的方式來讓電腦自己辨識樹的圖片，可能會比寫幾百條判斷規則更有效。這有點像是教電腦學會釣魚（透過觀察資料學習），而不是直接給電腦魚吃（直接寫規則給電腦）。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-4.png\">\n</p>\n\n<h3 id=\"\">那麼什麼時候可以使用機器學習呢</h3>\n\n<p>從上個例子我們可以大概了解使用機器學習的使用時機，大致上如果觀察到現在你想要解決的問題有以下三個現象，應該就是機器學習上場的時刻了：</p>\n\n<ol>\n<li>存在某種潛在規則  </li>\n<li>但沒有很辦法很簡單地用程式直接定義來作邏輯判斷（if else 就可以做到，就不用機器學習）  </li>\n<li>這些潛在規則有很多資料可以作為觀察、學習的來源</li>\n</ol>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-5.png\">\n</p>\n\n<h3 id=\"1\">舉個實際的機器學習例子 1</h3>\n\n<p>Netflix 現在出了一個問題，如果你能讓使用者對電影喜好程度星級預測準確率提升 10%，那就可以獲得 100 萬美金，馬上讓你從碼農無產階級晉升到天龍人資產階級，而這個問題是這樣的：他們給了你大量使用者對一些電影的星級評分資料，你必須要讓電腦學到一個技能，這個技能可以預測到使用者對他還沒看過的電影評分會是多少星級，如果電腦能準確預測的話，那某種程度它就有了可以知道使用者會不會喜歡這些電影的技能，進而可以推薦使用者他們會喜歡的電影，讓他們從口袋裡拿錢過來～</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-6.png\">\n</p>\n\n<h3 id=\"2\">舉個實際的機器學習例子 2</h3>\n\n<p>這邊偷偷告訴大家一個很常見的機器學習方法的模型，我們再來整理一下，其實這個問題可以轉化成這樣，使用者有很多個會喜歡這部電影的因素，比如電影中有沒有爆破場景、有沒有養眼畫面、有沒有外星人等等，這個我們就稱之為使用者的特徵值（feature），而電影本身也有很多因素，比如電影中有出現炸彈、是很有魅力的史嘉蕾·喬韓森所主演、片名是 ET 第二集等等，這個我們就稱之為電影的特徵值，我們把這兩個特徵值表示成向量（vector），如此如果使用者與電影特徵值有對應的特徵越多，那就代表使用者很有可能喜歡這部電影，而這可以很快地用向量內積的方式計算出來。也就是說，機器學習在這個問題上，只要能學習出這些會影響使用者喜好的<strong>因素</strong>也就是機器學習所說的<strong>特徵值</strong>會是什麼，那這樣當一部新的電影出來，我們只要叫電腦對一下這部新電影與使用者的特徵值的對應起來的向量內積值高不高就可以知道使用者會不會喜歡這部電影了～</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-7.png\">\n</p>\n\n<h3 id=\"\">將剛剛的問題用數學式來描述</h3>\n\n<p>我們在用銀行核發信用卡的例子來描述機器學習，我們可以把信用卡申請者的資料想成是 x，而 y 是銀行是否核發信用卡。所以這就是一個函式，它有一個潛在規則，可以讓 x 對應到 y，機器學習就是要算出這個 f 函式是什麼。現在我們有大量的信用卡對申請者核發信用卡的資料，就是 D，我們可以從資料觀察中得到一些假設，然後讓電腦去學習這些假設是對的還是錯的，慢慢習得技能，最後電腦可能會算出一個 g 函式，雖然不是完全跟 f 一樣，但跟 f 很像，所以能夠做出還蠻精確的預測。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-8.png\">\n</p>\n\n<h3 id=\"\">機器學習流程</h3>\n\n<p>所以機器學習銀行是否核發信用卡的流程就像這樣，我們想要找到 target function，可以完整預測銀行對申請者是否要核發信用卡才會賺錢，這時我們會餵給電腦大量的資料，然後透過學習演算法找出重要的特徵值，這些重要的特徵值就可以組成函式 g，雖然跟 f 不一樣，但很接近。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-9.png\">\n</p>\n\n<h3 id=\"model\">機器學習的 Model</h3>\n\n<p>從上面的學習流程，我們可以知道最後電腦會學習出 g 可以辨認資料中較重要的特徵值，這些特徵值可能是我們一開始觀察資料所整理出來的假設，所以我們餵資料給電腦做學習演算法做計算時，也會餵一些假設進去，以銀行核發信用卡的例子就是這個申請者年薪是否有 80 萬、負債是否有 10 萬、工作是否小於兩年等等假設，這些假設就是 H，學習演算法再去計算實際資料與假設是否吻合，這個演算法就是 A，最後演算法會挑出最好的假設集合是哪些。 H 與 A 我們就稱為是機器學習 Model</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-10.png\">\n</p>\n\n<h3 id=\"\">機器學習的基本定義</h3>\n\n<p>機器學習的基本定義可以用這個圖來概括，用一句話來說的話就是「use data to compute hypothesis g that approximates target f」，你如果問我為何要用英文寫下這句話，其實只是因為這樣看起來比較像是一個偉人大學者寫的啊！拿來唬人用的！</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji-11.png\">\n</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-08-17T07:17:34.000Z","created_by":1,"updated_at":"2016-08-09T05:29:23.000Z","updated_by":1,"published_at":"2015-08-17T13:04:46.000Z","published_by":1},{"id":62,"uuid":"b680ec91-c393-4f59-94be-48a7ae822e5f","title":"林軒田教授機器學習基石 Machine Learning Foundations 第二講學習筆記","slug":"lin-xuan-tian-jiao-shou-machine-learning-foundations-di-er-jiang-xue-xi-bi-ji","markdown":"### 前言\n\n本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 [第一講](http://blog.fukuball.com/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji/) 的碼農們，我建議可以先回頭去讀一下再回來喔！\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 熱身回顧一下\n\n在前一章我們基本上可以了解機器學習的架構大致上就是 *A takes D and H to get g*，也就是說我們會使用演算法來基於資料與假設集合計算出一個符合資料呈現結果的方程式 g，在這邊我們就會看到 H 會長什麼樣子，然後介紹 Perceptron Learning Algorithm（PLA）來讓機器學習如何回答是非題，比如讓機器回答銀行是否要發信用卡給申請人這樣的問題。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-0.png\">\n</p>\n\n### 再看一次是否要發信用卡這個問題\n\n是否要發信用卡這個問題我們可以想成它是一個方程式 f，而申請者的資料集合 X 丟進去就可以得到 Y 這些是否核發信用卡的記錄，我們現在不知道 f，將歷史資料 D 拿來當成訓練資料，其中每個 xi 就是申請者的資料，它會一個多維相向，比如第一個維度是年齡，第二個維度是性別...等等，然後我們會將這些資料 D 及假設集合 H 丟到機器學習演算法 Ａ，最後算出一個最像 f 的 g，這個 g 其實就是從假設集合 H 挑出一個最好的假設的結果。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-1.png\">\n</p>\n\n### 簡單的假設集合：感知器\n\n要回答是否核發信用卡，可以用這樣簡單的想法來實現，現在我們知道申請者有很多基本資料，這些資料可以關係到是否核發信用卡，學術上就稱為是「特徵值」，這些特徵值有的重要、有的不重要，我們可以為這些特徵值依照重要性配上一個權重分數 wi，所以當這些分數加總起來之後，如果超過一個界線 threshold 時，我們就可以就可以決定核發信用卡，否則就不核發。這些 wi 及 threshold 就是所謂的假設集合，可以表示成如投影片中的線性方程式。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-2.png\">\n</p>\n\n### 將假設集合的線性方程式整理一下\n\nThreshold 我們也可以視為是一種加權分數，所以就可以將假設集合的線性方程式整理得更簡潔，整個線性方程式就變成了很簡易的兩個向量內積而已。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-3.png\">\n</p>\n\n### 用二維空間來看看這個例子\n\n假如現在申請者的只有兩個特徵值，那就可以用一個二維空間來標出每個申請者的位置，而是否核發信用卡，則用藍色圈圈來代表核發，紅色叉叉代表不核發，而假設 h 就是在這空間的一條線的法向量，可以將藍色圈圈跟紅色叉叉完美的分開來，這在機器學習上就是所謂的「分類」，Perceptrons 就是一種線性分類器。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-4.png\">\n</p>\n\n### 怎麼從所有的假設中得到最好的假設 g\n\n我們希望 g 可以跟 f 一樣完美的分類信用卡的核發與否，只要從 H 這個假設集合中挑到可以完美分類信用卡核發與否的線，我們就可以得到 g 了，但這很難，因為平面中可以有無限多條線，這樣算不完。所以我們改變想法，我們先隨便切一條線，然後如果有錯的地方，就修正這條線。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-5.png\">\n</p>\n\n### Perceptron Learning Algorithm 感知學習模型\n\n剛剛這樣有錯就去修正的想法，就是感知學習模型（Perceptron Learning Algorithm）的核心思想，實際上我們怎麼修正呢？我們來仔細看一下。假設現在有一個點 x 分錯了，它實際是核發的點，但卻被分在不核發的那一邊，這就代表 wt 向量與 x 之間的夾角太大，那就要讓它們之間的夾角變小，我們可以很簡單的用向量相加的方式來做到。如果現在 x 分錯了，它實際是不核發，那就代表 wt 向量與 x 向量之間的夾角太小，那就要讓他們之間的夾角變大，我們可以用 wt 向量減掉 x 向量來做到。這樣的計算很容易可以用程式做到。PLA 也是一個最簡易的神經網路算法。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-6.png\">\n</p>\n\n### 看看 PLA 演算法修正 h 的過程\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-8.png\">\n</p>\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-9.png\">\n</p>\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-10.png\">\n</p>\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-11.png\">\n</p>\n\n### PLA 的一些問題\n\nPLA 這個演算法會一直修正 h 直到對所有的 D 都沒有錯誤時，就會停止。但真實世界的資料不會這麼完美，PLA 可能會有不會停止的情況發生，所以我們只能修正 PLA，只算出夠好的 h 就可以了。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-12.png\">\n</p>\n\n### 什麼時候 PLA 不會停止\n\n什麼時候 PLA 會停止，什麼時候不會停止？當資料集合 D 為線性可分的時候，PLA 就會停止，但如果不是線性可分的時候就不會停止。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-13.png\">\n</p>\n\n### 處理雜訊\n\n其實真實世界的資料就是這樣，充滿了雜訊，這些雜訊也有可能本身就是錯誤的資料，比如銀行一開始核發就是錯的，這也就是為何我們只要得到一個接近 f 的 g 就可以了，而不一定要得到完美的 f。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-14.png\">\n</p>\n\n### 找出犯最少錯的線\n\n既然真實世界的資料有雜訊，那我們就用程式找出犯最少錯的線吧！說起來簡單，做起來很難，這個問題其實是個 NP-hard 的問題。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-15.png\">\n</p>\n\n### Pocket Algorithm\n\n所以折衷的方式就是找到夠好的線就好，我們修改一下 PLA，讓他每次計算時，如果得到更好的線，就先暫時存下來，然後算個幾百輪，我們就可以假設目前得到的線就是一個不錯的 h 了。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-16.png\">\n</p>\n\n### 演算法原始碼\n\n以上就是第二講的內容，這邊我找到了有人實作這兩個演算法的[原始碼](http://wizmann.tk/ml-foundations-pla.html)，讓大家可以參考一下。\n\n#### Naive PLA\n\n```python\nfrom numpy import *\n\ndef naive_pla(datas):\n    w = datas[0][0]\n    iteration = 0\n    while True:\n        iteration += 1\n        false_data = 0\n\n        for data in datas:\n            t = dot(w, data[0])\n            if sign(data[1]) != sign(t):\n                error = data[1]  \n                false_data += 1\n                w += error * data[0]\n        print 'iter%d (%d / %d)' % (iteration, false_data, len(datas))\n        if not false_data:\n            break\n    return w\n```\n\n#### Pocket PLA\n\n```python\nimport numpy as np\n\ndef pocket_pla(datas, limit):\n    ###############\n    def _calc_false(vec):\n        res = 0\n        for data in datas:\n            t = np.dot(vec, data[0])\n            if np.sign(data[1]) != np.sign(t):\n                res += 1\n        return res\n    ###############\n    w = np.random.rand(5)\n    least_false = _calc_false(w)\n    res = w\n\n    for i in xrange(limit):\n        data = random.choice(datas)\n        t = np.dot(w, data[0])\n        if np.sign(data[1]) != np.sign(t):\n            t = w + data[1] * data[0]\n            t_false = _calc_false(t)\n\n            w = t\n\n            if t_false <= least_false:\n                least_false = t_false\n                res = t\n    return res, least_false\n```","html":"<h3 id=\"\">前言</h3>\n\n<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href=\"http://blog.fukuball.com/machine-learning-foundations-by-lin-xuan-tian-di-jiang-xue-xi-bi-ji/\">第一講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">熱身回顧一下</h3>\n\n<p>在前一章我們基本上可以了解機器學習的架構大致上就是 <em>A takes D and H to get g</em>，也就是說我們會使用演算法來基於資料與假設集合計算出一個符合資料呈現結果的方程式 g，在這邊我們就會看到 H 會長什麼樣子，然後介紹 Perceptron Learning Algorithm（PLA）來讓機器學習如何回答是非題，比如讓機器回答銀行是否要發信用卡給申請人這樣的問題。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-0.png\">\n</p>\n\n<h3 id=\"\">再看一次是否要發信用卡這個問題</h3>\n\n<p>是否要發信用卡這個問題我們可以想成它是一個方程式 f，而申請者的資料集合 X 丟進去就可以得到 Y 這些是否核發信用卡的記錄，我們現在不知道 f，將歷史資料 D 拿來當成訓練資料，其中每個 xi 就是申請者的資料，它會一個多維相向，比如第一個維度是年齡，第二個維度是性別...等等，然後我們會將這些資料 D 及假設集合 H 丟到機器學習演算法 Ａ，最後算出一個最像 f 的 g，這個 g 其實就是從假設集合 H 挑出一個最好的假設的結果。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-1.png\">\n</p>\n\n<h3 id=\"\">簡單的假設集合：感知器</h3>\n\n<p>要回答是否核發信用卡，可以用這樣簡單的想法來實現，現在我們知道申請者有很多基本資料，這些資料可以關係到是否核發信用卡，學術上就稱為是「特徵值」，這些特徵值有的重要、有的不重要，我們可以為這些特徵值依照重要性配上一個權重分數 wi，所以當這些分數加總起來之後，如果超過一個界線 threshold 時，我們就可以就可以決定核發信用卡，否則就不核發。這些 wi 及 threshold 就是所謂的假設集合，可以表示成如投影片中的線性方程式。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-2.png\">\n</p>\n\n<h3 id=\"\">將假設集合的線性方程式整理一下</h3>\n\n<p>Threshold 我們也可以視為是一種加權分數，所以就可以將假設集合的線性方程式整理得更簡潔，整個線性方程式就變成了很簡易的兩個向量內積而已。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-3.png\">\n</p>\n\n<h3 id=\"\">用二維空間來看看這個例子</h3>\n\n<p>假如現在申請者的只有兩個特徵值，那就可以用一個二維空間來標出每個申請者的位置，而是否核發信用卡，則用藍色圈圈來代表核發，紅色叉叉代表不核發，而假設 h 就是在這空間的一條線的法向量，可以將藍色圈圈跟紅色叉叉完美的分開來，這在機器學習上就是所謂的「分類」，Perceptrons 就是一種線性分類器。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-4.png\">\n</p>\n\n<h3 id=\"g\">怎麼從所有的假設中得到最好的假設 g</h3>\n\n<p>我們希望 g 可以跟 f 一樣完美的分類信用卡的核發與否，只要從 H 這個假設集合中挑到可以完美分類信用卡核發與否的線，我們就可以得到 g 了，但這很難，因為平面中可以有無限多條線，這樣算不完。所以我們改變想法，我們先隨便切一條線，然後如果有錯的地方，就修正這條線。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-5.png\">\n</p>\n\n<h3 id=\"perceptronlearningalgorithm\">Perceptron Learning Algorithm 感知學習模型</h3>\n\n<p>剛剛這樣有錯就去修正的想法，就是感知學習模型（Perceptron Learning Algorithm）的核心思想，實際上我們怎麼修正呢？我們來仔細看一下。假設現在有一個點 x 分錯了，它實際是核發的點，但卻被分在不核發的那一邊，這就代表 wt 向量與 x 之間的夾角太大，那就要讓它們之間的夾角變小，我們可以很簡單的用向量相加的方式來做到。如果現在 x 分錯了，它實際是不核發，那就代表 wt 向量與 x 向量之間的夾角太小，那就要讓他們之間的夾角變大，我們可以用 wt 向量減掉 x 向量來做到。這樣的計算很容易可以用程式做到。PLA 也是一個最簡易的神經網路算法。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-6.png\">\n</p>\n\n<h3 id=\"plah\">看看 PLA 演算法修正 h 的過程</h3>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-8.png\">\n</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-9.png\">\n</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-10.png\">\n</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-11.png\">\n</p>\n\n<h3 id=\"pla\">PLA 的一些問題</h3>\n\n<p>PLA 這個演算法會一直修正 h 直到對所有的 D 都沒有錯誤時，就會停止。但真實世界的資料不會這麼完美，PLA 可能會有不會停止的情況發生，所以我們只能修正 PLA，只算出夠好的 h 就可以了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-12.png\">\n</p>\n\n<h3 id=\"pla\">什麼時候 PLA 不會停止</h3>\n\n<p>什麼時候 PLA 會停止，什麼時候不會停止？當資料集合 D 為線性可分的時候，PLA 就會停止，但如果不是線性可分的時候就不會停止。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-13.png\">\n</p>\n\n<h3 id=\"\">處理雜訊</h3>\n\n<p>其實真實世界的資料就是這樣，充滿了雜訊，這些雜訊也有可能本身就是錯誤的資料，比如銀行一開始核發就是錯的，這也就是為何我們只要得到一個接近 f 的 g 就可以了，而不一定要得到完美的 f。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-14.png\">\n</p>\n\n<h3 id=\"\">找出犯最少錯的線</h3>\n\n<p>既然真實世界的資料有雜訊，那我們就用程式找出犯最少錯的線吧！說起來簡單，做起來很難，這個問題其實是個 NP-hard 的問題。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-15.png\">\n</p>\n\n<h3 id=\"pocketalgorithm\">Pocket Algorithm</h3>\n\n<p>所以折衷的方式就是找到夠好的線就好，我們修改一下 PLA，讓他每次計算時，如果得到更好的線，就先暫時存下來，然後算個幾百輪，我們就可以假設目前得到的線就是一個不錯的 h 了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-2-16.png\">\n</p>\n\n<h3 id=\"\">演算法原始碼</h3>\n\n<p>以上就是第二講的內容，這邊我找到了有人實作這兩個演算法的<a href=\"http://wizmann.tk/ml-foundations-pla.html\">原始碼</a>，讓大家可以參考一下。</p>\n\n<h4 id=\"naivepla\">Naive PLA</h4>\n\n<pre><code class=\"python\">from numpy import *\n\ndef naive_pla(datas):  \n    w = datas[0][0]\n    iteration = 0\n    while True:\n        iteration += 1\n        false_data = 0\n\n        for data in datas:\n            t = dot(w, data[0])\n            if sign(data[1]) != sign(t):\n                error = data[1]  \n                false_data += 1\n                w += error * data[0]\n        print 'iter%d (%d / %d)' % (iteration, false_data, len(datas))\n        if not false_data:\n            break\n    return w\n</code></pre>\n\n<h4 id=\"pocketpla\">Pocket PLA</h4>\n\n<pre><code class=\"python\">import numpy as np\n\ndef pocket_pla(datas, limit):  \n    ###############\n    def _calc_false(vec):\n        res = 0\n        for data in datas:\n            t = np.dot(vec, data[0])\n            if np.sign(data[1]) != np.sign(t):\n                res += 1\n        return res\n    ###############\n    w = np.random.rand(5)\n    least_false = _calc_false(w)\n    res = w\n\n    for i in xrange(limit):\n        data = random.choice(datas)\n        t = np.dot(w, data[0])\n        if np.sign(data[1]) != np.sign(t):\n            t = w + data[1] * data[0]\n            t_false = _calc_false(t)\n\n            w = t\n\n            if t_false &lt;= least_false:\n                least_false = t_false\n                res = t\n    return res, least_false\n</code></pre>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-08-27T19:04:48.000Z","created_by":1,"updated_at":"2016-08-09T05:30:30.000Z","updated_by":1,"published_at":"2015-08-28T09:05:06.000Z","published_by":1},{"id":63,"uuid":"73c240b4-75c4-4acb-b548-79a0cb41ea8b","title":"林軒田教授機器學習基石 MACHINE LEARNING FOUNDATIONS 第三講學習筆記","slug":"lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-san-jiang-xue-xi-bi-ji","markdown":"### 前言\n\n本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 [第二講](http://blog.fukuball.com/lin-xuan-tian-jiao-shou-machine-learning-foundations-di-er-jiang-xue-xi-bi-ji/) 的碼農們，我建議可以先回頭去讀一下再回來喔！\n\n第三講的內容偏向介紹各種機器學習方法，以前念論文的時候看到這些名詞都會覺得高深莫測，但其實這各式各樣的機器學習方法其實都是從最基礎的核心變化而來，所以不要被嚇到。了解各種機器學習方法的輸入輸出對於日後面對一些問題的時候，我們才能夠知道要挑選什麼機器學習方法來解決問題。\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 熱身回顧一下\n\n我們先來回顧一下上一講的內容，在上一講我們知道了如何使用 PLA 讓機器學會回答是非題這樣的兩類問題（Binary Classificaction），套到機器學習的那句名句，我們可以清楚的了解，PLA 這個演算法 A 觀察了線性可分（linear separable）的 D 及感知假設集合 H 去得到一個最好的假設 g，這一句話就可以概括到上一講的內容了。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-0.png\">\n</p>\n\n### 從輸出 y 的角度看機器學習，y 只有兩個答案選一個，就叫 Binary Classification\n\n接下來我們來了解一下各式各樣的學習方法，從輸出 y 的角度看機器學習，y 只有兩個答案選一個，就叫 Binary Classification，像是之前的是否發信用卡的例子就是 Binary Classification。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-1.png\">\n</p>\n\n### 從輸出 y 的角度看機器學習，y 有多個答案選一個，就叫 Multiclass Classification\n\n從輸出 y 的角度看機器學習，y 有多個答案選一個，就叫 Multiclass Classification，像是使用投飲機辨識錢幣的問題就是一個 Multiclass Classification 的問題，所以我們可以將分類問題推廣到分成 K 類，這樣 Binary Classificatin 就是一個 K=2 的分類問題。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-2.png\">\n</p>\n\n### 從輸出 y 的角度看機器學習，y 為一個實數，就叫 Regression\n\n從輸出 y 的角度看機器學習，y 為一個實數，就叫 Regression，像是要預估病人再過幾天病會好，這就需要用到 Regression，這也會用到許多統計學相關的工具。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-3.png\">\n</p>\n\n### 從輸出 y 的角度看機器學習，y 為一個結構序列，就叫 Structured Learning\n\n從輸出 y 的角度看機器學習，y 為一個結構序列，就叫 Structured Learning，比如一個句子的詞性分析，會需要考慮到句子中的前後文，而句子的組合可能有無限多種，因此不能單純用 Multiclass Classification 來做到，這就需要用到 Structured Learning 相關的機器學習方法。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-4.png\">\n</p>\n\n### 從輸出 y 的角度看機器學習，做個小結\n\n從輸出 y 的角度看機器學習，如果 y 是兩類，那就是 Binary Classification；如果 y 是 k 類，那就是 Multiclass Classification；如果 y 是一個實數，那就是 Regression；如果 y 是一種結構關係，那就是 Structured Learning。當然還有其他變化，不過基礎上就是 Binary Classification 及 Regression，我們可以透過這兩個基礎核心來延伸出其他機器學習方法。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-5.png\">\n</p>\n\n### 從輸入的資料 Yn 的角度看機器學習，如果每個 Xn 都有明確對應的 Yn，這就叫監督式學習（Supervised Learning）\n\n從輸入的資料 Yn 的角度看機器學習，如果每個 Xn 都有明確對應的 Yn，這就叫監督式學習（Supervised Learning），比如在訓練投飲機辨識錢幣的時候，我們很完整個告訴他什麼大小、什麼重量就是什麼幣值的錢幣，這樣就是一種監督式學習方法。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-6.png\">\n</p>\n\n### 從輸入的資料 Yn 的角度看機器學習，如果每個 Xn 都沒有標示 Yn，這就叫非監督式學習（Unsupervised Learning）\n\n從輸入的資料 Yn 的角度看機器學習，如果每個 Xn 都沒有標示 Yn，這就叫非監督式學習（Unsupervised Learning）。比如在訓練投飲機辨識錢幣的時候，我們只告訴投飲機錢幣的大小及重量，但不告訴他什麼大小及重量個錢幣是哪個幣值的錢幣，讓機器自己去觀察特徵將這些錢幣分成一群一群，這又叫做分群，這就是一種非監督式學習方法。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-7.png\">\n</p>\n\n### 從輸入的資料 Yn 的角度看機器學習，如果 Xn 只有部分有標示 Yn，這就叫半監督式學習（Semi-supervised Learning）\n\n從輸入的資料 Yn 的角度看機器學習，如果 Xn 只有部分有標示 Yn，這就叫半監督式學習（Semi-supervised Learning），有些資料較難取得的狀況下，我們會使用到半監度式學習，比如在預測藥物是否對病人有效時，由於做人體實驗成本高且可能要等一段時間來看藥效，這樣的情況下標示藥物有效或沒效的成本很高，所以就可能需要用到半監度式學習。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-8.png\">\n</p>\n\n### 從輸入的資料 Yn 的角度看機器學習，如果 Yn 是很難確知描述的，只能在機器作出反應時使用處罰及獎勵的方式讓機器知道對或錯，這就叫增強式學習（Reinforcement Learning）\n\n從輸入的資料 Yn 的角度看機器學習，如果 Yn 是很難確知描述的，只能在機器作出反應時使用處罰及獎勵的方式讓機器知道對或錯，這就叫增強式學習（Reinforcement Learning）。這樣的機器學習方式，比較像自然界生物的學習方式，就像你要教一隻狗坐下，你很難直接告訴他怎麼做，而是用獎勵或處罰的方式讓狗狗漸漸知道坐下是什麼。增強式學習也就是這樣的機器學習方法，透過一次一次經驗的累積讓機器能夠學習到一個技能。比如像是教機器學習下棋，我們也可以透過勝負讓機器漸漸學習到如何下棋會下得更好。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-9.png\">\n</p>\n\n### 從輸入的資料 Yn 的角度看機器學習，做個小結\n\n從輸入的資料 Yn 的角度看機器學習，如果明確告知每個 Yn，那就是監督式學習；如果沒有告知任何 Yn，那就是非監督式學習；如果只有部份 Yn 的資料，那就是半監督式學習；如果是用獎勵、處罰的方式來告知 Yn，那就是增強式學習。當然還有其他種機器學習方法，其中最重要的核心就是監督式學習方法。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-10.png\">\n</p>\n\n### 從餵資料給機器的角度看機器學習，一次餵進全部資料，這就叫 Batch Learning\n\n從餵資料給機器的角度看機器學習，一次餵進全部資料，這就叫 Batch Learning。監督式學習方法，可能也會常使用 Batch Learning 的方式為資料。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-11.png\">\n</p>\n\n### 從餵資料給機器的角度看機器學習，可以再慢慢餵進新資料，這就叫 Online Learning\n\n從餵資料給機器的角度看機器學習，可以再慢慢餵進新資料，這就叫 Online Learning。Batch Learging 訓練好的機器，就無法調整他的技巧，可能會有越來越不準的情況，所以 Online Learning 可以再慢慢調整、增進技巧。PLA 算法可以很容易應用在 Online Learning 上，增強式學習方法也常常是使用 Online Learning 的方式餵資料。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-12.png\">\n</p>\n\n### 從餵資料給機器的角度看機器學習，機器可以問問題，然後從問題的答案再餵進資料，這就叫 Active Learning\n\n從餵資料給機器的角度看機器學習，機器可以問問題，然後從問題的答案再餵進資料，這就叫 Active Learning。這樣的學習方法是要希望讓機器可以用一些策略問問題，然後慢慢學習、改善技巧。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-13.png\">\n</p>\n\n### 從餵資料給機器的角度看機器學習，做個小結\n\n從餵資料給機器的角度看機器學習，如果一次餵進所有資料，就叫 Batch Learning；如果後續可以再慢慢餵進資料，就叫 Online Learning；如果機器可以問問題來餵進資料，就叫 Active Learning。當然還有其他種機器學習方法，其中最重要的核心就是 Batch Learning。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-14.png\">\n</p>\n\n### 從輸入 X 的角度看機器學習，如果 X 的特徵很明確定義，這就叫 Concrete Feature\n\n從輸入 X 的角度看機器學習，如果 X 的特徵很明確定義，這就叫 Concrete Feature。Concrete Featrue 的取得通常需要人去介入，比如為何發不發信用卡要看申請者的年收入，這就是因為人們覺得年收入對於付不得付出卡費有關係。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-15.png\">\n</p>\n\n### 從輸入 X 的角度看機器學習，如果 X 的特徵是用最基礎未人為整理過的，這就叫 Raw Feature\n\n從輸入 X 的角度看機器學習，如果 X 的特徵是用最基礎未人為整理過的，這就叫 Raw Feature。比如聲音訊號的頻率，圖片的像素，這都是 Raw Feature。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-16.png\">\n</p>\n\n### 從輸入 X 的角度看機器學習，如果 X 的特徵是抽象的像是編號這樣的資料，這就叫 Abstract Feature\n\n從輸入 X 的角度看機器學習，如果 X 的特徵是抽象的像是編號這樣的資料，這就叫 Abstract Feature。這通常就需要有人去抽取出更具象的特徵資料出來，這些特徵可能包含 Concrete Feature 或 Raw Feature。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-17.png\">\n</p>\n\n### 從輸入 X 的角度看機器學習，做個小結\n\n從輸入 X 的角度看機器學習，如果 X 是明確定義的，那就是 Concrete Feature；如果 X 是未經人為定義過的，那就是 Raw Feature；如果 X 是抽象的編號，那就是 Abstract Feature。當然還有其他種特徵，其中最重要的核心就是 Concrete Feature。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-18.png\">\n</p>\n\n### 總結\n\n從輸出 y 的角度看機器學習、從輸入的資料 Yn 的角度看機器學習、從餵資料給機器的角度看機器學習、從輸入 X 的角度看機器學習都會有許多不同的機器學習方法，但重要的是了解哪些是核心，其他機器學習方法也都是從這些核心發展而來。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-19.png\">\n</p>","html":"<h3 id=\"\">前言</h3>\n\n<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href=\"http://blog.fukuball.com/lin-xuan-tian-jiao-shou-machine-learning-foundations-di-er-jiang-xue-xi-bi-ji/\">第二講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>\n\n<p>第三講的內容偏向介紹各種機器學習方法，以前念論文的時候看到這些名詞都會覺得高深莫測，但其實這各式各樣的機器學習方法其實都是從最基礎的核心變化而來，所以不要被嚇到。了解各種機器學習方法的輸入輸出對於日後面對一些問題的時候，我們才能夠知道要挑選什麼機器學習方法來解決問題。</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">熱身回顧一下</h3>\n\n<p>我們先來回顧一下上一講的內容，在上一講我們知道了如何使用 PLA 讓機器學會回答是非題這樣的兩類問題（Binary Classificaction），套到機器學習的那句名句，我們可以清楚的了解，PLA 這個演算法 A 觀察了線性可分（linear separable）的 D 及感知假設集合 H 去得到一個最好的假設 g，這一句話就可以概括到上一講的內容了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-0.png\">\n</p>\n\n<h3 id=\"yybinaryclassification\">從輸出 y 的角度看機器學習，y 只有兩個答案選一個，就叫 Binary Classification</h3>\n\n<p>接下來我們來了解一下各式各樣的學習方法，從輸出 y 的角度看機器學習，y 只有兩個答案選一個，就叫 Binary Classification，像是之前的是否發信用卡的例子就是 Binary Classification。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-1.png\">\n</p>\n\n<h3 id=\"yymulticlassclassification\">從輸出 y 的角度看機器學習，y 有多個答案選一個，就叫 Multiclass Classification</h3>\n\n<p>從輸出 y 的角度看機器學習，y 有多個答案選一個，就叫 Multiclass Classification，像是使用投飲機辨識錢幣的問題就是一個 Multiclass Classification 的問題，所以我們可以將分類問題推廣到分成 K 類，這樣 Binary Classificatin 就是一個 K=2 的分類問題。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-2.png\">\n</p>\n\n<h3 id=\"yyregression\">從輸出 y 的角度看機器學習，y 為一個實數，就叫 Regression</h3>\n\n<p>從輸出 y 的角度看機器學習，y 為一個實數，就叫 Regression，像是要預估病人再過幾天病會好，這就需要用到 Regression，這也會用到許多統計學相關的工具。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-3.png\">\n</p>\n\n<h3 id=\"yystructuredlearning\">從輸出 y 的角度看機器學習，y 為一個結構序列，就叫 Structured Learning</h3>\n\n<p>從輸出 y 的角度看機器學習，y 為一個結構序列，就叫 Structured Learning，比如一個句子的詞性分析，會需要考慮到句子中的前後文，而句子的組合可能有無限多種，因此不能單純用 Multiclass Classification 來做到，這就需要用到 Structured Learning 相關的機器學習方法。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-4.png\">\n</p>\n\n<h3 id=\"y\">從輸出 y 的角度看機器學習，做個小結</h3>\n\n<p>從輸出 y 的角度看機器學習，如果 y 是兩類，那就是 Binary Classification；如果 y 是 k 類，那就是 Multiclass Classification；如果 y 是一個實數，那就是 Regression；如果 y 是一種結構關係，那就是 Structured Learning。當然還有其他變化，不過基礎上就是 Binary Classification 及 Regression，我們可以透過這兩個基礎核心來延伸出其他機器學習方法。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-5.png\">\n</p>\n\n<h3 id=\"ynxnynsupervisedlearning\">從輸入的資料 Yn 的角度看機器學習，如果每個 Xn 都有明確對應的 Yn，這就叫監督式學習（Supervised Learning）</h3>\n\n<p>從輸入的資料 Yn 的角度看機器學習，如果每個 Xn 都有明確對應的 Yn，這就叫監督式學習（Supervised Learning），比如在訓練投飲機辨識錢幣的時候，我們很完整個告訴他什麼大小、什麼重量就是什麼幣值的錢幣，這樣就是一種監督式學習方法。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-6.png\">\n</p>\n\n<h3 id=\"ynxnynunsupervisedlearning\">從輸入的資料 Yn 的角度看機器學習，如果每個 Xn 都沒有標示 Yn，這就叫非監督式學習（Unsupervised Learning）</h3>\n\n<p>從輸入的資料 Yn 的角度看機器學習，如果每個 Xn 都沒有標示 Yn，這就叫非監督式學習（Unsupervised Learning）。比如在訓練投飲機辨識錢幣的時候，我們只告訴投飲機錢幣的大小及重量，但不告訴他什麼大小及重量個錢幣是哪個幣值的錢幣，讓機器自己去觀察特徵將這些錢幣分成一群一群，這又叫做分群，這就是一種非監督式學習方法。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-7.png\">\n</p>\n\n<h3 id=\"ynxnynsemisupervisedlearning\">從輸入的資料 Yn 的角度看機器學習，如果 Xn 只有部分有標示 Yn，這就叫半監督式學習（Semi-supervised Learning）</h3>\n\n<p>從輸入的資料 Yn 的角度看機器學習，如果 Xn 只有部分有標示 Yn，這就叫半監督式學習（Semi-supervised Learning），有些資料較難取得的狀況下，我們會使用到半監度式學習，比如在預測藥物是否對病人有效時，由於做人體實驗成本高且可能要等一段時間來看藥效，這樣的情況下標示藥物有效或沒效的成本很高，所以就可能需要用到半監度式學習。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-8.png\">\n</p>\n\n<h3 id=\"ynynreinforcementlearning\">從輸入的資料 Yn 的角度看機器學習，如果 Yn 是很難確知描述的，只能在機器作出反應時使用處罰及獎勵的方式讓機器知道對或錯，這就叫增強式學習（Reinforcement Learning）</h3>\n\n<p>從輸入的資料 Yn 的角度看機器學習，如果 Yn 是很難確知描述的，只能在機器作出反應時使用處罰及獎勵的方式讓機器知道對或錯，這就叫增強式學習（Reinforcement Learning）。這樣的機器學習方式，比較像自然界生物的學習方式，就像你要教一隻狗坐下，你很難直接告訴他怎麼做，而是用獎勵或處罰的方式讓狗狗漸漸知道坐下是什麼。增強式學習也就是這樣的機器學習方法，透過一次一次經驗的累積讓機器能夠學習到一個技能。比如像是教機器學習下棋，我們也可以透過勝負讓機器漸漸學習到如何下棋會下得更好。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-9.png\">\n</p>\n\n<h3 id=\"yn\">從輸入的資料 Yn 的角度看機器學習，做個小結</h3>\n\n<p>從輸入的資料 Yn 的角度看機器學習，如果明確告知每個 Yn，那就是監督式學習；如果沒有告知任何 Yn，那就是非監督式學習；如果只有部份 Yn 的資料，那就是半監督式學習；如果是用獎勵、處罰的方式來告知 Yn，那就是增強式學習。當然還有其他種機器學習方法，其中最重要的核心就是監督式學習方法。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-10.png\">\n</p>\n\n<h3 id=\"batchlearning\">從餵資料給機器的角度看機器學習，一次餵進全部資料，這就叫 Batch Learning</h3>\n\n<p>從餵資料給機器的角度看機器學習，一次餵進全部資料，這就叫 Batch Learning。監督式學習方法，可能也會常使用 Batch Learning 的方式為資料。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-11.png\">\n</p>\n\n<h3 id=\"onlinelearning\">從餵資料給機器的角度看機器學習，可以再慢慢餵進新資料，這就叫 Online Learning</h3>\n\n<p>從餵資料給機器的角度看機器學習，可以再慢慢餵進新資料，這就叫 Online Learning。Batch Learging 訓練好的機器，就無法調整他的技巧，可能會有越來越不準的情況，所以 Online Learning 可以再慢慢調整、增進技巧。PLA 算法可以很容易應用在 Online Learning 上，增強式學習方法也常常是使用 Online Learning 的方式餵資料。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-12.png\">\n</p>\n\n<h3 id=\"activelearning\">從餵資料給機器的角度看機器學習，機器可以問問題，然後從問題的答案再餵進資料，這就叫 Active Learning</h3>\n\n<p>從餵資料給機器的角度看機器學習，機器可以問問題，然後從問題的答案再餵進資料，這就叫 Active Learning。這樣的學習方法是要希望讓機器可以用一些策略問問題，然後慢慢學習、改善技巧。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-13.png\">\n</p>\n\n<h3 id=\"\">從餵資料給機器的角度看機器學習，做個小結</h3>\n\n<p>從餵資料給機器的角度看機器學習，如果一次餵進所有資料，就叫 Batch Learning；如果後續可以再慢慢餵進資料，就叫 Online Learning；如果機器可以問問題來餵進資料，就叫 Active Learning。當然還有其他種機器學習方法，其中最重要的核心就是 Batch Learning。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-14.png\">\n</p>\n\n<h3 id=\"xxconcretefeature\">從輸入 X 的角度看機器學習，如果 X 的特徵很明確定義，這就叫 Concrete Feature</h3>\n\n<p>從輸入 X 的角度看機器學習，如果 X 的特徵很明確定義，這就叫 Concrete Feature。Concrete Featrue 的取得通常需要人去介入，比如為何發不發信用卡要看申請者的年收入，這就是因為人們覺得年收入對於付不得付出卡費有關係。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-15.png\">\n</p>\n\n<h3 id=\"xxrawfeature\">從輸入 X 的角度看機器學習，如果 X 的特徵是用最基礎未人為整理過的，這就叫 Raw Feature</h3>\n\n<p>從輸入 X 的角度看機器學習，如果 X 的特徵是用最基礎未人為整理過的，這就叫 Raw Feature。比如聲音訊號的頻率，圖片的像素，這都是 Raw Feature。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-16.png\">\n</p>\n\n<h3 id=\"xxabstractfeature\">從輸入 X 的角度看機器學習，如果 X 的特徵是抽象的像是編號這樣的資料，這就叫 Abstract Feature</h3>\n\n<p>從輸入 X 的角度看機器學習，如果 X 的特徵是抽象的像是編號這樣的資料，這就叫 Abstract Feature。這通常就需要有人去抽取出更具象的特徵資料出來，這些特徵可能包含 Concrete Feature 或 Raw Feature。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-17.png\">\n</p>\n\n<h3 id=\"x\">從輸入 X 的角度看機器學習，做個小結</h3>\n\n<p>從輸入 X 的角度看機器學習，如果 X 是明確定義的，那就是 Concrete Feature；如果 X 是未經人為定義過的，那就是 Raw Feature；如果 X 是抽象的編號，那就是 Abstract Feature。當然還有其他種特徵，其中最重要的核心就是 Concrete Feature。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-18.png\">\n</p>\n\n<h3 id=\"\">總結</h3>\n\n<p>從輸出 y 的角度看機器學習、從輸入的資料 Yn 的角度看機器學習、從餵資料給機器的角度看機器學習、從輸入 X 的角度看機器學習都會有許多不同的機器學習方法，但重要的是了解哪些是核心，其他機器學習方法也都是從這些核心發展而來。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/machine-learning-foundations-3-19.png\">\n</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-09-11T12:50:54.000Z","created_by":1,"updated_at":"2016-08-09T05:31:07.000Z","updated_by":1,"published_at":"2015-09-12T11:32:31.000Z","published_by":1},{"id":64,"uuid":"03f223f8-48a2-4df9-bee5-3c61c6f3c3f3","title":"林軒田教授機器學習基石 MACHINE LEARNING FOUNDATIONS 第四講學習筆記","slug":"lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-si-jiang-xue-xi-bi-ji","markdown":"### 前言\n\n本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 [第三講](http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-san-jiang-xue-xi-bi-ji/) 的碼農們，我建議可以先回頭去讀一下再回來喔！\n\n第四講的內容主要是讓我們知道機器學習是否真的可能，並利用數學上的定理來說明機器學習在某些情境之下是可能的，有數學上定理的支持，我們就可以放心的利用機器學習來解決我們所面對的一些問題。\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 熱身回顧一下\n\n我們先來回顧一下上一講的內容，在上一講我們知道了各式各樣的機器學習方法及名詞，而我們未來會專注於二元分類及迴歸這樣的問題，然後使用大量監督式標示好的資料且定義明確的特徵來進行機器學習。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-1.png\">\n</p>\n\n### 看看這個問題，想想如何使用學習\n\n有人會問，說了這麼多，如何知道機器學習是不是真的可能？說不定根本無法做到。比如這個問題，g(x)可以回答 +1 還是 -1。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-2.png\">\n</p>\n\n### 見仁見智的問題無法解\n\n像這樣的問題，你可以回答 +1，因為 +1 的圖都是對稱的，而這個圖是對稱的，所以是 +1。你可以回答 -1，因為 -1 的圖都是左上方黑色的，而這個圖是左上方黑色的，所以是 -1。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-3.png\">\n</p>\n\n### 套到二元分類的問題\n\n現在我們套到二元分類的問題，給了 Xn 及 Yn，然後機器學習出了 g，我們可以說 g 近似於 f 嗎？\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-4.png\">\n</p>\n\n### 天下沒有白吃的午餐\n\n在驗證 g 的時候，如果是用原本的 D，那我們很容易的說明 g 近似於 f，但是如果資料是用 D 以外的資料來驗證，那我們無法很明確的說明 g 近似於 f，但我們要的其實就是希望 g 在 D 以外的資料也近似於 f，這有可能嗎？\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-5.png\">\n</p>\n\n### 利用罐子取彈珠的例子來說明是否可能\n\n現在想像我們有一個裡面有很多橘色和綠色彈珠的罐子，我們可能無法知道橘色彈珠的真實比例，但我們可以推估出橘色彈珠出現的機率嗎？\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-6.png\">\n</p>\n\n### 取樣看看\n\n我們取樣看看，比如從罐子中取出十顆彈珠，假設現在取出的是三顆橘色七顆綠色，那我們可以說罐子中的比例是 30% 橘色 70% 綠色嗎？可能不能這樣說，有可能不是這樣比例，但很可能 30% 橘色 70% 綠色是一個很接近的比例數字。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-7.png\">\n</p>\n\n### Hoeffding 不等式 1\n\n我們可以說取樣出來的結果會很接近真實情況，是因為 Hoeffding's Inequality 這個定理。這個定理的數字如下圖，這說明了當 N 很大，也就是我們取樣的數字很大，那們 v - u 就會是一個很小的數字，也就是我們預估的 g 跟真實的 f 的差距很小。而當 ε 很大，也就是我們可以容忍的誤差很大的話，那當然我們就可以很容易地說 g 跟真實的 f 的差距很小。只要符合了這個不等式，那就叫做 probably approximately correct（PAC）。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-8.png\">\n</p>\n\n### Hoeffding 不等式 2\n\n通常我們不會希望容忍誤差很大，所以通常我們會取 N 很大來推估 g 是否接近 f 的真實情況。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-9.png\">\n</p>\n\n### 把 Hoeffding 不等式連結到機器學習\n\n當我們現在有一個固定的 h(x)（假設集合 H 的其中一個，他有可能是 g），我們想要知道是否接近 f(x)，x 從 X 中取出，如果 h(x)  ̸= f(x)，就是 h 答錯，就像是罐子中的橘色彈珠，如果 h(x) = f(x)，就是 h 答對，就像是罐子中的綠色彈珠，如果取樣的數字很大的話，那我們就可以用部分的已知資料來大概推估 h(x) 在未知資料的表現情況。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-10.png\">\n</p>\n\n### 把這個誤差推估的概念加進我們的機器學習圖表\n\n把這個誤差推估的概念加進我們的機器學習圖表，任何一個 h 都可以用已知的 Ein 來推估未知的 Eout，如果 N 夠大。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-11.png\">\n</p>\n\n### 套進 Hoeffding 不等式\n\n我們套進 Hoeffding 不等式，Ein 代表在已知取樣的中 h(x) 跟 f(x) 的誤差，Eout 代表其他未知的資料中 h(x) 跟 f(x) 的誤差，這在 Hoeffding 不等式的定理下，我們知道 N 很大時，Ein 與 Eout 的誤差很接近，所以我們可以說 Ein 與 Eout 是 PAC。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-12.png\">\n</p>\n\n### 驗證 h(x) 好不好\n\n所以這常常會被機器學習用在驗證得出來的 h(x) 好不好，可以用這個圖來簡單表示，只要 N 夠大，取樣的分佈一致，那他的表現結果好與不好，是可以很明確地用部分已知資料來推估它在其他未知資料表現得好不好。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-13.png\">\n</p>\n\n### 好死不死取到壞資料的情況\n\n我們還是會有好死不死取到壞資料的情況，但一樣可以用 Hoeffding 不等式說明這個機率很小，但是還是會發生。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-14.png\">\n</p>\n\n### 如果有很多個 h\n\n我們剛剛都是用一個 h 來推估是否機器學習是可能的，的確單一個 h 的時候，我們可以用取樣的資料來說明 h(x) 是否跟 f(x) 接近。那如果有很多個 h 呢？\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-15.png\">\n</p>\n\n### 一樣套進 Hoeffding 不等式\n\n如果是有限個 h，如 M 個 h，那一樣我們可以用取樣的資料來說明 h(x) 是否跟 f(x) 接近。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-16.png\">\n</p>\n\n### 將這樣的概念加進去機器學習圖表\n\n如果 H 這個集合只有 M 個，然後取樣的 N 夠大，利用 A 取出 g，如果 Ein(g) 接近 0，那麼 g 就是一個 PAC 的答案了，我可由此可知機器學習是可能的。那如果 M 是無限大呢？這個就要下一講來解答了。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-17.png\">\n</p>\n\n### 總結\n\n這一張說明了，我們無法直觀地說明得到的 h 是否能就是 f，因為天下沒有白吃的午餐。但我們用定理說明的 h 可以在未知的資料中 probably approximate correct。我們將定理連結到機器學習，就可以驗證 h。然後學習的過程也可以套進這樣的概念，當 H 中的 h 數量非無限的時候，機器學習是可能的。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-18.png\">\n</p>","html":"<h3 id=\"\">前言</h3>\n\n<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href=\"http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-san-jiang-xue-xi-bi-ji/\">第三講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>\n\n<p>第四講的內容主要是讓我們知道機器學習是否真的可能，並利用數學上的定理來說明機器學習在某些情境之下是可能的，有數學上定理的支持，我們就可以放心的利用機器學習來解決我們所面對的一些問題。</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">熱身回顧一下</h3>\n\n<p>我們先來回顧一下上一講的內容，在上一講我們知道了各式各樣的機器學習方法及名詞，而我們未來會專注於二元分類及迴歸這樣的問題，然後使用大量監督式標示好的資料且定義明確的特徵來進行機器學習。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-1.png\">\n</p>\n\n<h3 id=\"\">看看這個問題，想想如何使用學習</h3>\n\n<p>有人會問，說了這麼多，如何知道機器學習是不是真的可能？說不定根本無法做到。比如這個問題，g(x)可以回答 +1 還是 -1。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-2.png\">\n</p>\n\n<h3 id=\"\">見仁見智的問題無法解</h3>\n\n<p>像這樣的問題，你可以回答 +1，因為 +1 的圖都是對稱的，而這個圖是對稱的，所以是 +1。你可以回答 -1，因為 -1 的圖都是左上方黑色的，而這個圖是左上方黑色的，所以是 -1。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-3.png\">\n</p>\n\n<h3 id=\"\">套到二元分類的問題</h3>\n\n<p>現在我們套到二元分類的問題，給了 Xn 及 Yn，然後機器學習出了 g，我們可以說 g 近似於 f 嗎？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-4.png\">\n</p>\n\n<h3 id=\"\">天下沒有白吃的午餐</h3>\n\n<p>在驗證 g 的時候，如果是用原本的 D，那我們很容易的說明 g 近似於 f，但是如果資料是用 D 以外的資料來驗證，那我們無法很明確的說明 g 近似於 f，但我們要的其實就是希望 g 在 D 以外的資料也近似於 f，這有可能嗎？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-5.png\">\n</p>\n\n<h3 id=\"\">利用罐子取彈珠的例子來說明是否可能</h3>\n\n<p>現在想像我們有一個裡面有很多橘色和綠色彈珠的罐子，我們可能無法知道橘色彈珠的真實比例，但我們可以推估出橘色彈珠出現的機率嗎？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-6.png\">\n</p>\n\n<h3 id=\"\">取樣看看</h3>\n\n<p>我們取樣看看，比如從罐子中取出十顆彈珠，假設現在取出的是三顆橘色七顆綠色，那我們可以說罐子中的比例是 30% 橘色 70% 綠色嗎？可能不能這樣說，有可能不是這樣比例，但很可能 30% 橘色 70% 綠色是一個很接近的比例數字。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-7.png\">\n</p>\n\n<h3 id=\"hoeffding1\">Hoeffding 不等式 1</h3>\n\n<p>我們可以說取樣出來的結果會很接近真實情況，是因為 Hoeffding's Inequality 這個定理。這個定理的數字如下圖，這說明了當 N 很大，也就是我們取樣的數字很大，那們 v - u 就會是一個很小的數字，也就是我們預估的 g 跟真實的 f 的差距很小。而當 ε 很大，也就是我們可以容忍的誤差很大的話，那當然我們就可以很容易地說 g 跟真實的 f 的差距很小。只要符合了這個不等式，那就叫做 probably approximately correct（PAC）。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-8.png\">\n</p>\n\n<h3 id=\"hoeffding2\">Hoeffding 不等式 2</h3>\n\n<p>通常我們不會希望容忍誤差很大，所以通常我們會取 N 很大來推估 g 是否接近 f 的真實情況。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-9.png\">\n</p>\n\n<h3 id=\"hoeffding\">把 Hoeffding 不等式連結到機器學習</h3>\n\n<p>當我們現在有一個固定的 h(x)（假設集合 H 的其中一個，他有可能是 g），我們想要知道是否接近 f(x)，x 從 X 中取出，如果 h(x)  ̸= f(x)，就是 h 答錯，就像是罐子中的橘色彈珠，如果 h(x) = f(x)，就是 h 答對，就像是罐子中的綠色彈珠，如果取樣的數字很大的話，那我們就可以用部分的已知資料來大概推估 h(x) 在未知資料的表現情況。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-10.png\">\n</p>\n\n<h3 id=\"\">把這個誤差推估的概念加進我們的機器學習圖表</h3>\n\n<p>把這個誤差推估的概念加進我們的機器學習圖表，任何一個 h 都可以用已知的 Ein 來推估未知的 Eout，如果 N 夠大。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-11.png\">\n</p>\n\n<h3 id=\"hoeffding\">套進 Hoeffding 不等式</h3>\n\n<p>我們套進 Hoeffding 不等式，Ein 代表在已知取樣的中 h(x) 跟 f(x) 的誤差，Eout 代表其他未知的資料中 h(x) 跟 f(x) 的誤差，這在 Hoeffding 不等式的定理下，我們知道 N 很大時，Ein 與 Eout 的誤差很接近，所以我們可以說 Ein 與 Eout 是 PAC。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-12.png\">\n</p>\n\n<h3 id=\"hx\">驗證 h(x) 好不好</h3>\n\n<p>所以這常常會被機器學習用在驗證得出來的 h(x) 好不好，可以用這個圖來簡單表示，只要 N 夠大，取樣的分佈一致，那他的表現結果好與不好，是可以很明確地用部分已知資料來推估它在其他未知資料表現得好不好。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-13.png\">\n</p>\n\n<h3 id=\"\">好死不死取到壞資料的情況</h3>\n\n<p>我們還是會有好死不死取到壞資料的情況，但一樣可以用 Hoeffding 不等式說明這個機率很小，但是還是會發生。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-14.png\">\n</p>\n\n<h3 id=\"h\">如果有很多個 h</h3>\n\n<p>我們剛剛都是用一個 h 來推估是否機器學習是可能的，的確單一個 h 的時候，我們可以用取樣的資料來說明 h(x) 是否跟 f(x) 接近。那如果有很多個 h 呢？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-15.png\">\n</p>\n\n<h3 id=\"hoeffding\">一樣套進 Hoeffding 不等式</h3>\n\n<p>如果是有限個 h，如 M 個 h，那一樣我們可以用取樣的資料來說明 h(x) 是否跟 f(x) 接近。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-16.png\">\n</p>\n\n<h3 id=\"\">將這樣的概念加進去機器學習圖表</h3>\n\n<p>如果 H 這個集合只有 M 個，然後取樣的 N 夠大，利用 A 取出 g，如果 Ein(g) 接近 0，那麼 g 就是一個 PAC 的答案了，我可由此可知機器學習是可能的。那如果 M 是無限大呢？這個就要下一講來解答了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-17.png\">\n</p>\n\n<h3 id=\"\">總結</h3>\n\n<p>這一張說明了，我們無法直觀地說明得到的 h 是否能就是 f，因為天下沒有白吃的午餐。但我們用定理說明的 h 可以在未知的資料中 probably approximate correct。我們將定理連結到機器學習，就可以驗證 h。然後學習的過程也可以套進這樣的概念，當 H 中的 h 數量非無限的時候，機器學習是可能的。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Learning is Impossible-4-18.png\">\n</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-09-27T09:35:33.000Z","created_by":1,"updated_at":"2016-12-14T05:36:34.000Z","updated_by":1,"published_at":"2015-09-27T10:49:15.000Z","published_by":1},{"id":65,"uuid":"003852f0-ba05-4ec0-aa7d-3b2d331f9b68","title":"林軒田教授機器學習基石 MACHINE LEARNING FOUNDATIONS 第五講學習筆記","slug":"lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-wu-jiang-xue-xi-bi-ji","markdown":"### 前言\n\n本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 [第四講](http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-si-jiang-xue-xi-bi-ji/) 的碼農們，我建議可以先回頭去讀一下再回來喔！\n\n在第四講中我們了解了在有限假設集合的情況下機器學習是可能的，而第五講就是想要將有限假設集合可以推廣出去，讓我們在無限的假設集合裡也可以透過一些理論慢慢收斂到一個多項式集合，如此我們就可以放心的利用機器學習來解決我們所面對的一些問題。\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 熱身回顧一下\n\n我們先來回顧一下上一講的內容，在上一講我們知道了機器學習在足夠的資料及有限的假設集合這種情況下是可行的。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-1.png\">\n</p>\n\n### 更新機器學習流程圖\n\n如果假設集合 H 是有限 M 個，然後資料量夠多 N，不管我們的演算法 A 是什麼，我們由定理可以知道 Eout 跟 Ein 是很接近的。所以如果 A 找到了一個假設 g 讓 Ein 近似於 0，那我們就可以說 Eout 大概就會是 0，因此機器學習在這樣的情況下是可行的。\n\n有了這樣的概念，我們擴充了我們的機器學習流程圖。從輸入資料中去訓練機器學習，然後得到 Ein 近似於 0，之後再從同一個資料分佈中去測試機器學習的結果，如此可以證明機器有學習到技能。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-2.png\">\n</p>\n\n### 機器學習的兩個核心問題\n\n機器學習有兩個核心問題，我們希望 Eout 跟 Ein 是很接近的，這個意思就是說，我們希望後來測試學習的結果，會跟訓練時得到的結果很接近，這樣我們才能說機器有學習到技能。\n\n另一個就是，我們希望 Ein 可以很小，也就是訓練的過程中，我們希望機器就可以得到很好的效果，也就是誤差很小。\n\n那之前我們所說的有限集合 M 在這邊扮演什麼角色呢？\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-3.png\">\n</p>\n\n### Ｍ 的兩難\n\n如果假設集合 M 很小，我們可以保證 Eout 可以接近 Ein，但是因為假設集合小，可以挑選的選擇就少，也因此 Ein 可能會是一個不小的值，也就是誤差會大。\n\n如果假設集合 M 很大，我們可以會得到一個比較好的 Ein，也就是誤差比較小，但是 M 太大我們就無法保證 Eout 會跟 Ein 接近，也就是我們無法保證學習的結果，那機器就白學了。\n\n所以 M 是在哪個值剛好能同時解決這個兩個問題就是一個重點。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-4.png\">\n</p>\n\n### Ｍ 從哪裡來？\n\n從上一講的定理證明中，我們知道 M 是從當較大的誤差發生的情況累積出來的，所以如果假設集合越大，那累積出來的就一個越大的數字，如果是無限集合，那就沒有上限，但如果是有限集合的話，那就會有個上限。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-6.png\">\n</p>\n\n### 大誤差發生的情況有很多時候是重疊的\n\n但其實那個上限值我們是高估的，因為許多種大誤差發生的情況都可能是很相似的情況，所以其實這些大誤差發生的情況有很大一部份是重疊的。\n\n那我們可以轉一個想法，我們可以把無限的假設集合依照它們的類型來分類嗎？這樣就可以轉換成一個有限的集合。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-7.png\">\n</p>\n\n### 一個點的時候有幾種線\n\n之前的 PLA 演算法其實有無限多的假設集合 H，所以在平面上分類一個點的線有無限多條，但是如果說有幾種線，那就只有兩種，一種線是將 x1 分成 o 的，一種線是將 x1 分成 x 的。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-8.png\">\n</p>\n\n### 兩個點的時候有幾種線\n\n以此類推，兩個點的時候，平面上會多多少種分類這兩個點的線呢？答案是四種線。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-9.png\">\n</p>\n\n### 四個點的時候有幾種線\n\n再以此類推，四個點的時候，平面上會多多少種分類這四個點的線呢？我們會發現下圖中有兩個組合已經無法找到直線可以做好分類，我們最多只能找到 14 條可以分類四個點的線。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-10.png\">\n</p>\n\n### Effective Number of Lines\n\n這個故事告訴我們，隨著輸入變多，線的種類會慢慢變成不是指數型成長。依這樣的概念，我們將原本無限多的線轉變為有限多的不同種類的線，我們就可以用這個有限多的不同種類的線的數字來取代 M，這個數字會小於 2 的 N 次方。\n\n這樣我們就可以再套到霍夫丁的定理，知道這樣的過程機器學習的解決會是有效可行的。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-11.png\">\n</p>\n\n### Dichotomies\n\n這樣的過程就是將原本無限多的線，轉換成分種類的線 Dichotomies，一個 Dichotomy 就是一種分類組合，在二元分類裡這樣組合的上界就是 2 的 N 次方，我們可以用這個數字來取代無限大的 M。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-12.png\">\n</p>\n\n### 找出問題的成長函數\n\n了解這樣的特性之後，我們只要找出我們要解的問題的成長函數，而這個成長函數不會無限增加，那機器學習就可以證明是可行的。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-13.png\">\n</p>\n\n### 四種成長函數\n\n我們這邊列出了四種成長函數，分屬不同的問題，PLA 是屬於 2D perceptrons，mH 會小於 2 的 N 次方，代進霍夫丁不等式，由於是指數型成長，並無法保証式子會成立。所以我們希望能證明 PLA 的 mH 會是一個多項式，這樣才能保證霍夫丁不等式成立。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-14.png\">\n</p>\n\n### Break Point 的概念\n\nBreak Point 指的是，當下一個輸入出現時，Dichotomies 組合不再是指數型成長的的那個點，在 2D perceptrons 這邊從我們剛剛的例子可以知道 break point 出現在 4 個輸入時。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-15.png\">\n</p>\n\n### 四種成長函數的 Break Points\n\n我們知道 positive rays 的 break point 出現在 2，他的成長函數是 big O N，positive intervals 的 break point 出現在 3，他的成長函數是 big O N 平方，那麼在 2D perceptrons 時，我們知道 break point 出現在 4，那他的成長函數是 big O N 三次方嗎？我們可以推廣成一個通式嗎？這就是下次課程的內容了。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-16.png\">\n</p>\n\n### 總結\n\n機器學習可能的兩個核心問題是 Ein 近似於 0，且 Eout 跟 Ein 要接近。PLA 無限多的線我們可以轉換成 Effective Number of Lines 也就是轉換成 Effective Numbers of Hypotheses，M 無限集合也就轉為 mH 的有限集合，然後觀察 break point 的出現，讓我們可以知道假設集合不會是一個指數型成長的情況，如此依照霍夫丁不等式所說的，我們就可以讓機器學習的理論有充分的證明為可行了。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-17.png\">\n</p>\n\n\n","html":"<h3 id=\"\">前言</h3>\n\n<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href=\"http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-si-jiang-xue-xi-bi-ji/\">第四講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>\n\n<p>在第四講中我們了解了在有限假設集合的情況下機器學習是可能的，而第五講就是想要將有限假設集合可以推廣出去，讓我們在無限的假設集合裡也可以透過一些理論慢慢收斂到一個多項式集合，如此我們就可以放心的利用機器學習來解決我們所面對的一些問題。</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">熱身回顧一下</h3>\n\n<p>我們先來回顧一下上一講的內容，在上一講我們知道了機器學習在足夠的資料及有限的假設集合這種情況下是可行的。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-1.png\">\n</p>\n\n<h3 id=\"\">更新機器學習流程圖</h3>\n\n<p>如果假設集合 H 是有限 M 個，然後資料量夠多 N，不管我們的演算法 A 是什麼，我們由定理可以知道 Eout 跟 Ein 是很接近的。所以如果 A 找到了一個假設 g 讓 Ein 近似於 0，那我們就可以說 Eout 大概就會是 0，因此機器學習在這樣的情況下是可行的。</p>\n\n<p>有了這樣的概念，我們擴充了我們的機器學習流程圖。從輸入資料中去訓練機器學習，然後得到 Ein 近似於 0，之後再從同一個資料分佈中去測試機器學習的結果，如此可以證明機器有學習到技能。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-2.png\">\n</p>\n\n<h3 id=\"\">機器學習的兩個核心問題</h3>\n\n<p>機器學習有兩個核心問題，我們希望 Eout 跟 Ein 是很接近的，這個意思就是說，我們希望後來測試學習的結果，會跟訓練時得到的結果很接近，這樣我們才能說機器有學習到技能。</p>\n\n<p>另一個就是，我們希望 Ein 可以很小，也就是訓練的過程中，我們希望機器就可以得到很好的效果，也就是誤差很小。</p>\n\n<p>那之前我們所說的有限集合 M 在這邊扮演什麼角色呢？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-3.png\">\n</p>\n\n<h3 id=\"\">Ｍ 的兩難</h3>\n\n<p>如果假設集合 M 很小，我們可以保證 Eout 可以接近 Ein，但是因為假設集合小，可以挑選的選擇就少，也因此 Ein 可能會是一個不小的值，也就是誤差會大。</p>\n\n<p>如果假設集合 M 很大，我們可以會得到一個比較好的 Ein，也就是誤差比較小，但是 M 太大我們就無法保證 Eout 會跟 Ein 接近，也就是我們無法保證學習的結果，那機器就白學了。</p>\n\n<p>所以 M 是在哪個值剛好能同時解決這個兩個問題就是一個重點。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-4.png\">\n</p>\n\n<h3 id=\"\">Ｍ 從哪裡來？</h3>\n\n<p>從上一講的定理證明中，我們知道 M 是從當較大的誤差發生的情況累積出來的，所以如果假設集合越大，那累積出來的就一個越大的數字，如果是無限集合，那就沒有上限，但如果是有限集合的話，那就會有個上限。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-6.png\">\n</p>\n\n<h3 id=\"\">大誤差發生的情況有很多時候是重疊的</h3>\n\n<p>但其實那個上限值我們是高估的，因為許多種大誤差發生的情況都可能是很相似的情況，所以其實這些大誤差發生的情況有很大一部份是重疊的。</p>\n\n<p>那我們可以轉一個想法，我們可以把無限的假設集合依照它們的類型來分類嗎？這樣就可以轉換成一個有限的集合。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-7.png\">\n</p>\n\n<h3 id=\"\">一個點的時候有幾種線</h3>\n\n<p>之前的 PLA 演算法其實有無限多的假設集合 H，所以在平面上分類一個點的線有無限多條，但是如果說有幾種線，那就只有兩種，一種線是將 x1 分成 o 的，一種線是將 x1 分成 x 的。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-8.png\">\n</p>\n\n<h3 id=\"\">兩個點的時候有幾種線</h3>\n\n<p>以此類推，兩個點的時候，平面上會多多少種分類這兩個點的線呢？答案是四種線。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-9.png\">\n</p>\n\n<h3 id=\"\">四個點的時候有幾種線</h3>\n\n<p>再以此類推，四個點的時候，平面上會多多少種分類這四個點的線呢？我們會發現下圖中有兩個組合已經無法找到直線可以做好分類，我們最多只能找到 14 條可以分類四個點的線。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-10.png\">\n</p>\n\n<h3 id=\"effectivenumberoflines\">Effective Number of Lines</h3>\n\n<p>這個故事告訴我們，隨著輸入變多，線的種類會慢慢變成不是指數型成長。依這樣的概念，我們將原本無限多的線轉變為有限多的不同種類的線，我們就可以用這個有限多的不同種類的線的數字來取代 M，這個數字會小於 2 的 N 次方。</p>\n\n<p>這樣我們就可以再套到霍夫丁的定理，知道這樣的過程機器學習的解決會是有效可行的。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-11.png\">\n</p>\n\n<h3 id=\"dichotomies\">Dichotomies</h3>\n\n<p>這樣的過程就是將原本無限多的線，轉換成分種類的線 Dichotomies，一個 Dichotomy 就是一種分類組合，在二元分類裡這樣組合的上界就是 2 的 N 次方，我們可以用這個數字來取代無限大的 M。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-12.png\">\n</p>\n\n<h3 id=\"\">找出問題的成長函數</h3>\n\n<p>了解這樣的特性之後，我們只要找出我們要解的問題的成長函數，而這個成長函數不會無限增加，那機器學習就可以證明是可行的。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-13.png\">\n</p>\n\n<h3 id=\"\">四種成長函數</h3>\n\n<p>我們這邊列出了四種成長函數，分屬不同的問題，PLA 是屬於 2D perceptrons，mH 會小於 2 的 N 次方，代進霍夫丁不等式，由於是指數型成長，並無法保証式子會成立。所以我們希望能證明 PLA 的 mH 會是一個多項式，這樣才能保證霍夫丁不等式成立。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-14.png\">\n</p>\n\n<h3 id=\"breakpoint\">Break Point 的概念</h3>\n\n<p>Break Point 指的是，當下一個輸入出現時，Dichotomies 組合不再是指數型成長的的那個點，在 2D perceptrons 這邊從我們剛剛的例子可以知道 break point 出現在 4 個輸入時。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-15.png\">\n</p>\n\n<h3 id=\"breakpoints\">四種成長函數的 Break Points</h3>\n\n<p>我們知道 positive rays 的 break point 出現在 2，他的成長函數是 big O N，positive intervals 的 break point 出現在 3，他的成長函數是 big O N 平方，那麼在 2D perceptrons 時，我們知道 break point 出現在 4，那他的成長函數是 big O N 三次方嗎？我們可以推廣成一個通式嗎？這就是下次課程的內容了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-16.png\">\n</p>\n\n<h3 id=\"\">總結</h3>\n\n<p>機器學習可能的兩個核心問題是 Ein 近似於 0，且 Eout 跟 Ein 要接近。PLA 無限多的線我們可以轉換成 Effective Number of Lines 也就是轉換成 Effective Numbers of Hypotheses，M 無限集合也就轉為 mH 的有限集合，然後觀察 break point 的出現，讓我們可以知道假設集合不會是一個指數型成長的情況，如此依照霍夫丁不等式所說的，我們就可以讓機器學習的理論有充分的證明為可行了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-5-17.png\">\n</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-10-15T06:33:18.000Z","created_by":1,"updated_at":"2016-08-09T05:33:07.000Z","updated_by":1,"published_at":"2015-10-15T13:08:14.000Z","published_by":1},{"id":66,"uuid":"49210670-1be5-4f30-9430-59062c3eeaea","title":"林軒田教授機器學習基石 MACHINE LEARNING FOUNDATIONS 第六講學習筆記","slug":"lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-liu-jiang-xue-xi-bi-ji","markdown":"### 前言\n\n本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 [第五講](http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-wu-jiang-xue-xi-bi-ji/) 的碼農們，我建議可以先回頭去讀一下再回來喔！\n\n在第五講中我們了解了如何將 PLA 無限的假設集合透過 Dichotomy、Break Point 這樣的方式轉換成有限的集合，在第六講中我們將更進一步去推導其實這個假設集合的成長函數會是一個多項式，如此我們就可以完全相信PLA 機器學習方法的確在數學理論上是可行的了。\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 熱身回顧一下\n\n我們先來回顧一下上一講的內容，在上一講我們知道了成長函數似乎跟 break point 有些關係，這一講我們將慢慢導出這樣的關係其實是一個多項式。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-1.png\">\n</p>\n\n### 從 break point 找其他線索\n\n從上一講提到的 break point 我們知道了成長函數至少會小於 2 的 N 次方，且如果 break point 在k取到了，那 k+1, k+2,... 都會是 break point。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-2.png\">\n</p>\n\n### 由例子觀察 break point\n\n這邊林軒田教授用了一連串的例子說明了如果 H 有Break Point k，那當 N 大於 k 時，mH(N) 成長函數會大大縮減（以 binary classification 這個問題來說，所有的 H 為 2 的 N 次方個）。\n\n既然知道 mH(N) 成長函數會大大縮減，那究竟能縮減到什麼程度？會不會是一個多項式的形式呢？\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-3.png\">\n</p>\n\n### Bounding Function\n\n接下来，引出了 Bounding Function 的概念，Bounding Function 的意義就是為了看在 N 個樣本點的情況下，如果 Break Point 為 K，那 mH(N) 會縮減到什麼程度的一個函式。\n\n值得一提的是，這裡的 Bounding Function B(N, K) 只與 N 和 K 有關，與假設集合 H 無關，這樣在 Binary Classification 這個問題下，無論是 positive intervals 還是 1D perceptrons 他們的 Bounding Function 就都會是一樣的，如果我們能夠得到 Bounding Function 的結果，那就可以推廣到各個 Binary Classification 的問題。\n\n所以我們新的目標就是找出 B(N, K) 是不是小於等於 poly(N) 了。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-4.png\">\n</p>\n\n### 推導 Bounding Function 數學式\n\n從以下投影片的 Dichotomies 例子可以導出 Bounding Function 的一個上界，所以 B(N, k) 會是小於等於 B(N-1, k)+B(N-1, k-1)。（詳細過程可以參考影片）\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-5.png\">\n</p>\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-6.png\">\n</p>\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-7.png\">\n</p>\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-8.png\">\n</p>\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-9.png\">\n</p>\n\n### Bounding Function 定理\n\nBounding Function 的上界出現了我們想要的多項式形式，這裡得出了一個定理，只要 Break Point 存在，那 mH(N) 成長函數一定能夠被一個多項式包含住，我們可以用投影片中的式子來表示 B(N, k)。\n\n事實上式子中的\"小於等於\"可以直接是\"等於\"，這已有證明，但不在這次課程範圍，有興趣可以去找其他資料來看。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-10.png\">\n</p>\n\n### 找出 break point 就能算出成長函數\n\n有了 Bounding Function 定理，我們就可以透過找出 break point 來算出假設集合的成長函數，比如上一講中我們說 PLA 2D perceptrons 的成長函數小於 2 的 N 次方，現在我們可以說PLA 2D perceptrons 的成長函數小於等於 B(N, 4)，因為 2D perceptrons 的 break point 出現在 4。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-11.png\">\n</p>\n\n### 把 Bound Function 的定理引入 Hoeffding 不等式\n\n有了上面的推導，我們知道 mH(N) 是可以在某種條件下（Break Point k）被 Bound 住。現在回到 Hoeffding 不等式的 union bound 形式，把 Bound Function 的定理概念引入這個 Hoeffding 不等式可以慢慢推導出投影片中下面的式子。\n\n直觀的想法我們可以把 mH(N) 的上界直接代進 Hoeffding 不等式，但理論上我們不能這樣做，因為目前 Eout(h) 是無限個，因此我們需要做一些轉換才能購買 B(N, k) 引入這個 Hoeffding 不等式。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-12.png\">\n</p>\n\n### 第一步把 Eout(h) 轉換成有限個\n\n第一步我們需要把 Eout(h) 轉換成有限個，這裡沒有做嚴格的證明，只個了直觀的解釋，假設還有一筆資料 D‘（數量也是 N），得到的結果為 Ein‘，那 Ein 與 Eout 發生 BAD（差距很大），應該跟 Ein 與 Ein‘ 發生 BAD 的情況接近。\n\n所以這個無限的 Eout(h) 就轉換成有限個的 Ein‘(h) 了。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-13.png\">\n</p>\n\n### 第二步整理式子中的成長函數 mH(N)\n\n利用第一步的结果，再直觀想像一下：為了產生 Ein‘ 我們多了 N 個樣本點，所以成長函數中的 N 就變成了 2N 了。\n\n我們可以想像就樣的結果其實就是我們用了 Bound Function 這個定理，將原本無限的 Union Bound 限縮在一個 BAD overlap 的空間。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-14.png\">\n</p>\n\n### 第三步代進 Hoeffding 不等式\n\n接下來代進 Hoeffding 不等式就完成了。這樣的轉換就像是 2N 個樣本，現在抽了 N 的樣本出來，這 N 個樣本跟原來 2N 個樣本的真實情況相比發生 BAD 的機率會是多少。這樣還是一個 Hoeffding 不等式，只是就像罐子變小、誤差變小、不放回抽樣的 Hoeffding 不等式。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-15.png\">\n</p>\n\n### Vapnik-Chervonenkis（VC）bound\n\n這最終的式子就是 Vapnik-Chervonenkis（VC）bound，它將 Eout 用  Ein' 來代換，然後將假設集合專換成分類（使用 Bound Function 證明為一個多項式），然後再代進 Hoeffding 不等式。\n\n在 2D perceptrons 中，break point k=4，mH(N) 是 O(N3)，由 VC bound 可知 2D perceptrons 是可以從數據中得到學習效果。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-17.png\">\n</p>\n\n### 總結\n\n從這一講中，我們可以知道 Break Point 的出現可以大大限縮假設集合成長函數，而這個成長函數的上界是 B(N, k)，且可推導出 B(N, k) 是一個多項式，經過一些轉換與推導我們可以把無限的假設集合代換成有限的假設集合，因此可以從數學理論中得知 2D perceptrons 是可以從數據中得到學習效果的。\n\n<p style=\"text-align:center\">\n\t<img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-16.png\">\n</p>","html":"<h3 id=\"\">前言</h3>\n\n<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href=\"http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-wu-jiang-xue-xi-bi-ji/\">第五講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>\n\n<p>在第五講中我們了解了如何將 PLA 無限的假設集合透過 Dichotomy、Break Point 這樣的方式轉換成有限的集合，在第六講中我們將更進一步去推導其實這個假設集合的成長函數會是一個多項式，如此我們就可以完全相信PLA 機器學習方法的確在數學理論上是可行的了。</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">熱身回顧一下</h3>\n\n<p>我們先來回顧一下上一講的內容，在上一講我們知道了成長函數似乎跟 break point 有些關係，這一講我們將慢慢導出這樣的關係其實是一個多項式。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-1.png\">\n</p>\n\n<h3 id=\"breakpoint\">從 break point 找其他線索</h3>\n\n<p>從上一講提到的 break point 我們知道了成長函數至少會小於 2 的 N 次方，且如果 break point 在k取到了，那 k+1, k+2,... 都會是 break point。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-2.png\">\n</p>\n\n<h3 id=\"breakpoint\">由例子觀察 break point</h3>\n\n<p>這邊林軒田教授用了一連串的例子說明了如果 H 有Break Point k，那當 N 大於 k 時，mH(N) 成長函數會大大縮減（以 binary classification 這個問題來說，所有的 H 為 2 的 N 次方個）。</p>\n\n<p>既然知道 mH(N) 成長函數會大大縮減，那究竟能縮減到什麼程度？會不會是一個多項式的形式呢？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-3.png\">\n</p>\n\n<h3 id=\"boundingfunction\">Bounding Function</h3>\n\n<p>接下来，引出了 Bounding Function 的概念，Bounding Function 的意義就是為了看在 N 個樣本點的情況下，如果 Break Point 為 K，那 mH(N) 會縮減到什麼程度的一個函式。</p>\n\n<p>值得一提的是，這裡的 Bounding Function B(N, K) 只與 N 和 K 有關，與假設集合 H 無關，這樣在 Binary Classification 這個問題下，無論是 positive intervals 還是 1D perceptrons 他們的 Bounding Function 就都會是一樣的，如果我們能夠得到 Bounding Function 的結果，那就可以推廣到各個 Binary Classification 的問題。</p>\n\n<p>所以我們新的目標就是找出 B(N, K) 是不是小於等於 poly(N) 了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-4.png\">\n</p>\n\n<h3 id=\"boundingfunction\">推導 Bounding Function 數學式</h3>\n\n<p>從以下投影片的 Dichotomies 例子可以導出 Bounding Function 的一個上界，所以 B(N, k) 會是小於等於 B(N-1, k)+B(N-1, k-1)。（詳細過程可以參考影片）</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-5.png\">\n</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-6.png\">\n</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-7.png\">\n</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-8.png\">\n</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-9.png\">\n</p>\n\n<h3 id=\"boundingfunction\">Bounding Function 定理</h3>\n\n<p>Bounding Function 的上界出現了我們想要的多項式形式，這裡得出了一個定理，只要 Break Point 存在，那 mH(N) 成長函數一定能夠被一個多項式包含住，我們可以用投影片中的式子來表示 B(N, k)。</p>\n\n<p>事實上式子中的\"小於等於\"可以直接是\"等於\"，這已有證明，但不在這次課程範圍，有興趣可以去找其他資料來看。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-10.png\">\n</p>\n\n<h3 id=\"breakpoint\">找出 break point 就能算出成長函數</h3>\n\n<p>有了 Bounding Function 定理，我們就可以透過找出 break point 來算出假設集合的成長函數，比如上一講中我們說 PLA 2D perceptrons 的成長函數小於 2 的 N 次方，現在我們可以說PLA 2D perceptrons 的成長函數小於等於 B(N, 4)，因為 2D perceptrons 的 break point 出現在 4。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-11.png\">\n</p>\n\n<h3 id=\"boundfunctionhoeffding\">把 Bound Function 的定理引入 Hoeffding 不等式</h3>\n\n<p>有了上面的推導，我們知道 mH(N) 是可以在某種條件下（Break Point k）被 Bound 住。現在回到 Hoeffding 不等式的 union bound 形式，把 Bound Function 的定理概念引入這個 Hoeffding 不等式可以慢慢推導出投影片中下面的式子。</p>\n\n<p>直觀的想法我們可以把 mH(N) 的上界直接代進 Hoeffding 不等式，但理論上我們不能這樣做，因為目前 Eout(h) 是無限個，因此我們需要做一些轉換才能購買 B(N, k) 引入這個 Hoeffding 不等式。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-12.png\">\n</p>\n\n<h3 id=\"eouth\">第一步把 Eout(h) 轉換成有限個</h3>\n\n<p>第一步我們需要把 Eout(h) 轉換成有限個，這裡沒有做嚴格的證明，只個了直觀的解釋，假設還有一筆資料 D‘（數量也是 N），得到的結果為 Ein‘，那 Ein 與 Eout 發生 BAD（差距很大），應該跟 Ein 與 Ein‘ 發生 BAD 的情況接近。</p>\n\n<p>所以這個無限的 Eout(h) 就轉換成有限個的 Ein‘(h) 了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-13.png\">\n</p>\n\n<h3 id=\"mhn\">第二步整理式子中的成長函數 mH(N)</h3>\n\n<p>利用第一步的结果，再直觀想像一下：為了產生 Ein‘ 我們多了 N 個樣本點，所以成長函數中的 N 就變成了 2N 了。</p>\n\n<p>我們可以想像就樣的結果其實就是我們用了 Bound Function 這個定理，將原本無限的 Union Bound 限縮在一個 BAD overlap 的空間。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-14.png\">\n</p>\n\n<h3 id=\"hoeffding\">第三步代進 Hoeffding 不等式</h3>\n\n<p>接下來代進 Hoeffding 不等式就完成了。這樣的轉換就像是 2N 個樣本，現在抽了 N 的樣本出來，這 N 個樣本跟原來 2N 個樣本的真實情況相比發生 BAD 的機率會是多少。這樣還是一個 Hoeffding 不等式，只是就像罐子變小、誤差變小、不放回抽樣的 Hoeffding 不等式。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-15.png\">\n</p>\n\n<h3 id=\"vapnikchervonenkisvcbound\">Vapnik-Chervonenkis（VC）bound</h3>\n\n<p>這最終的式子就是 Vapnik-Chervonenkis（VC）bound，它將 Eout 用  Ein' 來代換，然後將假設集合專換成分類（使用 Bound Function 證明為一個多項式），然後再代進 Hoeffding 不等式。</p>\n\n<p>在 2D perceptrons 中，break point k=4，mH(N) 是 O(N3)，由 VC bound 可知 2D perceptrons 是可以從數據中得到學習效果。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-17.png\">\n</p>\n\n<h3 id=\"\">總結</h3>\n\n<p>從這一講中，我們可以知道 Break Point 的出現可以大大限縮假設集合成長函數，而這個成長函數的上界是 B(N, k)，且可推導出 B(N, k) 是一個多項式，經過一些轉換與推導我們可以把無限的假設集合代換成有限的假設集合，因此可以從數學理論中得知 2D perceptrons 是可以從數據中得到學習效果的。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-6-16.png\">\n</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-11-21T13:41:57.000Z","created_by":1,"updated_at":"2016-08-09T05:33:39.000Z","updated_by":1,"published_at":"2015-11-21T15:59:16.000Z","published_by":1},{"id":67,"uuid":"68b26e8a-1f13-4c72-9ad6-58024f274454","title":"林軒田教授機器學習基石 Machine Learning Foundations 第七講學習筆記","slug":"lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-qi-jiang-xue-xi-bi-ji","markdown":"### 前言\n\n本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 [第六講](http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-liu-jiang-xue-xi-bi-ji/) 的碼農們，我建議可以先回頭去讀一下再回來喔！\n\n在第六講我們可以知道 Break Point 的出現可以大大限縮假設集合成長函數，而這個成長函數的上界是 B(N, k)，且可推導出 B(N, k) 是一個多項式，經過一些轉換與推導我們可以把無限的假設集合代換成有限的假設集合，這個上界我們就稱之為 VC Bond。因此可以從數學理論中得知 2D perceptrons 是可以從數據中得到學習效果，而且也不會有 Ein 與 Eout 誤差過大的情況發生，而這一講將進一步說明 VC Bond。\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 熱身回顧一下\n\n上一講，我們從 break point 的性質慢慢推導出成長函數是一個多項式，因此我們可以保證 Ein 與 Eout 的差距在 N 資料量夠大的情況下不會因為假設集合的無限累積而差距變得很大。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-1.png\">\n</p>\n\n### 將成長函數寫得更簡潔一點\n\n目前我們知道假設集合的成長函數是 B(N, k) 這個多項式，實際算出來的值如左圖，其實我們可以直接用 N 的 k-1 次方來表示，實際算出來的值如右圖，總之 B(N, k) 會小於 N 的 k-1 次方，所以我們可以直接用 N 的 k-1 次方來表示成長函數。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-2.png\">\n</p>\n\n### VC Bond 告訴我們什麼\n\n將 VC Bond 簡化的成長函數 N 的 k-1 次方帶進上一講的霍夫丁不等式裡，式子可以簡化成下圖。這個式子告訴我們：1. 當成長函數有 break point k，也就是說假設集合很好，2. N 的資料量夠大，也就是說有足夠的好資料集，3. 如果我們有一個演算法可以找出一個 g 讓 Ein 很小，也就是說我們有好的演算法，那麼我們大概就可以說我們讓機器可以學到技巧了。\n\n（當然還是要有好運氣，因為有可能壞事情還是會發生，只是機率較低）\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-3.png\">\n</p>\n\n### VC Dimension\n\n我們將最大的非 break point 的維度稱之為 VC Dimension。所以 d＿vc 就是 k-1。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-4.png\">\n</p>\n\n### 定義好 VC Dimension 之後\n\n定義好 VC Dimension 之後，之前舉的 break point 例子就可以改寫成 vc dimension，其中 2D perceptrons 的 vc dimension 就是 3（break point 是 4）。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-5.png\">\n</p>\n\n### VC Dimension 與機器學習\n\n我們回到機器學習的概觀圖，vc dimension 的概念告訴我們，當我們有有限的 d＿vc 時，此時演算法所找出的 g 會讓 Eout 與 Ein 很接近。而且不用管用的演算法 A 是什麼、資料 D 是從哪種資料分佈產生的、也不用管 target f 到底是什麼。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-6.png\">\n</p>\n\n### 對應到 2D PLA\n\n我們把這樣的概念對應到 2D PLA，2D PLA 會從線性可分的資料集 D 中找出一個 g 讓 Ein 為 0。然後 VC Dimention 告訴我們 2D PLA 的 d＿vc 是 3，在資料集 N 夠大的情況下可以保證 Ein(g) - Eout(g) 的差距很小，因此我們就可以說 2D PLA 的 Eout(g) 也會接近 0，這也就是說我們讓機器學習到技巧了。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-7.png\">\n</p>\n\n### VC Dimention 跟 Perceptrons 的關係\n\n我們知道 1D perceptron 的 d＿vc 是 2，2D Perceptron 的 d＿vc 是 3，所以我們會猜，d-D 的 perceptrons 的 d＿vc 會不會是 d+1，理論上證明這樣的想法是對的，證明的方法有兩個步驟，第一步證明 d＿vc >= d+1，第二步證明 d＿vc <= d+1，這邊不詳述證明細節。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-8.png\">\n</p>\n\n### 模型複雜度\n\n我們將新的霍夫丁不等式做一些轉換，將右式表示成 δ（壞事情發生的機率），那 1-δ 就代表是好事情發生的機率，就就是 Ein(g) - Eout(g) <= ε，我們可以知道 d＿vc 會影響壞事情（Ein 與 Eout 差距大）發生的機率，d＿vc 越高，就代表假設集合模型複雜度越高，但也可能讓壞事情發生的機率變高。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-9.png\">\n</p>\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-10.png\">\n</p>\n\n### 模型複雜度圖示\n\n從式子中我們可以看出機器學習的模型複雜度關係如下圖，當 d＿vc 上升時，Ein 會下降，但 Ω 會上升，當 d＿vc 下降時，Ein 會上升，但 Ω 會下降，所以 Eout 要最小 d＿vc 一定是在中間，也就是我們的模型複雜度不能太高也不能太低。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-11.png\">\n</p>\n\n### 樣本複雜度\n\n從新的霍夫丁不等式我們也可以來算一下樣本複雜度，比如現在我們需要壞事情 Ein 與 Eout 的差距 > ε＝0.1 的機率 <= δ=0.1，且已知 d＿vc=3，這樣我們會需要多少個樣本呢？理論上我們會大概需要 10000 X d＿vc 個樣本點，也就是說 2D perceptrons 大概就需要 30000 個樣本點。但實務上發現只要大概 10 x d＿vc 個樣本點就足夠了。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-12.png\">\n</p>\n\n### 總結\n\n在第七講中我們定義了 VC Dimension，就是最大的 non-break point，然後在 perceptrons 這樣的模型中，d＿vc 剛好等於 d+1，物理意義上可以想成可以調整的自由度有幾個維度。引進 VC Dimension 的概念之後，我們可以了解到機器學習的模型複雜度及樣本複雜度關係。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-13.png\">\n</p>","html":"<h3 id=\"\">前言</h3>\n\n<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href=\"http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-liu-jiang-xue-xi-bi-ji/\">第六講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>\n\n<p>在第六講我們可以知道 Break Point 的出現可以大大限縮假設集合成長函數，而這個成長函數的上界是 B(N, k)，且可推導出 B(N, k) 是一個多項式，經過一些轉換與推導我們可以把無限的假設集合代換成有限的假設集合，這個上界我們就稱之為 VC Bond。因此可以從數學理論中得知 2D perceptrons 是可以從數據中得到學習效果，而且也不會有 Ein 與 Eout 誤差過大的情況發生，而這一講將進一步說明 VC Bond。</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">熱身回顧一下</h3>\n\n<p>上一講，我們從 break point 的性質慢慢推導出成長函數是一個多項式，因此我們可以保證 Ein 與 Eout 的差距在 N 資料量夠大的情況下不會因為假設集合的無限累積而差距變得很大。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-1.png\">\n</p>\n\n<h3 id=\"\">將成長函數寫得更簡潔一點</h3>\n\n<p>目前我們知道假設集合的成長函數是 B(N, k) 這個多項式，實際算出來的值如左圖，其實我們可以直接用 N 的 k-1 次方來表示，實際算出來的值如右圖，總之 B(N, k) 會小於 N 的 k-1 次方，所以我們可以直接用 N 的 k-1 次方來表示成長函數。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-2.png\">\n</p>\n\n<h3 id=\"vcbond\">VC Bond 告訴我們什麼</h3>\n\n<p>將 VC Bond 簡化的成長函數 N 的 k-1 次方帶進上一講的霍夫丁不等式裡，式子可以簡化成下圖。這個式子告訴我們：1. 當成長函數有 break point k，也就是說假設集合很好，2. N 的資料量夠大，也就是說有足夠的好資料集，3. 如果我們有一個演算法可以找出一個 g 讓 Ein 很小，也就是說我們有好的演算法，那麼我們大概就可以說我們讓機器可以學到技巧了。</p>\n\n<p>（當然還是要有好運氣，因為有可能壞事情還是會發生，只是機率較低）</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-3.png\">\n</p>\n\n<h3 id=\"vcdimension\">VC Dimension</h3>\n\n<p>我們將最大的非 break point 的維度稱之為 VC Dimension。所以 d＿vc 就是 k-1。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-4.png\">\n</p>\n\n<h3 id=\"vcdimension\">定義好 VC Dimension 之後</h3>\n\n<p>定義好 VC Dimension 之後，之前舉的 break point 例子就可以改寫成 vc dimension，其中 2D perceptrons 的 vc dimension 就是 3（break point 是 4）。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-5.png\">\n</p>\n\n<h3 id=\"vcdimension\">VC Dimension 與機器學習</h3>\n\n<p>我們回到機器學習的概觀圖，vc dimension 的概念告訴我們，當我們有有限的 d＿vc 時，此時演算法所找出的 g 會讓 Eout 與 Ein 很接近。而且不用管用的演算法 A 是什麼、資料 D 是從哪種資料分佈產生的、也不用管 target f 到底是什麼。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-6.png\">\n</p>\n\n<h3 id=\"2dpla\">對應到 2D PLA</h3>\n\n<p>我們把這樣的概念對應到 2D PLA，2D PLA 會從線性可分的資料集 D 中找出一個 g 讓 Ein 為 0。然後 VC Dimention 告訴我們 2D PLA 的 d＿vc 是 3，在資料集 N 夠大的情況下可以保證 Ein(g) - Eout(g) 的差距很小，因此我們就可以說 2D PLA 的 Eout(g) 也會接近 0，這也就是說我們讓機器學習到技巧了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-7.png\">\n</p>\n\n<h3 id=\"vcdimentionperceptrons\">VC Dimention 跟 Perceptrons 的關係</h3>\n\n<p>我們知道 1D perceptron 的 d＿vc 是 2，2D Perceptron 的 d＿vc 是 3，所以我們會猜，d-D 的 perceptrons 的 d＿vc 會不會是 d+1，理論上證明這樣的想法是對的，證明的方法有兩個步驟，第一步證明 d＿vc >= d+1，第二步證明 d＿vc &lt;= d+1，這邊不詳述證明細節。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-8.png\">\n</p>\n\n<h3 id=\"\">模型複雜度</h3>\n\n<p>我們將新的霍夫丁不等式做一些轉換，將右式表示成 δ（壞事情發生的機率），那 1-δ 就代表是好事情發生的機率，就就是 Ein(g) - Eout(g) &lt;= ε，我們可以知道 d＿vc 會影響壞事情（Ein 與 Eout 差距大）發生的機率，d＿vc 越高，就代表假設集合模型複雜度越高，但也可能讓壞事情發生的機率變高。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-9.png\">\n</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-10.png\">\n</p>\n\n<h3 id=\"\">模型複雜度圖示</h3>\n\n<p>從式子中我們可以看出機器學習的模型複雜度關係如下圖，當 d＿vc 上升時，Ein 會下降，但 Ω 會上升，當 d＿vc 下降時，Ein 會上升，但 Ω 會下降，所以 Eout 要最小 d＿vc 一定是在中間，也就是我們的模型複雜度不能太高也不能太低。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-11.png\">\n</p>\n\n<h3 id=\"\">樣本複雜度</h3>\n\n<p>從新的霍夫丁不等式我們也可以來算一下樣本複雜度，比如現在我們需要壞事情 Ein 與 Eout 的差距 > ε＝0.1 的機率 &lt;= δ=0.1，且已知 d＿vc=3，這樣我們會需要多少個樣本呢？理論上我們會大概需要 10000 X d＿vc 個樣本點，也就是說 2D perceptrons 大概就需要 30000 個樣本點。但實務上發現只要大概 10 x d＿vc 個樣本點就足夠了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-12.png\">\n</p>\n\n<h3 id=\"\">總結</h3>\n\n<p>在第七講中我們定義了 VC Dimension，就是最大的 non-break point，然後在 perceptrons 這樣的模型中，d＿vc 剛好等於 d+1，物理意義上可以想成可以調整的自由度有幾個維度。引進 VC Dimension 的概念之後，我們可以了解到機器學習的模型複雜度及樣本複雜度關係。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-7-13.png\">\n</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-12-09T18:10:38.000Z","created_by":1,"updated_at":"2016-08-09T05:34:12.000Z","updated_by":1,"published_at":"2015-12-09T19:31:24.000Z","published_by":1},{"id":68,"uuid":"71e12ea7-49c9-49ef-ba94-856f3e6d2f43","title":"林軒田教授機器學習基石 Machine Learning Foundations 第八講學習筆記","slug":"lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-ba-jiang-xue-xi-bi-ji","markdown":"### 前言\n\n本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 [第七講](http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-qi-jiang-xue-xi-bi-ji/) 的碼農們，我建議可以先回頭去讀一下再回來喔！\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 熱身回顧一下\n\n在第七講中我們定義了 VC Dimension，就是最大的 non-break point，當 d_vc 是有限的，且資料 N 夠大，Ein 很小的時候，理論上機器學習是可以達成目標的。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-1.png\">\n</p>\n\n### 重溫機器學習流程圖\n\n重溫機器學習流程圖，大致的理論我們都已經完備了，但這時又會想，如果資料來源有雜訊（noise）又會如何呢？\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-2.png\">\n</p>\n\n### 雜訊（Noise）是什麼\n\n雜訊是什麼呢？以之前的銀行發卡的例子來說明，比如該發卡未發卡、不該發卡卻發了卡、或是一開始收集的基本資料就是錯的，這些就會是我們搜集到資料時的雜訊，在有雜訊的情況下 VC bond 還會正常運作嗎？\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-3.png\">\n</p>\n\n### 用彈珠顏色會改變來代表雜訊來看 VC bound\n\n之前在推導 VC bound 時是用彈珠來說明，我們可以用不固定顏色的彈珠來代表雜訊推導 VC bound（以 pocket algorithm 可以想成是 o 和 x 在同一線上，所以無法確定 o 或 x，這就是雜訊），也就是資料來源會多了一個 y ~ P(y|x)這個條件，從之前的理論推導中，我們了解了訓練資料跟測試資料都來自同一個資料分佈的話，那 VC bound 就會成立。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-4.png\">\n</p>\n\n### 新的機器學習流程圖\n\n所以新的機器學習流程圖就可以容忍雜訊。也因此可以容忍雜訊的 pocket algorithm 就有理論基礎了。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-5.png\">\n</p>\n\n### 錯誤衡量\n\n接下來探討如何衡量 g 跟 f 是否很接近，我們使用 E_out(g) 來衡量，這個衡量方式有三個特點：1. 使用 out of sample 來衡量 2. 用 point-wise 的方式一個點一個點衡量 3. 如果是二元分類的問題就會使用 0/1 error 來衡量。但其實也不一定要遵照這三個特點，有很多不同的方法被提出來，只是我們同常還是會這樣來做錯誤衡量。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-6.png\">\n</p>\n\n### 兩個重要的 Pointwise 錯誤衡量\n\n有兩個重要的 Pointwise 錯誤衡量方式可以記起來，一個是 0/1 錯誤，這個可以用在分類上；一個是平方錯誤，這個通常用在迴歸問題上，不同的錯誤衡量方式會怎麼影響機器學習呢？\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-8.png\">\n</p>\n\n### Ideal Mini-Target\n\n從下面可以例子，我們可以從資料來源 P(y|x) 及 error function 了解它們如何互相影響，在 0/1 錯誤的情況下，選擇 y=0.2 會得到最小的 error 0.3，但在平方錯誤的情況下，選擇 y=1.9 才會得到最小的 error 0.29，機器學習的過程也會在 P(y|x) 及 error function 的關係去找出 ideal mini-target f(x)。 \n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-9.png\">\n</p>\n\n### 再次更新機器學習流程圖\n\n所以選擇用什麼 error function 也會影響機器學習的結果。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-10.png\">\n</p>\n\n### 錯誤衡量的選擇\n\n錯誤衡量的選擇也還會有其他考量，具體了解一下錯誤，在 0/1 error 這種 error function，會有 false reject 及 false accept 這兩種錯誤，我們會同等的看待這兩種錯誤，只要出錯，那 error 就是 1。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-11.png\">\n</p>\n\n### 錯誤有權重：超級市場指紋辨識\n\n但實際的世界可能會有這種情況，錯誤是有權重的，我們可以用超級市場指紋辨識給優惠這種例子來說明，如老顧客有優惠，新顧客沒有優惠，如果指紋辨識 false reject 老顧客沒優惠，那就會失去這位老顧客，影響較大，我們可以給這個錯誤較大的權重。但新顧客 false accept 就沒什麼大不了，虧點小錢而已，錯誤的權重應該比較小。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-12.png\">\n</p>\n\n### 錯誤有權重：CIA 指紋辨識\n\n但如果是在 CIA 機密文件的指紋辨識上，那就反過來了，有權限的人 false reject 沒有什麼大不了，但沒權限的人 false accept 那就會出大問題。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-13.png\">\n</p>\n\n### 最後更新機器學習流程圖\n\n將這樣的概念套用到機器學習流程圖上面，其實就是在演算法 A 這邊對權重的情況作一些調整。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-14.png\">\n</p>\n\n### 將錯誤權重套用到演算法\n\n如何將錯誤權重套用到演算法呢？我們用 CIA 指紋辨識這個例子來說明，我們可以直接將 false accept 的錯誤乘上 1000 倍，但這其實只做了一半。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-15.png\">\n</p>\n\n### 更嚴謹一點\n\n因為資料量沒有變，卻讓 error 直接乘上 1000 倍並不嚴謹，所以我們可以用下面這個方式來想，假設沒權限（false accept 錯誤）的資料複製了 1000 筆相同的資料下去訓練，這樣 false accept 乘上 1000 倍就合理，但實務上我們不會真的複製 1000 筆資料進去，而是在演算法上面多去使用沒權限（false accept 錯誤）的資料做運算的機率大了 1000 倍，這樣就很像虛擬的放大資料。用這樣的方式去調整演算法才比較嚴謹。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-17.png\">\n</p>\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-18.png\">\n</p>\n\n### 結語\n\n在這一講中，我們了解了有雜訊的情況下，機器學習理論上還是可以運作的，並且介紹了錯誤衡量（error measure）的方式也會影響演算法挑選的結果，如果 error measure 有權重的話，那調整演算法時也要嚴謹些，直接在 error function 乘上權重並不夠嚴謹。這一講完備之後，機器學習的基礎理論已經告一段落了，下一講將開始介紹其他機器學習演算法。（我們目前只會 PLA 及 Pocket PLA）\n","html":"<h3 id=\"\">前言</h3>\n\n<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href=\"http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-qi-jiang-xue-xi-bi-ji/\">第七講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">熱身回顧一下</h3>\n\n<p>在第七講中我們定義了 VC Dimension，就是最大的 non-break point，當 d_vc 是有限的，且資料 N 夠大，Ein 很小的時候，理論上機器學習是可以達成目標的。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-1.png\">\n</p>\n\n<h3 id=\"\">重溫機器學習流程圖</h3>\n\n<p>重溫機器學習流程圖，大致的理論我們都已經完備了，但這時又會想，如果資料來源有雜訊（noise）又會如何呢？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-2.png\">\n</p>\n\n<h3 id=\"noise\">雜訊（Noise）是什麼</h3>\n\n<p>雜訊是什麼呢？以之前的銀行發卡的例子來說明，比如該發卡未發卡、不該發卡卻發了卡、或是一開始收集的基本資料就是錯的，這些就會是我們搜集到資料時的雜訊，在有雜訊的情況下 VC bond 還會正常運作嗎？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-3.png\">\n</p>\n\n<h3 id=\"vcbound\">用彈珠顏色會改變來代表雜訊來看 VC bound</h3>\n\n<p>之前在推導 VC bound 時是用彈珠來說明，我們可以用不固定顏色的彈珠來代表雜訊推導 VC bound（以 pocket algorithm 可以想成是 o 和 x 在同一線上，所以無法確定 o 或 x，這就是雜訊），也就是資料來源會多了一個 y ~ P(y|x)這個條件，從之前的理論推導中，我們了解了訓練資料跟測試資料都來自同一個資料分佈的話，那 VC bound 就會成立。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-4.png\">\n</p>\n\n<h3 id=\"\">新的機器學習流程圖</h3>\n\n<p>所以新的機器學習流程圖就可以容忍雜訊。也因此可以容忍雜訊的 pocket algorithm 就有理論基礎了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-5.png\">\n</p>\n\n<h3 id=\"\">錯誤衡量</h3>\n\n<p>接下來探討如何衡量 g 跟 f 是否很接近，我們使用 E_out(g) 來衡量，這個衡量方式有三個特點：1. 使用 out of sample 來衡量 2. 用 point-wise 的方式一個點一個點衡量 3. 如果是二元分類的問題就會使用 0/1 error 來衡量。但其實也不一定要遵照這三個特點，有很多不同的方法被提出來，只是我們同常還是會這樣來做錯誤衡量。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-6.png\">\n</p>\n\n<h3 id=\"pointwise\">兩個重要的 Pointwise 錯誤衡量</h3>\n\n<p>有兩個重要的 Pointwise 錯誤衡量方式可以記起來，一個是 0/1 錯誤，這個可以用在分類上；一個是平方錯誤，這個通常用在迴歸問題上，不同的錯誤衡量方式會怎麼影響機器學習呢？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-8.png\">\n</p>\n\n<h3 id=\"idealminitarget\">Ideal Mini-Target</h3>\n\n<p>從下面可以例子，我們可以從資料來源 P(y|x) 及 error function 了解它們如何互相影響，在 0/1 錯誤的情況下，選擇 y=0.2 會得到最小的 error 0.3，但在平方錯誤的情況下，選擇 y=1.9 才會得到最小的 error 0.29，機器學習的過程也會在 P(y|x) 及 error function 的關係去找出 ideal mini-target f(x)。 </p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-9.png\">\n</p>\n\n<h3 id=\"\">再次更新機器學習流程圖</h3>\n\n<p>所以選擇用什麼 error function 也會影響機器學習的結果。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-10.png\">\n</p>\n\n<h3 id=\"\">錯誤衡量的選擇</h3>\n\n<p>錯誤衡量的選擇也還會有其他考量，具體了解一下錯誤，在 0/1 error 這種 error function，會有 false reject 及 false accept 這兩種錯誤，我們會同等的看待這兩種錯誤，只要出錯，那 error 就是 1。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-11.png\">\n</p>\n\n<h3 id=\"\">錯誤有權重：超級市場指紋辨識</h3>\n\n<p>但實際的世界可能會有這種情況，錯誤是有權重的，我們可以用超級市場指紋辨識給優惠這種例子來說明，如老顧客有優惠，新顧客沒有優惠，如果指紋辨識 false reject 老顧客沒優惠，那就會失去這位老顧客，影響較大，我們可以給這個錯誤較大的權重。但新顧客 false accept 就沒什麼大不了，虧點小錢而已，錯誤的權重應該比較小。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-12.png\">\n</p>\n\n<h3 id=\"cia\">錯誤有權重：CIA 指紋辨識</h3>\n\n<p>但如果是在 CIA 機密文件的指紋辨識上，那就反過來了，有權限的人 false reject 沒有什麼大不了，但沒權限的人 false accept 那就會出大問題。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-13.png\">\n</p>\n\n<h3 id=\"\">最後更新機器學習流程圖</h3>\n\n<p>將這樣的概念套用到機器學習流程圖上面，其實就是在演算法 A 這邊對權重的情況作一些調整。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-14.png\">\n</p>\n\n<h3 id=\"\">將錯誤權重套用到演算法</h3>\n\n<p>如何將錯誤權重套用到演算法呢？我們用 CIA 指紋辨識這個例子來說明，我們可以直接將 false accept 的錯誤乘上 1000 倍，但這其實只做了一半。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-15.png\">\n</p>\n\n<h3 id=\"\">更嚴謹一點</h3>\n\n<p>因為資料量沒有變，卻讓 error 直接乘上 1000 倍並不嚴謹，所以我們可以用下面這個方式來想，假設沒權限（false accept 錯誤）的資料複製了 1000 筆相同的資料下去訓練，這樣 false accept 乘上 1000 倍就合理，但實務上我們不會真的複製 1000 筆資料進去，而是在演算法上面多去使用沒權限（false accept 錯誤）的資料做運算的機率大了 1000 倍，這樣就很像虛擬的放大資料。用這樣的方式去調整演算法才比較嚴謹。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-17.png\">\n</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-8-18.png\">\n</p>\n\n<h3 id=\"\">結語</h3>\n\n<p>在這一講中，我們了解了有雜訊的情況下，機器學習理論上還是可以運作的，並且介紹了錯誤衡量（error measure）的方式也會影響演算法挑選的結果，如果 error measure 有權重的話，那調整演算法時也要嚴謹些，直接在 error function 乘上權重並不夠嚴謹。這一講完備之後，機器學習的基礎理論已經告一段落了，下一講將開始介紹其他機器學習演算法。（我們目前只會 PLA 及 Pocket PLA）</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2015-12-23T12:02:57.000Z","created_by":1,"updated_at":"2016-08-09T05:34:47.000Z","updated_by":1,"published_at":"2015-12-23T13:08:10.000Z","published_by":1},{"id":69,"uuid":"6381ca15-caae-432f-bb56-3cc55b5d4c9f","title":"林軒田教授機器學習基石 Machine Learning Foundations 第九講學習筆記","slug":"lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-jiu-jiang-xue-xi-bi-ji","markdown":"### 前言\n\n本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 [第八講](http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-ba-jiang-xue-xi-bi-ji/) 的碼農們，我建議可以先回頭去讀一下再回來喔！\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 熱身回顧一下\n\n在第八講中我們機器學習流程圖裡加入了 error function 及 noise 的概念，並了解在這樣的情況下機器學習還是可行的。前面花了很大的篇幅在說機器為何可以學習，接下來是要說明機器是怎麼學習的。本篇以眾所皆知的線性迴歸為例，從方程式的形式、誤差的衡量方式、如何最小化 Ein，讓我們對線性迴歸在機器學習上的應用有些理解。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-1.png\">\n</p>\n\n### 信用卡額度問題\n\n回到信用卡這個例子，假設我們現在需要的是判定要發多少信用額度給申請者，這種問題的答案就從發不發卡變成了一個實數，這就是線性迴歸的問題。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-2.png\">\n</p>\n\n### 線性迴歸\n\n將上述問題寫成線性迴歸式就如下圖，其實與 perceptron 很像，但不用再取正負號。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-3.png\">\n</p>\n\n### 圖解線性迴歸\n\n圖解線性迴歸問題就會如下圖所示，找一條線或超平面來讓所有的資料點與之的差距最小。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-4.png\">\n</p>\n\n### 線性迴歸的錯誤衡量\n\n線性迴歸的錯誤衡量一般是使用平方錯誤，所以在計算 Ein 的過程是算出平均平方錯誤最小的值，而 Eout 的差距就可以用平方錯誤來計算效果。最小的 Ein 如何計算呢？\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-5.png\">\n</p>\n\n### 整理成矩陣的形式\n\n我們將 Ein 的式子整理成矩陣的形式，推導過程如下。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-6.png\">\n</p>\n\n### 最小的 Ein\n\n整理成這種式子的 Ein 理論上已知是個連續、處處可微的凸函數，所以計算最小的 Ein 就是去解每一個維度進行偏微分為 0 所得到的值。他的物理意義就是在這個凸函數中這個點在各個維度都無法再下滑。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-7.png\">\n</p>\n\n### 最小的 Ein 就是計算 Ein 的梯度\n\n這就是在計算 Ein 的梯度，我們將原本的式子展開後，並對這個式子做偏微分。對矩陣的偏微分的理解我們可以從 1 個維度慢慢推廣，如此就可知 XTXw - XTy = 0 時可以得到最佳解。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-8.png\">\n</p>\n\n### 線性迴歸最佳解\n\n有上所得到的結果，如果 X^TX 是有反矩陣的話，那就可以很容易的算出 (X^TX)-X^Ty 是我們得到的最佳解，其中 (X^TX)-X^T 又稱為 pseudo-inverse，但如果 X^TX 是 singular，那就無法這樣算出一個最佳的 pseudo-inverse，但還是有辦法得出一個 pseudo-inverse，記為 X+，通常我們會用內建的數學方法得出 pseudo-inverse。 \n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-9.png\">\n</p>\n\n### 線性迴歸機器學習演算法\n\n如此線性迴歸機器學習演算法就會是這樣，第一步，特徵資料組成一個矩陣 X，答案組成一個向量 y，第二步，計算 X 的 pseudo-inverse，第三步，將 X pseudo-inverse 與 y 做內積，如此就可以得到最佳解 W。 \n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-10.png\">\n</p>\n\n### 線性迴歸到底是不是機器學習演算法\n\n由於好像是用一個式子就算出答案，所以有人會懷疑線性迴歸到底是不是機器學習演算法，線性迴歸在本質上的確跟遵循機器學習流程，在計算 psedo-inverse 的過程其實就有學習調整的過程，只是我們直接用別人寫好的函式沒辦法看出來，另外，機器學習其實就是想要做到 Eout 好，如果 Eout 好那就是一個機器學習演算法。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-11.png\">\n</p>\n\n### 分析線性迴歸的 Ein 與 Eout\n\n用一些理論來分析線性迴歸的 Ein 與 Eout，Ein 平均可以推導到 noise level x (1-(d+1)/N)，第一步推導如下圖，我們又稱 XX+ 為 hat matrix H，因為它讓真實的答案 y 變成了預測的答案 y hat，所以就叫 hat matrix H。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-12.png\">\n</p>\n\n### Hat Matrix H 的物理意義\n\ny hat 物理意義就是 X 的線性組合，原本有很多個 y hat，可以組成一個線性組合空間，迴歸分析就是在算 y - yhat 最小的，Hat Matrix H 就是把 y 投影到 yhat，然後 I-H 可以將 y 線性轉換到 y - yhat，代表真實答案與預測答案的差距。其中數學上有個性質 trace(I-H) = N-(d+1)，這個沒有證明，只有用自由度來說明。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-13.png\">\n</p>\n\n### 用這樣的物理意義來證明 noise level x (1-(d+1)/N)\n\n我們用這樣的物理意義來證明 noise level x (1-(d+1)/N)，我們可以想成 y 是從 f(x) 加上一些 noise 得出來的，這個 noise 被 I-H 線性轉換到 y-yhat，如就可以計算出 Ein 平均會是 noise level x (1-(d+1)/N)。\n\n有趣的是 Eout 會是 noise level x (1＋(d+1)/N)，但證明很複雜。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-14.png\">\n</p>\n\n### 學習曲線\n\nEin 跟 Eout 都會隨著 N 越大而收斂，而且 Ein 及 Eout 的差距會被 bound 住，所以也有 VC bound 的性質，因此迴歸分析的確可以達成機器學習效果。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-15.png\">\n</p>\n\n### 線性分類跟線性迴歸\n\n分類 y 是 +1 或 -1，迴歸是實數；分類的假設集合要取 sign，迴歸不用；分類是 0/1 錯誤，迴歸是平方錯誤；分類是個 NP-hard 的問題，不容易算出最佳解，但迴歸可以容易算出最佳解，我們可以把分類問題轉成用迴歸來解嗎？\n\n直覺來想，我們可以算出迴歸解之後，再取 sign，那就變成分類模型了，理論上可以嗎？\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-16.png\">\n</p>\n\n### 觀察分類與迴歸的錯誤衡量\n\n觀察分類與迴歸的錯誤衡量我們可以發現迴歸的平方錯誤曲線是壓在分類的0/1錯誤曲線上的，也就是說平方錯誤一定會比較大。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-17.png\">\n</p>\n\n### 使用 VC bound 理論將迴歸套用到二元分類\n\n使用 VC bound 理論將迴歸套用到二元分類，可以知道迴歸錯誤會是上界，如果迴歸可以得到 Ein 小，那 Eout 就會小，因此可以將迴歸套用在二元分類上，這樣會比算 PLA 有效率，雖然效果不一定比較好。\n\n我們也可以將迴歸用在先期計算，先計算出最好的 PLA 初始線，然後再進行 PLA 演算法，如此就可以增加 PLA 的效能。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-18.png\">\n</p>\n\n### 總結\n\n當我們要機器回答實數解的時候，那就是線性迴歸問題。而線性迴歸演算法可以用 pseudo-inverse 很快地算出來，Ein 與 Eout 的差距會隨著 N 越大而收斂，而且在理論上我們可以用線性迴歸來解分類問題。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-19.png\">\n</p>\n","html":"<h3 id=\"\">前言</h3>\n\n<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href=\"http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-ba-jiang-xue-xi-bi-ji/\">第八講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">熱身回顧一下</h3>\n\n<p>在第八講中我們機器學習流程圖裡加入了 error function 及 noise 的概念，並了解在這樣的情況下機器學習還是可行的。前面花了很大的篇幅在說機器為何可以學習，接下來是要說明機器是怎麼學習的。本篇以眾所皆知的線性迴歸為例，從方程式的形式、誤差的衡量方式、如何最小化 Ein，讓我們對線性迴歸在機器學習上的應用有些理解。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-1.png\">\n</p>\n\n<h3 id=\"\">信用卡額度問題</h3>\n\n<p>回到信用卡這個例子，假設我們現在需要的是判定要發多少信用額度給申請者，這種問題的答案就從發不發卡變成了一個實數，這就是線性迴歸的問題。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-2.png\">\n</p>\n\n<h3 id=\"\">線性迴歸</h3>\n\n<p>將上述問題寫成線性迴歸式就如下圖，其實與 perceptron 很像，但不用再取正負號。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-3.png\">\n</p>\n\n<h3 id=\"\">圖解線性迴歸</h3>\n\n<p>圖解線性迴歸問題就會如下圖所示，找一條線或超平面來讓所有的資料點與之的差距最小。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-4.png\">\n</p>\n\n<h3 id=\"\">線性迴歸的錯誤衡量</h3>\n\n<p>線性迴歸的錯誤衡量一般是使用平方錯誤，所以在計算 Ein 的過程是算出平均平方錯誤最小的值，而 Eout 的差距就可以用平方錯誤來計算效果。最小的 Ein 如何計算呢？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-5.png\">\n</p>\n\n<h3 id=\"\">整理成矩陣的形式</h3>\n\n<p>我們將 Ein 的式子整理成矩陣的形式，推導過程如下。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-6.png\">\n</p>\n\n<h3 id=\"ein\">最小的 Ein</h3>\n\n<p>整理成這種式子的 Ein 理論上已知是個連續、處處可微的凸函數，所以計算最小的 Ein 就是去解每一個維度進行偏微分為 0 所得到的值。他的物理意義就是在這個凸函數中這個點在各個維度都無法再下滑。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-7.png\">\n</p>\n\n<h3 id=\"einein\">最小的 Ein 就是計算 Ein 的梯度</h3>\n\n<p>這就是在計算 Ein 的梯度，我們將原本的式子展開後，並對這個式子做偏微分。對矩陣的偏微分的理解我們可以從 1 個維度慢慢推廣，如此就可知 XTXw - XTy = 0 時可以得到最佳解。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-8.png\">\n</p>\n\n<h3 id=\"\">線性迴歸最佳解</h3>\n\n<p>有上所得到的結果，如果 X^TX 是有反矩陣的話，那就可以很容易的算出 (X^TX)-X^Ty 是我們得到的最佳解，其中 (X^TX)-X^T 又稱為 pseudo-inverse，但如果 X^TX 是 singular，那就無法這樣算出一個最佳的 pseudo-inverse，但還是有辦法得出一個 pseudo-inverse，記為 X+，通常我們會用內建的數學方法得出 pseudo-inverse。 </p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-9.png\">\n</p>\n\n<h3 id=\"\">線性迴歸機器學習演算法</h3>\n\n<p>如此線性迴歸機器學習演算法就會是這樣，第一步，特徵資料組成一個矩陣 X，答案組成一個向量 y，第二步，計算 X 的 pseudo-inverse，第三步，將 X pseudo-inverse 與 y 做內積，如此就可以得到最佳解 W。 </p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-10.png\">\n</p>\n\n<h3 id=\"\">線性迴歸到底是不是機器學習演算法</h3>\n\n<p>由於好像是用一個式子就算出答案，所以有人會懷疑線性迴歸到底是不是機器學習演算法，線性迴歸在本質上的確跟遵循機器學習流程，在計算 psedo-inverse 的過程其實就有學習調整的過程，只是我們直接用別人寫好的函式沒辦法看出來，另外，機器學習其實就是想要做到 Eout 好，如果 Eout 好那就是一個機器學習演算法。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-11.png\">\n</p>\n\n<h3 id=\"eineout\">分析線性迴歸的 Ein 與 Eout</h3>\n\n<p>用一些理論來分析線性迴歸的 Ein 與 Eout，Ein 平均可以推導到 noise level x (1-(d+1)/N)，第一步推導如下圖，我們又稱 XX+ 為 hat matrix H，因為它讓真實的答案 y 變成了預測的答案 y hat，所以就叫 hat matrix H。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-12.png\">\n</p>\n\n<h3 id=\"hatmatrixh\">Hat Matrix H 的物理意義</h3>\n\n<p>y hat 物理意義就是 X 的線性組合，原本有很多個 y hat，可以組成一個線性組合空間，迴歸分析就是在算 y - yhat 最小的，Hat Matrix H 就是把 y 投影到 yhat，然後 I-H 可以將 y 線性轉換到 y - yhat，代表真實答案與預測答案的差距。其中數學上有個性質 trace(I-H) = N-(d+1)，這個沒有證明，只有用自由度來說明。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-13.png\">\n</p>\n\n<h3 id=\"noiselevelx1d1n\">用這樣的物理意義來證明 noise level x (1-(d+1)/N)</h3>\n\n<p>我們用這樣的物理意義來證明 noise level x (1-(d+1)/N)，我們可以想成 y 是從 f(x) 加上一些 noise 得出來的，這個 noise 被 I-H 線性轉換到 y-yhat，如就可以計算出 Ein 平均會是 noise level x (1-(d+1)/N)。</p>\n\n<p>有趣的是 Eout 會是 noise level x (1＋(d+1)/N)，但證明很複雜。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-14.png\">\n</p>\n\n<h3 id=\"\">學習曲線</h3>\n\n<p>Ein 跟 Eout 都會隨著 N 越大而收斂，而且 Ein 及 Eout 的差距會被 bound 住，所以也有 VC bound 的性質，因此迴歸分析的確可以達成機器學習效果。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-15.png\">\n</p>\n\n<h3 id=\"\">線性分類跟線性迴歸</h3>\n\n<p>分類 y 是 +1 或 -1，迴歸是實數；分類的假設集合要取 sign，迴歸不用；分類是 0/1 錯誤，迴歸是平方錯誤；分類是個 NP-hard 的問題，不容易算出最佳解，但迴歸可以容易算出最佳解，我們可以把分類問題轉成用迴歸來解嗎？</p>\n\n<p>直覺來想，我們可以算出迴歸解之後，再取 sign，那就變成分類模型了，理論上可以嗎？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-16.png\">\n</p>\n\n<h3 id=\"\">觀察分類與迴歸的錯誤衡量</h3>\n\n<p>觀察分類與迴歸的錯誤衡量我們可以發現迴歸的平方錯誤曲線是壓在分類的0/1錯誤曲線上的，也就是說平方錯誤一定會比較大。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-17.png\">\n</p>\n\n<h3 id=\"vcbound\">使用 VC bound 理論將迴歸套用到二元分類</h3>\n\n<p>使用 VC bound 理論將迴歸套用到二元分類，可以知道迴歸錯誤會是上界，如果迴歸可以得到 Ein 小，那 Eout 就會小，因此可以將迴歸套用在二元分類上，這樣會比算 PLA 有效率，雖然效果不一定比較好。</p>\n\n<p>我們也可以將迴歸用在先期計算，先計算出最好的 PLA 初始線，然後再進行 PLA 演算法，如此就可以增加 PLA 的效能。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-18.png\">\n</p>\n\n<h3 id=\"\">總結</h3>\n\n<p>當我們要機器回答實數解的時候，那就是線性迴歸問題。而線性迴歸演算法可以用 pseudo-inverse 很快地算出來，Ein 與 Eout 的差距會隨著 N 越大而收斂，而且在理論上我們可以用線性迴歸來解分類問題。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-9-19.png\">\n</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-01-06T11:52:22.000Z","created_by":1,"updated_at":"2016-08-09T05:35:09.000Z","updated_by":1,"published_at":"2016-01-06T13:29:31.000Z","published_by":1},{"id":70,"uuid":"7db0a796-1937-4b61-acd1-6fe9ef55ee8e","title":"林軒田教授機器學習基石 Machine Learning Foundations 第十講學習筆記","slug":"lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-jiang-xue-xi-bi-ji","markdown":"### 前言\n\n本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 [第九講](http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-jiu-jiang-xue-xi-bi-ji/) 的碼農們，我建議可以先回頭去讀一下再回來喔！\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 熱身回顧一下\n\n在上一講中我們了解了如何使用線性迴歸的方法來讓機器學習回答實數解的問題，第十講我們將介紹使用 Logistic Regression 來讓機器學習回答可能機率的問題。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-1.png\">\n</p>\n\n### 可能性問題\n\n我們用心臟病發生的機率這樣的問題來做為例子，假設我們有一組病患的數據，我們需要預測他們在一段時間之後患上心臟病的「可能性」為何。我們拿到的歷史數據跟之前的二元分類問題是一樣的，我們只知道病患之後有或者沒有發生心臟病，從這樣的數據我們用之前學過的二元分類學習方法可以讓機器學習到如何預測病患會不會發病，但現在我們想讓機器回答的是可能性，不像二元分類那麼硬，所以又稱軟性二元分類，即 f(x) = P(+1|x)，取值的範圍區間在 [0, 1]。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-2.png\">\n</p>\n\n### 軟性二元分類\n\n我們的訓練數據跟二元分類是完全一樣的，x 是病人的特徵值，y 是 +1（患心臟病）或 -1（沒有患心臟病），沒有告訴我們有關「患病機率」的訊息。回想在二元分類中，我們使用 w\\*x 得到一個「分數」之後，再利用取號運算 sign 來預測 y 是 +1 或是 -1。對於目前這個問題，如果我們能夠將這個「分數」映射到 [0, 1] 區間，似乎就可以解這個問題了～\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-3.png\">\n</p>\n\n### Logistic 假設\n\n我們把上面的想法寫成式子，就是 h(x) = theta(wx)，映射分數到 [0, 1] 的 function 就叫做 logistic function，用 theta 來表示\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-4.png\">\n</p>\n\n### Logistic 函式\n\nLogistic regression 選擇使用了 sigmoid 來將值域映射到 [0, 1]，sigmoid 是 f(s) = 1 / (1 + exp(-s))，f(x) 單調遞增，0 <= f(x) <= 1。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-5.png\">\n</p>\n\n### 如何定義 Logistic Regression 的 Error Function\n\n那麼對於 Logistic 假設，我們要如何定義 Logistic Regression 的 Error Function E_in(h)？回顧一下之前學過的方法，Linear Classificaiton 是使用 0/1 Error，Linear Regression 是使用 Squared Error。\n\nLogistic Regression 的目標是找到 h(x) 接近 f(x)＝P(+1|x)，也就是說當 y=+1 時，P(y|x) = f(x)，當 y=-1 時，P(y|x) = 1-f(x)。我們要讓 Error 小，就是要讓 h(x) 與 f(x) 接近。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-6.png\">\n</p>\n\n### 使用 Likelihood 的概念\n\n我們知道資料集 D 是由 f(x) 產生的，我們可以用這樣的思路去想，f(x) 跟 h(x) 生成目前看到的資料集 D 的機率是多少，用這種 Likelihood 的概念，我們只要在訓練過程讓 h(x) 跟 f(x) 產生資料集 D 的機率接近就可以。\n\n不過我們並不知道 f(x)，但我們可以知道 f(x) 產生資料集 D 的機率很高，因為確實就是由 f(x) 產生的。\n\n簡而言之就是產生目前所看到的資料集是一個我們不知道的 f(x)，f(x) 可能會產生出很多種資料集，我們看到的只是其中之一，但 f(x) 是我們無法知道的，我們可以去算 g(x)，從眾多的 g(x) 去算出那一個最可能產生我們目前看到的資料集，這就是去算 max likelihood，之後我們就可以說 g(x) 跟 f(x) 很像～\n\n所以我們只要計算讓 h(x) Likelihood 越高越好。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-7.png\">\n</p>\n\n### 將 Likelihood 套用到 Logistic Regression\n\n所以現在問題轉換成了計算 h(x) 越高越好了，得到最高 likelihood 機率值的就是我們所要的 g(x)。\n\n式子就是 g = argmax likelihood(h)\n\nlikelihood(h) 的式子可以描述成 P(x1)h(x1) x P(x2)(1-h(x2)) ... P(xn)h(1-h(xn))\n\n又因為 sigmoid 性質 1-h(x) = h(-x)，所以 likelihood(h) 的式子可以描述成 P(x1)h(+x1) x P(x2)(-h(x2)) ... P(xn)h(-h(xn))\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-8.png\">\n</p>\n\n### 將式子做一些轉換\n\nlikelihood(h)＝P(x1)h(+x1) x P(x2)(-h(x2)) ... P(xn)h(-h(xn))，我們要計算的是 max likelihood，所以 \tmax likelihood(h) 正比於 h(score) 連乘，正比於 theta(yn)theta(wxn) 連乘。\n\n我們取上 ln，不會影響計算結果，所以取 ln 做後續的推導。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-10.png\">\n</p>\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-11.png\">\n</p>\n\n### Cross-Entropy Error\n\n如果 max Likelihood 轉換成 Error，作為 Logistic Regression 的 Error Function，就會是求 min 的 -ln theta(ynwxn) 連加，代進 sigmoid theta，就會得到 ln(1+exp(-ywx))，這又稱為 Cross-Entropy Error \n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-12.png\">\n</p>\n\n### 最小化 E_in(W)\n\n機器學習的訓練過程就是最小化 E_in(W)，推導出 Cross-Entropy Error 之後，就可以將 E_in(W) 寫成如下圖所示，該函數是連續、可微，並且是凸函數。所以求解最小值就是用偏微分找到谷底梯度為 0 的地方。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-13.png\">\n</p>\n\n### 微分過程\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-14.png\">\n</p>\n\n### E_in(W) 梯度為 0 無法容易求出解\n\n和之前的 Linear Regression 不同，這個 ▿Ein(w) 不是一個線性的式子，要求解 ▿Ein(w)=0 是很困難的。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-15.png\">\n</p>\n\n### 使用 PLA 優化的方式最佳化\n\n這種情況可以使用類似 PLA 利用迭代的方式來求解，這樣的方法又稱為梯度下降法（Gradient Descent）\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-16.png\">\n</p>\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-17.png\">\n</p>\n\n### 逐步優化\n\n要尋找目標函數曲線的波谷，可以想像成一個人站在半山腰，每次都選擇往某個方向走一小步，讓他可以距離谷底更近，哪個方向可以更近谷底(greedy)，就選擇往這個方向前進。\n\n所以優化方式就是 wt+1 = wt + ita(v)，v 代表方向（單位長度），ita 代表步伐大小。\n\n找出 min Ein(wt+ita(v)) 就可以完成機器的訓練了。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-18.png\">\n</p>\n\n### 轉換成線性\n\nmin Ein(wt+ita(v)) 還是非線性的，很難求解，但在細微的觀點上決定怎麼走下一步，原本的 min Ein(wt+ita(v)) 可以使用泰勒展開式轉換成線性的 min Ein(Wt) + ita(V)▿Ein(w)。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-19.png\">\n</p>\n\n### Gradient Descent 梯度下降\n\n所以我們真正要求解的是 v 乘上梯度如何才能越小越好，也就是 v 與梯度是完全相反的方向。想像一下：如果一條直線的斜率 k>0，說明向右是上升的方向，應該要向左走，反之，斜率 k<0，向右走才是對的。如此才能逐步降低 E_in，求得 min Ein。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-20.png\">\n</p>\n\n### ita 參數的選擇\n\n解決的方向的問題，代表步伐大小的 ita 也很重要，如果步伐太小，到達山谷的速度會太慢，如果步伐太大，則會很不穩定，E_in 會一下太高一下太低，有可能也會無法到達山谷。理想上在距離谷底較遠時，步伐要大一些，接近谷底時，步伐要小一些，所以我們可以讓步伐與梯度數值成正相關，來做到我們想要的效果。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-21.png\">\n</p>\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-22.png\">\n</p>\n\n### 完整的 Logistic Regression Learning Algorithm\n\n先初始化 w0，第一步計算 Logistic Regression 的 Ein 梯度，然後更新 w，wt+1 = wt - ita(V)▿Ein(wt)，直到 ▿Ein(wt+1) = 0 或是足夠多的回合之後回傳最後的 wt+1。（每回合計算的時間複雜度與 Pocket PLA 一樣）\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-23.png\">\n</p>\n\n### 總結\n\n第十講中我們了解了當要用 Logistic Regression 解可能性這樣的問題的時候，我們使用的 sigmoid 來將函數做轉換，因此導出了 cross-enropy error function，最佳化無法直接用偏微分的方式求出解，所以使用了 Gradient Descent 這樣的方式來逐步優化來求出解。 \n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-24.png\">\n</p>","html":"<h3 id=\"\">前言</h3>\n\n<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href=\"http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-jiu-jiang-xue-xi-bi-ji/\">第九講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">熱身回顧一下</h3>\n\n<p>在上一講中我們了解了如何使用線性迴歸的方法來讓機器學習回答實數解的問題，第十講我們將介紹使用 Logistic Regression 來讓機器學習回答可能機率的問題。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-1.png\">\n</p>\n\n<h3 id=\"\">可能性問題</h3>\n\n<p>我們用心臟病發生的機率這樣的問題來做為例子，假設我們有一組病患的數據，我們需要預測他們在一段時間之後患上心臟病的「可能性」為何。我們拿到的歷史數據跟之前的二元分類問題是一樣的，我們只知道病患之後有或者沒有發生心臟病，從這樣的數據我們用之前學過的二元分類學習方法可以讓機器學習到如何預測病患會不會發病，但現在我們想讓機器回答的是可能性，不像二元分類那麼硬，所以又稱軟性二元分類，即 f(x) = P(+1|x)，取值的範圍區間在 [0, 1]。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-2.png\">\n</p>\n\n<h3 id=\"\">軟性二元分類</h3>\n\n<p>我們的訓練數據跟二元分類是完全一樣的，x 是病人的特徵值，y 是 +1（患心臟病）或 -1（沒有患心臟病），沒有告訴我們有關「患病機率」的訊息。回想在二元分類中，我們使用 w*x 得到一個「分數」之後，再利用取號運算 sign 來預測 y 是 +1 或是 -1。對於目前這個問題，如果我們能夠將這個「分數」映射到 [0, 1] 區間，似乎就可以解這個問題了～</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-3.png\">\n</p>\n\n<h3 id=\"logistic\">Logistic 假設</h3>\n\n<p>我們把上面的想法寫成式子，就是 h(x) = theta(wx)，映射分數到 [0, 1] 的 function 就叫做 logistic function，用 theta 來表示</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-4.png\">\n</p>\n\n<h3 id=\"logistic\">Logistic 函式</h3>\n\n<p>Logistic regression 選擇使用了 sigmoid 來將值域映射到 [0, 1]，sigmoid 是 f(s) = 1 / (1 + exp(-s))，f(x) 單調遞增，0 &lt;= f(x) &lt;= 1。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-5.png\">\n</p>\n\n<h3 id=\"logisticregressionerrorfunction\">如何定義 Logistic Regression 的 Error Function</h3>\n\n<p>那麼對於 Logistic 假設，我們要如何定義 Logistic Regression 的 Error Function E_in(h)？回顧一下之前學過的方法，Linear Classificaiton 是使用 0/1 Error，Linear Regression 是使用 Squared Error。</p>\n\n<p>Logistic Regression 的目標是找到 h(x) 接近 f(x)＝P(+1|x)，也就是說當 y=+1 時，P(y|x) = f(x)，當 y=-1 時，P(y|x) = 1-f(x)。我們要讓 Error 小，就是要讓 h(x) 與 f(x) 接近。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-6.png\">\n</p>\n\n<h3 id=\"likelihood\">使用 Likelihood 的概念</h3>\n\n<p>我們知道資料集 D 是由 f(x) 產生的，我們可以用這樣的思路去想，f(x) 跟 h(x) 生成目前看到的資料集 D 的機率是多少，用這種 Likelihood 的概念，我們只要在訓練過程讓 h(x) 跟 f(x) 產生資料集 D 的機率接近就可以。</p>\n\n<p>不過我們並不知道 f(x)，但我們可以知道 f(x) 產生資料集 D 的機率很高，因為確實就是由 f(x) 產生的。</p>\n\n<p>簡而言之就是產生目前所看到的資料集是一個我們不知道的 f(x)，f(x) 可能會產生出很多種資料集，我們看到的只是其中之一，但 f(x) 是我們無法知道的，我們可以去算 g(x)，從眾多的 g(x) 去算出那一個最可能產生我們目前看到的資料集，這就是去算 max likelihood，之後我們就可以說 g(x) 跟 f(x) 很像～</p>\n\n<p>所以我們只要計算讓 h(x) Likelihood 越高越好。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-7.png\">\n</p>\n\n<h3 id=\"likelihoodlogisticregression\">將 Likelihood 套用到 Logistic Regression</h3>\n\n<p>所以現在問題轉換成了計算 h(x) 越高越好了，得到最高 likelihood 機率值的就是我們所要的 g(x)。</p>\n\n<p>式子就是 g = argmax likelihood(h)</p>\n\n<p>likelihood(h) 的式子可以描述成 P(x1)h(x1) x P(x2)(1-h(x2)) ... P(xn)h(1-h(xn))</p>\n\n<p>又因為 sigmoid 性質 1-h(x) = h(-x)，所以 likelihood(h) 的式子可以描述成 P(x1)h(+x1) x P(x2)(-h(x2)) ... P(xn)h(-h(xn))</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-8.png\">\n</p>\n\n<h3 id=\"\">將式子做一些轉換</h3>\n\n<p>likelihood(h)＝P(x1)h(+x1) x P(x2)(-h(x2)) ... P(xn)h(-h(xn))，我們要計算的是 max likelihood，所以     max likelihood(h) 正比於 h(score) 連乘，正比於 theta(yn)theta(wxn) 連乘。</p>\n\n<p>我們取上 ln，不會影響計算結果，所以取 ln 做後續的推導。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-10.png\">\n</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-11.png\">\n</p>\n\n<h3 id=\"crossentropyerror\">Cross-Entropy Error</h3>\n\n<p>如果 max Likelihood 轉換成 Error，作為 Logistic Regression 的 Error Function，就會是求 min 的 -ln theta(ynwxn) 連加，代進 sigmoid theta，就會得到 ln(1+exp(-ywx))，這又稱為 Cross-Entropy Error </p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-12.png\">\n</p>\n\n<h3 id=\"e_inw\">最小化 E_in(W)</h3>\n\n<p>機器學習的訓練過程就是最小化 E<em>in(W)，推導出 Cross-Entropy Error 之後，就可以將 E</em>in(W) 寫成如下圖所示，該函數是連續、可微，並且是凸函數。所以求解最小值就是用偏微分找到谷底梯度為 0 的地方。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-13.png\">\n</p>\n\n<h3 id=\"\">微分過程</h3>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-14.png\">\n</p>\n\n<h3 id=\"e_inw0\">E_in(W) 梯度為 0 無法容易求出解</h3>\n\n<p>和之前的 Linear Regression 不同，這個 ▿Ein(w) 不是一個線性的式子，要求解 ▿Ein(w)=0 是很困難的。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-15.png\">\n</p>\n\n<h3 id=\"pla\">使用 PLA 優化的方式最佳化</h3>\n\n<p>這種情況可以使用類似 PLA 利用迭代的方式來求解，這樣的方法又稱為梯度下降法（Gradient Descent）</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-16.png\">\n</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-17.png\">\n</p>\n\n<h3 id=\"\">逐步優化</h3>\n\n<p>要尋找目標函數曲線的波谷，可以想像成一個人站在半山腰，每次都選擇往某個方向走一小步，讓他可以距離谷底更近，哪個方向可以更近谷底(greedy)，就選擇往這個方向前進。</p>\n\n<p>所以優化方式就是 wt+1 = wt + ita(v)，v 代表方向（單位長度），ita 代表步伐大小。</p>\n\n<p>找出 min Ein(wt+ita(v)) 就可以完成機器的訓練了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-18.png\">\n</p>\n\n<h3 id=\"\">轉換成線性</h3>\n\n<p>min Ein(wt+ita(v)) 還是非線性的，很難求解，但在細微的觀點上決定怎麼走下一步，原本的 min Ein(wt+ita(v)) 可以使用泰勒展開式轉換成線性的 min Ein(Wt) + ita(V)▿Ein(w)。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-19.png\">\n</p>\n\n<h3 id=\"gradientdescent\">Gradient Descent 梯度下降</h3>\n\n<p>所以我們真正要求解的是 v 乘上梯度如何才能越小越好，也就是 v 與梯度是完全相反的方向。想像一下：如果一條直線的斜率 k>0，說明向右是上升的方向，應該要向左走，反之，斜率 k&lt;0，向右走才是對的。如此才能逐步降低 E_in，求得 min Ein。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-20.png\">\n</p>\n\n<h3 id=\"ita\">ita 參數的選擇</h3>\n\n<p>解決的方向的問題，代表步伐大小的 ita 也很重要，如果步伐太小，到達山谷的速度會太慢，如果步伐太大，則會很不穩定，E_in 會一下太高一下太低，有可能也會無法到達山谷。理想上在距離谷底較遠時，步伐要大一些，接近谷底時，步伐要小一些，所以我們可以讓步伐與梯度數值成正相關，來做到我們想要的效果。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-21.png\">\n</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-22.png\">\n</p>\n\n<h3 id=\"logisticregressionlearningalgorithm\">完整的 Logistic Regression Learning Algorithm</h3>\n\n<p>先初始化 w0，第一步計算 Logistic Regression 的 Ein 梯度，然後更新 w，wt+1 = wt - ita(V)▿Ein(wt)，直到 ▿Ein(wt+1) = 0 或是足夠多的回合之後回傳最後的 wt+1。（每回合計算的時間複雜度與 Pocket PLA 一樣）</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-23.png\">\n</p>\n\n<h3 id=\"\">總結</h3>\n\n<p>第十講中我們了解了當要用 Logistic Regression 解可能性這樣的問題的時候，我們使用的 sigmoid 來將函數做轉換，因此導出了 cross-enropy error function，最佳化無法直接用偏微分的方式求出解，所以使用了 Gradient Descent 這樣的方式來逐步優化來求出解。 </p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-10-24.png\">\n</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-01-17T06:03:36.000Z","created_by":1,"updated_at":"2016-08-09T05:35:49.000Z","updated_by":1,"published_at":"2016-01-17T18:37:49.000Z","published_by":1},{"id":71,"uuid":"3ea65929-1ca8-43f6-bdca-9912803770f7","title":"Laravel 4.2 to Laravel 5.2","slug":"laravel-4-2-to-laravel-5-2","markdown":"# Laravel 4.2 to Laravel 5.0\n\n## Directory Structure\n\n```\n    app/commandd    => app/Console\n    app/controllers => app/HTTP/Controllers\n\n    app/models      => app/User.php\n    app/start       => X\n\n    app/lang        => resources/lang\n    app/views       => resources/views\n\n    config, database, tests, storage => root\n```\n\n## Form & HTML \n\n```\n{!!\n\tForm::text('username');\n!!}\n```\n\n--<n>--\n\n## It's Gone\n\n![](https://github.com/fukuball/laravel5-slide/raw/master/assets/gif/gone.gif)\n\n--<n>--\n\n## Form & HTML \n\n\"laravelcollective/html\": \"~5.0\"\n\nconfig/app.php \n\n```\n'providers' => [\n\t...\n\n\tCollective\\Html\\HtmlServiceProvider::class,\n]\n\n'aliases' => [\n\t...\n\n    'Form'      => Collective\\Html\\FormFacade::class,\n    'Html'      => Collective\\Html\\HtmlFacade::class,\n]\n```\n\n--<n>--\n\n## Blade Raw Tags\n\n```\n{{\n}}\n```\n\n```\n{!!\n!!}\n```\n\n## FileSystem - Flysystem \n### config/filesystems.php\n\n```\n\n\t'default' => 'local',\n\n```\n\n```\n\t'cloud' => 's3',\n\n```\n\n\n```php\n$disk = Storage::disk('s3');\n\n$contents = Storage::disk('local')->get('file.jpg')\n```\n\n--<n>--\n\n### Cloud\n\n```\nuse Illuminate\\Contracts\\Cloud;\n\nclass HomeController extends BaseController {\n\tpublic function index(Cloud $cloud) {\n\n        $cloud->get('hello.text');\n\t}\n}\n``` \n\n--<n>--\n\n## Socilate\n\n--<n>--\n\n# Laravel 5.0 to Laravel 5.1\n\n## Namespace\n### PSR-2\n\nPSR-2 程式碼風格指南已經被 Laravel 框架採用為預設的風格指南。\n![](https://github.com/fukuball/laravel5-slide/raw/master/assets/img/controller.png)\n\nhttps://github.com/php-fig/fig-standards/blob/master/accepted/PSR-2-coding-style-guide.md\n\n## Event Broadcasting\n\nLaravel 讓你可以簡單的經由 websocket 連線來「廣播」你的事件。\n\n## Middleware\n\n## 測試翻修\n\n```\npublic function testNewUserRegistration()\n{\n    $this->visit('/register')\n         ->type('Taylor', 'name')\n         ->check('terms')\n         ->press('Register')\n         ->seePageIs('/dashboard');\n}\n```\n\n## 模型工廠\n\n模型工廠讓你簡單的為 Eloquent 模型定義一組「預設」的屬性，並為你的測試或資料填充產生測試模型實例。模型工廠也使用進階的 Faker PHP 函式庫來產生隨機屬性的資料\n\n```\n$factory->define(App\\User::class, function ($faker) {\n    return [\n        'name' => $faker->name,\n        'email' => $faker->email,\n        'password' => str_random(10),\n        'remember_token' => str_random(10),\n    ];\n});\n```\n\n## 資料夾結構\n\napp/Commands => app/Jobs\napp/Handler => app/Listeners\n\n# Laravel 5.1 to Laravel 5.1.11\n\n登入授權機制改善\n\n# Laravel 5.1.11 to 5.2\n\n## Authentication Scaffolding\n\n```\nphp artisan make:auth\n```\n\n## Middleware Groups\n\n```\n/**\n * The application's route middleware groups.\n *\n * @var array\n */\nprotected $middlewareGroups = [\n    'web' => [\n        \\App\\Http\\Middleware\\EncryptCookies::class,\n    ],\n\n    'api' => [\n        'throttle:60,1',\n    ],\n];\n```\n\n```\nRoute::group(['middleware' => ['web']], function () {\n    //\n});\n```\n\n## Rate Limiting\n\n```\nRoute::get('/api/users', ['middleware' => 'throttle:60,1', function () {\n    //\n}]);\n```\n\n## Array Validation\n\n```\n$validator = Validator::make($request->all(), [\n    'person.*.email' => 'email|unique:users'\n]);\n```\n\n```\n'custom' => [\n    'person.*.email' => [\n        'unique' => 'Each person must have a unique e-mail address',\n    ]\n],\n```\n\n### CronJob\n\nhttp://www.sitepoint.com/managing-cronjobs-with-laravel/\n","html":"<h1 id=\"laravel42tolaravel50\">Laravel 4.2 to Laravel 5.0</h1>\n\n<h2 id=\"directorystructure\">Directory Structure</h2>\n\n<pre><code>    app/commandd    =&gt; app/Console\n    app/controllers =&gt; app/HTTP/Controllers\n\n    app/models      =&gt; app/User.php\n    app/start       =&gt; X\n\n    app/lang        =&gt; resources/lang\n    app/views       =&gt; resources/views\n\n    config, database, tests, storage =&gt; root\n</code></pre>\n\n<h2 id=\"formhtml\">Form &amp; HTML</h2>\n\n<pre><code>{!!\n    Form::text('username');\n!!}\n</code></pre>\n\n<p>--<n>--</p>\n\n<h2 id=\"itsgone\">It's Gone</h2>\n\n<p><img src=\"https://github.com/fukuball/laravel5-slide/raw/master/assets/gif/gone.gif\" alt=\"\" /></p>\n\n<p>--<n>--</p>\n\n<h2 id=\"formhtml\">Form &amp; HTML</h2>\n\n<p>\"laravelcollective/html\": \"~5.0\"</p>\n\n<p>config/app.php </p>\n\n<pre><code>'providers' =&gt; [  \n    ...\n\n    Collective\\Html\\HtmlServiceProvider::class,\n]\n\n'aliases' =&gt; [  \n    ...\n\n    'Form'      =&gt; Collective\\Html\\FormFacade::class,\n    'Html'      =&gt; Collective\\Html\\HtmlFacade::class,\n]\n</code></pre>\n\n<p>--<n>--</p>\n\n<h2 id=\"bladerawtags\">Blade Raw Tags</h2>\n\n<pre><code>{{\n}}\n</code></pre>\n\n<pre><code>{!!\n!!}\n</code></pre>\n\n<h2 id=\"filesystemflysystem\">FileSystem - Flysystem</h2>\n\n<h3 id=\"configfilesystemsphp\">config/filesystems.php</h3>\n\n<pre><code>    'default' =&gt; 'local',\n</code></pre>\n\n<pre><code>    'cloud' =&gt; 's3',\n</code></pre>\n\n<pre><code class=\"php\">$disk = Storage::disk('s3');\n\n$contents = Storage::disk('local')-&gt;get('file.jpg')\n</code></pre>\n\n<p>--<n>--</p>\n\n<h3 id=\"cloud\">Cloud</h3>\n\n<pre><code>use Illuminate\\Contracts\\Cloud;\n\nclass HomeController extends BaseController {  \n    public function index(Cloud $cloud) {\n\n        $cloud-&gt;get('hello.text');\n    }\n}\n</code></pre>\n\n<p>--<n>--</p>\n\n<h2 id=\"socilate\">Socilate</h2>\n\n<p>--<n>--</p>\n\n<h1 id=\"laravel50tolaravel51\">Laravel 5.0 to Laravel 5.1</h1>\n\n<h2 id=\"namespace\">Namespace</h2>\n\n<h3 id=\"psr2\">PSR-2</h3>\n\n<p>PSR-2 程式碼風格指南已經被 Laravel 框架採用為預設的風格指南。 <br />\n<img src=\"https://github.com/fukuball/laravel5-slide/raw/master/assets/img/controller.png\" alt=\"\" /></p>\n\n<p><a href='https://github.com/php-fig/fig-standards/blob/master/accepted/PSR-2-coding-style-guide.md'>https://github.com/php-fig/fig-standards/blob/master/accepted/PSR-2-coding-style-guide.md</a></p>\n\n<h2 id=\"eventbroadcasting\">Event Broadcasting</h2>\n\n<p>Laravel 讓你可以簡單的經由 websocket 連線來「廣播」你的事件。</p>\n\n<h2 id=\"middleware\">Middleware</h2>\n\n<h2 id=\"\">測試翻修</h2>\n\n<pre><code>public function testNewUserRegistration()  \n{\n    $this-&gt;visit('/register')\n         -&gt;type('Taylor', 'name')\n         -&gt;check('terms')\n         -&gt;press('Register')\n         -&gt;seePageIs('/dashboard');\n}\n</code></pre>\n\n<h2 id=\"\">模型工廠</h2>\n\n<p>模型工廠讓你簡單的為 Eloquent 模型定義一組「預設」的屬性，並為你的測試或資料填充產生測試模型實例。模型工廠也使用進階的 Faker PHP 函式庫來產生隨機屬性的資料</p>\n\n<pre><code>$factory-&gt;define(App\\User::class, function ($faker) {\n    return [\n        'name' =&gt; $faker-&gt;name,\n        'email' =&gt; $faker-&gt;email,\n        'password' =&gt; str_random(10),\n        'remember_token' =&gt; str_random(10),\n    ];\n});\n</code></pre>\n\n<h2 id=\"\">資料夾結構</h2>\n\n<p>app/Commands => app/Jobs <br />\napp/Handler => app/Listeners</p>\n\n<h1 id=\"laravel51tolaravel5111\">Laravel 5.1 to Laravel 5.1.11</h1>\n\n<p>登入授權機制改善</p>\n\n<h1 id=\"laravel5111to52\">Laravel 5.1.11 to 5.2</h1>\n\n<h2 id=\"authenticationscaffolding\">Authentication Scaffolding</h2>\n\n<pre><code>php artisan make:auth  \n</code></pre>\n\n<h2 id=\"middlewaregroups\">Middleware Groups</h2>\n\n<pre><code>/**\n * The application's route middleware groups.\n *\n * @var array\n */\nprotected $middlewareGroups = [  \n    'web' =&gt; [\n        \\App\\Http\\Middleware\\EncryptCookies::class,\n    ],\n\n    'api' =&gt; [\n        'throttle:60,1',\n    ],\n];\n</code></pre>\n\n<pre><code>Route::group(['middleware' =&gt; ['web']], function () {  \n    //\n});\n</code></pre>\n\n<h2 id=\"ratelimiting\">Rate Limiting</h2>\n\n<pre><code>Route::get('/api/users', ['middleware' =&gt; 'throttle:60,1', function () {  \n    //\n}]);\n</code></pre>\n\n<h2 id=\"arrayvalidation\">Array Validation</h2>\n\n<pre><code>$validator = Validator::make($request-&gt;all(), [\n    'person.*.email' =&gt; 'email|unique:users'\n]);\n</code></pre>\n\n<pre><code>'custom' =&gt; [  \n    'person.*.email' =&gt; [\n        'unique' =&gt; 'Each person must have a unique e-mail address',\n    ]\n],\n</code></pre>\n\n<h3 id=\"cronjob\">CronJob</h3>\n\n<p><a href='http://www.sitepoint.com/managing-cronjobs-with-laravel/'>http://www.sitepoint.com/managing-cronjobs-with-laravel/</a></p>","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-01-19T17:01:29.000Z","created_by":1,"updated_at":"2016-01-20T18:56:54.000Z","updated_by":1,"published_at":null,"published_by":null},{"id":72,"uuid":"dc0b5b06-44fe-4ca7-ad57-a5e2b19f2725","title":"林軒田教授機器學習基石 Machine Learning Foundations 第十一講學習筆記","slug":"lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-jiang-xue-xi-bi-ji-2","markdown":"### 前言\n\n本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 [第十講](http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-jiang-xue-xi-bi-ji/) 的碼農們，我建議可以先回頭去讀一下再回來喔！\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 熱身回顧一下\n\n在上一講中我們了解了 Logistic Regression 演算法並了解了如何使用 Logistic Regrssion 來預測心臟病發病機率這樣的問題，這一講中將延伸之前學過的演算法，在理論上說明 Linear Regression 以及 Logistic Regression 都可以用來解 Binary Classification 的問題。學會了 Binary Classification 之後，我們也可以用這樣的技巧來解 Multi-Classification 的問題。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-1.png\">\n</p>\n\n### 比較之前學過的演算法\n\n比較之前學過的演算法，三個算法最後都會得到一個線性函數來輸出 scroe 值，但 PLA 做 Linear Classification 是一個 NP-hard 的問題，Linear Regression 及 Logistic Regression 則相對較容易求解，我們可以使用 Linear Regression 或是 Logistic Regression 演算法來解 Linear Classification 的問題嗎？\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-2.png\">\n</p>\n\n### 將三個演算法的 Error Function 整理一下\n\n依據各個 Error Function 的算法，我們都可以整理成 ys 的形式，在物理意義上，我們可以說 y 代表正確性，s 代表正確或錯誤程度多少。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-3.png\">\n</p>\n\n### 畫成圖來瞧瞧\n\n我們以 ys 為橫坐標，error 為縱坐標，把這三個函數畫出來。 0/1 error 在 ys <= 0 時 error 是 1，squre error 在 ys << 1 或 ys >> 1 時會很大，cross-entropy error 在 ys 很小時， error 也會變得很大，但當 squre error 跟 cross-entropy error 很小時，他們 ys 區間所對應的 squre error 也會很小，因此我們可以從這樣的圖得知 squre error 跟 cross-entropy error 都是 0/1 error 的上界。 \n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-4.png\">\n</p>\n\n### 套用至 VC Bound 理論\n\n將這樣的 Error 上界關係套用 VC Bound 理論，只要 Logistic Regression 或 Linear Regression 的 E_in 小，那 Linear Classfication 的 E_out 就會小，因此理論上我們可以使用 Logistic Regression 及 Linear Regression 來代替 Linear Classfication。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-5.png\">\n</p>\n\n### 使用 Regression 來做分類\n\n依據以上的理論，我們可以用 Regression 來代替 PLA/Pocket 做分類，會比較有效率（PLA 是一個 NP-hard 的問題），比較一下三個演算法，(1) PLA 在線性可分的時候可以得到一個最佳解，但資料常常不會是線性可分，所以就會用 Pocker 來替代。（2）Linear Regression 可以很快的優化求解，但當 |ys| 很大的時候，positive direction 及 negative direction 的 bound 都太鬆，E_out 可能會效果不好。(3) Logistic Regression 可以用 gradient descent 求解，但在 negative direction 的 bound 會太鬆，E_out 可能會效果不好。\n\n以據上述的特性，我們可以使用 Linear Regression 跑出一個 w 作為 (PLA/Pocket/Logistic Regression) 的 w0，然後再使用 w0 來跑其他模型，這樣可以加快其他模型的優化速度。然後實務上拿到的資料常常不是線性可分的，所以我們會比較常使用 Logistic Regression 而不是 PLA/Pocket。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-6.png\">\n</p>\n\n### 優化 Logistic Regression\n\n實務上我們會比較常使用 Logistic Regression，但 Logistic Regression 比起 PLA 比較沒有效率，因為 Logistic Regression 在決定優化方向時，會觀察所有的資料點再做決定，時間複雜度是 O(N)，但 PLA 每次只看一個點，時間複雜度是 O(1)，我們可以讓 Logistic Regression 優化到 O(1) 嗎？\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-7.png\">\n</p>\n\n### 隨機梯度下降\n\n我們可以使用隨機梯度下降的方式讓 Logistic Regresiion 優化到 O(1)，這樣的方法就是每次只透過隨機選取一個資料點（xi, yi）來取梯度，然後再用這個梯度對 w 進行更新，這種優化方法就叫做隨機梯度下降。\n\n原來的演算法是用所有的資料點在算梯度，然後取平均，再更新 W，隨機梯度下降是不用每次算所有的點，每次只算一個點來代替所有點的平均。可以這樣做的背後原理，我們可以想成隨機取一個點取很多次之後，大概就是跟所有的點做平均差不多，所以我們可以用隨機梯度下降取代原本的梯度下降。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-8.png\">\n</p>\n\n### 隨機梯度下降與 PLA 的關係\n\n將隨機梯度下降與 PLA 算式放在一起觀察，可以發現隨機梯度是一個軟性的 PLA，每次調整 0~1 ynxn，但 PLA 只有調與不調，比較硬。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-9.png\">\n</p>\n\n### 多元分類\n\n我們現在已經會 Binary Classification 了，那 Multi-Classification 的問題要怎麼解呢？\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-10.png\">\n</p>\n\n### 一次分一個類別出來\n\n既然我們會二元分類，我們可以一次分一個類別出來，讓現在要分類出來的資料是 o，其他資料設成 x 來做分類。以這個例子就是先把正方形變成 o，其他變成 x。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-11.png\">\n</p>\n\n### 結合所有的二元分類\n\n結合所有的二元分類模型之後，我們就可以做多元分類了，不過會有一些區域有模糊地帶。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-12.png\">\n</p>\n\n### 使用軟性二元分類來解模糊地帶的問題\n\n在這邊我們可以使用軟性二元分類來解模糊地帶的問題，比如將所有的二元分類模型用 Logistic Regression 來訓練，就能讓每個分類器預測出資料是屬於哪一類的百分比，這樣我們就可以從所有的分類器裡面找出百分比最大的那個來預測這個資料點是屬於哪一類，而解決模糊地帶的問題。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-13.png\">\n</p>\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-14.png\">\n</p>\n\n### One-Versus-ALL (OVA) Decomposition\n\n這樣的方法就是 One-Versus-ALL (OVA) Decomposition，每次把一個類別和非這個類別的當成兩類，用 Logistic Regresion 分類，當分類器輸入某個點，就看這個點在哪一個類別的機率最大。不過這個方法的缺點是，當類別很多的時候，比如 k=100，那每次用 logistic Regression 分類時正樣本和負樣本的差別就會非常大，訓練出來的結果可能會不好。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-15.png\">\n</p>\n\n### 一次只比較兩個類別\n\n我們可以用一次只比較兩個類別這樣的方法來解決 OVA 樣本資料差距過大的問題。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-16.png\">\n</p>\n\n### 投票預測分類\n\n這樣的方法，每次只取兩個類別來做訓練，如果一共有 K 類的話，就要做  C(K, 2) 次的 Logistic Regression。當一個資料點輸入做預測時，就用這 C(K, 2) 個分類器給所有 K 個類別投票，取票數大的作為輸出結果。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-17.png\">\n</p>\n\n### One-Versus-One (OVO) Decomposition\n\n這樣的方法就是 One-Versus-One (OVO) Decomposition，這種方法的好處是可以應用在任何 Binary Classification 方法，缺點是效率可能會低一些，因為要訓練 C(K,2) 個類別，不過如果類別很多且每個類別的樣本量都差不多的時候，OVO 的方法不一定會比 OVA 方法效率低。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-18.png\">\n</p>\n\n### 總結\n\n在第十一講中，我們比較了 PLA、Linear Regression 及 Logistic Regression，然後理論上這三個演算法都可以應用在 Linear Classifition 上，然後也學會了使用 Stochastic Greadient Descent 來讓 Logistic Regression 跟 PLA 演算法的計算時間複雜度一樣。學會了 Binary Classification 之後，我們也可以將這樣的方法運用 OVA 及 OVO 的方式來解 Multi-Classification 的問題。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-19.png\">\n</p>","html":"<h3 id=\"\">前言</h3>\n\n<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href=\"http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-jiang-xue-xi-bi-ji/\">第十講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">熱身回顧一下</h3>\n\n<p>在上一講中我們了解了 Logistic Regression 演算法並了解了如何使用 Logistic Regrssion 來預測心臟病發病機率這樣的問題，這一講中將延伸之前學過的演算法，在理論上說明 Linear Regression 以及 Logistic Regression 都可以用來解 Binary Classification 的問題。學會了 Binary Classification 之後，我們也可以用這樣的技巧來解 Multi-Classification 的問題。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-1.png\">\n</p>\n\n<h3 id=\"\">比較之前學過的演算法</h3>\n\n<p>比較之前學過的演算法，三個算法最後都會得到一個線性函數來輸出 scroe 值，但 PLA 做 Linear Classification 是一個 NP-hard 的問題，Linear Regression 及 Logistic Regression 則相對較容易求解，我們可以使用 Linear Regression 或是 Logistic Regression 演算法來解 Linear Classification 的問題嗎？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-2.png\">\n</p>\n\n<h3 id=\"errorfunction\">將三個演算法的 Error Function 整理一下</h3>\n\n<p>依據各個 Error Function 的算法，我們都可以整理成 ys 的形式，在物理意義上，我們可以說 y 代表正確性，s 代表正確或錯誤程度多少。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-3.png\">\n</p>\n\n<h3 id=\"\">畫成圖來瞧瞧</h3>\n\n<p>我們以 ys 為橫坐標，error 為縱坐標，把這三個函數畫出來。 0/1 error 在 ys &lt;= 0 時 error 是 1，squre error 在 ys &lt;&lt; 1 或 ys >> 1 時會很大，cross-entropy error 在 ys 很小時， error 也會變得很大，但當 squre error 跟 cross-entropy error 很小時，他們 ys 區間所對應的 squre error 也會很小，因此我們可以從這樣的圖得知 squre error 跟 cross-entropy error 都是 0/1 error 的上界。 </p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-4.png\">\n</p>\n\n<h3 id=\"vcbound\">套用至 VC Bound 理論</h3>\n\n<p>將這樣的 Error 上界關係套用 VC Bound 理論，只要 Logistic Regression 或 Linear Regression 的 E<em>in 小，那 Linear Classfication 的 E</em>out 就會小，因此理論上我們可以使用 Logistic Regression 及 Linear Regression 來代替 Linear Classfication。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-5.png\">\n</p>\n\n<h3 id=\"regression\">使用 Regression 來做分類</h3>\n\n<p>依據以上的理論，我們可以用 Regression 來代替 PLA/Pocket 做分類，會比較有效率（PLA 是一個 NP-hard 的問題），比較一下三個演算法，(1) PLA 在線性可分的時候可以得到一個最佳解，但資料常常不會是線性可分，所以就會用 Pocker 來替代。（2）Linear Regression 可以很快的優化求解，但當 |ys| 很大的時候，positive direction 及 negative direction 的 bound 都太鬆，E<em>out 可能會效果不好。(3) Logistic Regression 可以用 gradient descent 求解，但在 negative direction 的 bound 會太鬆，E</em>out 可能會效果不好。</p>\n\n<p>以據上述的特性，我們可以使用 Linear Regression 跑出一個 w 作為 (PLA/Pocket/Logistic Regression) 的 w0，然後再使用 w0 來跑其他模型，這樣可以加快其他模型的優化速度。然後實務上拿到的資料常常不是線性可分的，所以我們會比較常使用 Logistic Regression 而不是 PLA/Pocket。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-6.png\">\n</p>\n\n<h3 id=\"logisticregression\">優化 Logistic Regression</h3>\n\n<p>實務上我們會比較常使用 Logistic Regression，但 Logistic Regression 比起 PLA 比較沒有效率，因為 Logistic Regression 在決定優化方向時，會觀察所有的資料點再做決定，時間複雜度是 O(N)，但 PLA 每次只看一個點，時間複雜度是 O(1)，我們可以讓 Logistic Regression 優化到 O(1) 嗎？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-7.png\">\n</p>\n\n<h3 id=\"\">隨機梯度下降</h3>\n\n<p>我們可以使用隨機梯度下降的方式讓 Logistic Regresiion 優化到 O(1)，這樣的方法就是每次只透過隨機選取一個資料點（xi, yi）來取梯度，然後再用這個梯度對 w 進行更新，這種優化方法就叫做隨機梯度下降。</p>\n\n<p>原來的演算法是用所有的資料點在算梯度，然後取平均，再更新 W，隨機梯度下降是不用每次算所有的點，每次只算一個點來代替所有點的平均。可以這樣做的背後原理，我們可以想成隨機取一個點取很多次之後，大概就是跟所有的點做平均差不多，所以我們可以用隨機梯度下降取代原本的梯度下降。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-8.png\">\n</p>\n\n<h3 id=\"pla\">隨機梯度下降與 PLA 的關係</h3>\n\n<p>將隨機梯度下降與 PLA 算式放在一起觀察，可以發現隨機梯度是一個軟性的 PLA，每次調整 0~1 ynxn，但 PLA 只有調與不調，比較硬。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-9.png\">\n</p>\n\n<h3 id=\"\">多元分類</h3>\n\n<p>我們現在已經會 Binary Classification 了，那 Multi-Classification 的問題要怎麼解呢？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-10.png\">\n</p>\n\n<h3 id=\"\">一次分一個類別出來</h3>\n\n<p>既然我們會二元分類，我們可以一次分一個類別出來，讓現在要分類出來的資料是 o，其他資料設成 x 來做分類。以這個例子就是先把正方形變成 o，其他變成 x。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-11.png\">\n</p>\n\n<h3 id=\"\">結合所有的二元分類</h3>\n\n<p>結合所有的二元分類模型之後，我們就可以做多元分類了，不過會有一些區域有模糊地帶。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-12.png\">\n</p>\n\n<h3 id=\"\">使用軟性二元分類來解模糊地帶的問題</h3>\n\n<p>在這邊我們可以使用軟性二元分類來解模糊地帶的問題，比如將所有的二元分類模型用 Logistic Regression 來訓練，就能讓每個分類器預測出資料是屬於哪一類的百分比，這樣我們就可以從所有的分類器裡面找出百分比最大的那個來預測這個資料點是屬於哪一類，而解決模糊地帶的問題。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-13.png\">\n</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-14.png\">\n</p>\n\n<h3 id=\"oneversusallovadecomposition\">One-Versus-ALL (OVA) Decomposition</h3>\n\n<p>這樣的方法就是 One-Versus-ALL (OVA) Decomposition，每次把一個類別和非這個類別的當成兩類，用 Logistic Regresion 分類，當分類器輸入某個點，就看這個點在哪一個類別的機率最大。不過這個方法的缺點是，當類別很多的時候，比如 k=100，那每次用 logistic Regression 分類時正樣本和負樣本的差別就會非常大，訓練出來的結果可能會不好。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-15.png\">\n</p>\n\n<h3 id=\"\">一次只比較兩個類別</h3>\n\n<p>我們可以用一次只比較兩個類別這樣的方法來解決 OVA 樣本資料差距過大的問題。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-16.png\">\n</p>\n\n<h3 id=\"\">投票預測分類</h3>\n\n<p>這樣的方法，每次只取兩個類別來做訓練，如果一共有 K 類的話，就要做  C(K, 2) 次的 Logistic Regression。當一個資料點輸入做預測時，就用這 C(K, 2) 個分類器給所有 K 個類別投票，取票數大的作為輸出結果。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-17.png\">\n</p>\n\n<h3 id=\"oneversusoneovodecomposition\">One-Versus-One (OVO) Decomposition</h3>\n\n<p>這樣的方法就是 One-Versus-One (OVO) Decomposition，這種方法的好處是可以應用在任何 Binary Classification 方法，缺點是效率可能會低一些，因為要訓練 C(K,2) 個類別，不過如果類別很多且每個類別的樣本量都差不多的時候，OVO 的方法不一定會比 OVA 方法效率低。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-18.png\">\n</p>\n\n<h3 id=\"\">總結</h3>\n\n<p>在第十一講中，我們比較了 PLA、Linear Regression 及 Logistic Regression，然後理論上這三個演算法都可以應用在 Linear Classifition 上，然後也學會了使用 Stochastic Greadient Descent 來讓 Logistic Regression 跟 PLA 演算法的計算時間複雜度一樣。學會了 Binary Classification 之後，我們也可以將這樣的方法運用 OVA 及 OVO 的方式來解 Multi-Classification 的問題。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-11-19.png\">\n</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-01-29T09:02:33.000Z","created_by":1,"updated_at":"2016-08-09T05:36:16.000Z","updated_by":1,"published_at":"2016-02-01T09:45:06.000Z","published_by":1},{"id":73,"uuid":"dd0f9f79-7258-4093-9192-34504e72c210","title":"Auth and Test","slug":"auth-and-test","markdown":"```\nphp artisan migrate\n```\n\n```\nphp artisan make:auth\n```\n\n- welcome.blade.php - the public welcome page\n- home.blade.php - the dashboard for logged-in users\n- auth/login.blade.php - the login page\n- auth/register.blade.php - the register/signup page\n- auth/passwords/email.blade.php - the password reset confirmation page\n- auth/passwords/reset.blade.php - the password reset prompt page\n- auth/emails/password.blade.php - the password reset email\n\n```\nRoute::group(['middleware' => 'web'], function () {\n    Route::auth();\n\n    Route::get('/home', 'HomeController@index');\n});\n```\n\n```\n// Authentication Routes...\n$this->get('login', 'Auth\\AuthController@showLoginForm');\n$this->post('login', 'Auth\\AuthController@login');\n$this->get('logout', 'Auth\\AuthController@logout');\n\n// Registration Routes...\n$this->get('register', 'Auth\\AuthController@showRegistrationForm');\n$this->post('register', 'Auth\\AuthController@register');\n\n// Password Reset Routes...\n$this->get('password/reset/{token?}', 'Auth\\PasswordController@showResetForm');\n$this->post('password/email', 'Auth\\PasswordController@sendResetLinkEmail');\n$this->post('password/reset', 'Auth\\PasswordController@reset');\n```\n\n```\ncomposer require laravel/socialite\n```\n\n```\n\tpublic function testLoginPage()\n    {\n        $this->visit('/')\n             ->click('Login')\n             ->seePageIs('/login');\n    }\n\n    public function testNewUserRegistration()\n    {\n\n        $user = factory(App\\User::class)->make();\n\n        $this->visit('/register')\n             ->type($user->name, 'name')\n             ->type($user->email, 'email')\n             ->type($user->password, 'password')\n             ->type($user->password, 'password_confirmation')\n             ->press('Register')\n             ->seePageIs('/home');\n\n\n        $this->seeInDatabase('users', ['email' => $user->email]);\n\n    }\n\n    public function testLoginHeader()\n    {\n\n        $user = factory(App\\User::class)->make();\n\n        $this->actingAs($user)\n             ->visit('/')\n             ->see($user->name);\n\n    }\n```\n\n","html":"<pre><code>php artisan migrate  \n</code></pre>\n\n<pre><code>php artisan make:auth  \n</code></pre>\n\n<ul>\n<li>welcome.blade.php - the public welcome page</li>\n<li>home.blade.php - the dashboard for logged-in users</li>\n<li>auth/login.blade.php - the login page</li>\n<li>auth/register.blade.php - the register/signup page</li>\n<li>auth/passwords/email.blade.php - the password reset confirmation page</li>\n<li>auth/passwords/reset.blade.php - the password reset prompt page</li>\n<li>auth/emails/password.blade.php - the password reset email</li>\n</ul>\n\n<pre><code>Route::group(['middleware' =&gt; 'web'], function () {  \n    Route::auth();\n\n    Route::get('/home', 'HomeController@index');\n});\n</code></pre>\n\n<pre><code>// Authentication Routes...\n$this-&gt;get('login', 'Auth\\AuthController@showLoginForm');\n$this-&gt;post('login', 'Auth\\AuthController@login');\n$this-&gt;get('logout', 'Auth\\AuthController@logout');\n\n// Registration Routes...\n$this-&gt;get('register', 'Auth\\AuthController@showRegistrationForm');\n$this-&gt;post('register', 'Auth\\AuthController@register');\n\n// Password Reset Routes...\n$this-&gt;get('password/reset/{token?}', 'Auth\\PasswordController@showResetForm');\n$this-&gt;post('password/email', 'Auth\\PasswordController@sendResetLinkEmail');\n$this-&gt;post('password/reset', 'Auth\\PasswordController@reset');\n</code></pre>\n\n<pre><code>composer require laravel/socialite  \n</code></pre>\n\n<pre><code>    public function testLoginPage()\n    {\n        $this-&gt;visit('/')\n             -&gt;click('Login')\n             -&gt;seePageIs('/login');\n    }\n\n    public function testNewUserRegistration()\n    {\n\n        $user = factory(App\\User::class)-&gt;make();\n\n        $this-&gt;visit('/register')\n             -&gt;type($user-&gt;name, 'name')\n             -&gt;type($user-&gt;email, 'email')\n             -&gt;type($user-&gt;password, 'password')\n             -&gt;type($user-&gt;password, 'password_confirmation')\n             -&gt;press('Register')\n             -&gt;seePageIs('/home');\n\n\n        $this-&gt;seeInDatabase('users', ['email' =&gt; $user-&gt;email]);\n\n    }\n\n    public function testLoginHeader()\n    {\n\n        $user = factory(App\\User::class)-&gt;make();\n\n        $this-&gt;actingAs($user)\n             -&gt;visit('/')\n             -&gt;see($user-&gt;name);\n\n    }\n</code></pre>","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-02-03T18:27:22.000Z","created_by":1,"updated_at":"2016-02-03T18:42:48.000Z","updated_by":1,"published_at":null,"published_by":null},{"id":74,"uuid":"06c344d2-f748-482a-ac3a-3d4ae15f9d6e","title":"林軒田教授機器學習基石 Machine Learning Foundations 第十二講學習筆記","slug":"lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-er-jiang-xue-xi-bi-ji","markdown":"### 前言\n\n本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 [第十一講](http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-jiang-xue-xi-bi-ji-2/) 的碼農們，我建議可以先回頭去讀一下再回來喔！\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 熱身回顧一下\n\n在上一講中，我們將線性分類的模型擴展到可以進行多元分類，擴展的方法很直覺，就是使用 One vs One 及 One vs All 兩種分解成二元分類的方式來做到多元分類。在這一講中將講解如何讓線性模型擴展到非線性模型，讓我們可以將機器學習演算法的複雜度提高以解決更複雜的問題，並說明非線性模型會有什麼影響。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-1.png\">\n</p>\n\n### 線性假設\n\n之前的演算法目前都是基於線性的假設之下去找出分類最好的線，但這在線性不可分的情況下，會得到較大的 Ein，理論上較大的 Ein 未來 Eout 效果也會不佳，有沒有辦法讓我們演算法得出的線不一定要是一條直線以得到更佳的 Ein 來增加學習效果呢？\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-2.png\">\n</p>\n\n### 圈圈可分\n\n我們從肉眼觀察可以發現右邊的資料點是一個「圈圈可分」的情況，所以我們要解這個問題，我們可以基於圈圈可分的情況去推導之前所有的演算法，但這樣有點麻煩，沒有沒其他更通用的方法？\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-3.png\">\n</p>\n\n### 比較圈圈可分及線性可分\n\n為了讓演算法可以通用，我們會思考，如果我們可以讓圈圈可分轉換到一個空間之後變成線性可分，那就太好了。我們比較一下圈圈可分及線性可分，當我們將 Xn 圈圈可分的資料點，透過一個圈圈方程式轉換到 Z 空間，這時資料點 Zn 在 Z 空間就是一個線性可分的情況，不過在 Z 空間線性可分，在 X 空間不一定會是圈圈可分。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-4.png\">\n</p>\n\n### Z 空間的線性假設\n\n觀察在 Z 空間的線性方程式，不同的參數在 X 空間會是不同的曲線，有可能是圓、橢圓、雙曲線等等，因此我們了解在 Z 空間的線會是 X 空間的二次曲線。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-5.png\">\n</p>\n\n### 一般化二次假設\n\n我們剛剛是使用 x0, x1^2, X2^2 來簡化理解這個問題，現在將問題更一般化，將原本的 xn 用 Phi 二次展開來一般化剛剛個問題，這樣的 Z 空間學習出來的線性方程式在 X 空間就不一定會是以原點為中心，這樣所有的二次曲線都有辦法在 Z 空間學習到了，而起原本在 X 空間的線性方程式也會包含在按次曲線中。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-6.png\">\n</p>\n\n### 好的二次空間假設\n\n所以原本的問題可以透過這樣的非線性轉換到二次 Z 空間進行機器學習演算法，在 Z 空間的線性可分就可以對應到 X 空間的二次曲線可分。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-7.png\">\n</p>\n\n### 非線性轉換的學習步驟\n\n了解了這樣的思路之後，非線性轉換的學習步驟就是先將資料點透過 Phi 轉換到非線性空間，然後使用之前學過的線性演算法進行機器學習，由於學習出來的 Z 空間線性方程式不一定能轉回 X 空間，我們實務的上做法是將測試資料透過 Phi 轉換到 Z 空間，再進行預測。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-8.png\">\n</p>\n\n### 非線性模型是潘朵拉的盒子\n\n學會了特徵轉換使用非線性模型就像打開了潘朵拉的盒子，我們可以任意的將資料轉換到更高維的空間來進行機器學習，如此可以得到更低的 Ein，但這對機器學習效果不一定好，因此要慎用。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-9.png\">\n</p>\n\n### 代價一：更高的計算及儲存代價\n\n我們可以將資料點進行 Q 次轉換，這樣原本的資料點會有 0 次項、1 次項、2 次項 .... Q 次項，每筆資料的維度都增加了，理所當然計算量及儲存量也都變高了。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-10.png\">\n</p>\n\n### 代價二：更高的模型複雜度\n\n進行了 Q 次轉換後，資料的為度更高了，理論上 VC dimention 也跟著增加了， VC dimention 代表著模型複雜度，在之前的課程中我們知道較複雜的模型會讓 Eout 變高。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-11.png\">\n</p>\n\n### 可能會面臨的問題\n\n我們用下圖來說明我們可能會面臨的問題，左圖雖然不是線性可分，但一眼看來其實也是一個不錯的結果，右圖可以得到完美的 Ein，但會覺得有點過頭，我們會面臨的問題就是要如何抉擇左邊或右邊？較高的 Q 次轉換會造成 Eout 與 Ein 不會很接近，但可以得到較小的 Ein，較低的 Q 次轉換可以保證 Eout 跟 Ein 很接近，但 Ein 的效果可能不好，怎麼選 Q 呢？\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-12.png\">\n</p>\n\n### 用看的選有風險\n\n就上述的例子我們可以用圖示來觀察，而使用較低的 Q 次轉換，但如果為度很高，我們是無法畫成圖來看的，而且用看圖的方式，我們可能會不小心用我們的人工運算，直接加上圈圈方程式來降低 VC dimention，這也可能會造成 Eout 效果不佳，因為 VC dimention 是經過人腦降低的，會讓我們低估 VC dimention 複雜度，所以我們應該要避免用看資料的方式來調整演算法。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-13.png\">\n</p>\n\n### 多項式結構化\n\n我們將 Q 次轉換用下面的式子及圖示結構化，我們可以發現 0 次轉換的假設會包含在 1 次轉換的假設中，1 次轉換的假設會包含在 2 次轉換的假設中，一直到 Q 次轉換這樣的結構，表示成 H0, H1, H2, ...., Hq。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-14.png\">\n</p>\n\n### 假設集合結構化\n\n從 H0 包含於 H1、H1 包含於 H2 .... Hq-1 包含於 Hq 這樣的關係中，我們可以推論 d_vc(H0) <= d_vc(H1) ... <= d_vc(Hq)，而在理論上 Ein(g0) >= Ein(g1) ... >= Ein(gq)，從之前學過的理論可知，out of sample Eout 在 d_vc 很高、Ein 很低的情況下，不一定會是最低點。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-15.png\">\n</p>\n\n### 線性模型第一優先\n\n所以依據理論，我們不該為了追求 Ein 低、訓練效果好來做機器學習，這樣是一種自我欺騙，我們要做的應該是使用線性模型為第一優先，如果 Ein 很差，則考慮做二次轉換，慢慢升高 d_vc，而不是一步登天。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-16.png\">\n</p>\n\n### 總結\n\n在這一講中我們打開了潘朵拉的盒子，學會了使用非線性轉換來得到更好的 Ein，但這會付出一些代價，會讓計算量增加、資料儲存量增加，若一次升高太多模型複雜度，還會造成學習效果不佳，Eout 會比 Ein 高很多，所以要慎用。最好的學習方式就是先從線性模型開始，然後再慢慢升高模型複雜度。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-17.png\">\n</p>\n","html":"<h3 id=\"\">前言</h3>\n\n<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href=\"http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-jiang-xue-xi-bi-ji-2/\">第十一講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">熱身回顧一下</h3>\n\n<p>在上一講中，我們將線性分類的模型擴展到可以進行多元分類，擴展的方法很直覺，就是使用 One vs One 及 One vs All 兩種分解成二元分類的方式來做到多元分類。在這一講中將講解如何讓線性模型擴展到非線性模型，讓我們可以將機器學習演算法的複雜度提高以解決更複雜的問題，並說明非線性模型會有什麼影響。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-1.png\">\n</p>\n\n<h3 id=\"\">線性假設</h3>\n\n<p>之前的演算法目前都是基於線性的假設之下去找出分類最好的線，但這在線性不可分的情況下，會得到較大的 Ein，理論上較大的 Ein 未來 Eout 效果也會不佳，有沒有辦法讓我們演算法得出的線不一定要是一條直線以得到更佳的 Ein 來增加學習效果呢？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-2.png\">\n</p>\n\n<h3 id=\"\">圈圈可分</h3>\n\n<p>我們從肉眼觀察可以發現右邊的資料點是一個「圈圈可分」的情況，所以我們要解這個問題，我們可以基於圈圈可分的情況去推導之前所有的演算法，但這樣有點麻煩，沒有沒其他更通用的方法？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-3.png\">\n</p>\n\n<h3 id=\"\">比較圈圈可分及線性可分</h3>\n\n<p>為了讓演算法可以通用，我們會思考，如果我們可以讓圈圈可分轉換到一個空間之後變成線性可分，那就太好了。我們比較一下圈圈可分及線性可分，當我們將 Xn 圈圈可分的資料點，透過一個圈圈方程式轉換到 Z 空間，這時資料點 Zn 在 Z 空間就是一個線性可分的情況，不過在 Z 空間線性可分，在 X 空間不一定會是圈圈可分。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-4.png\">\n</p>\n\n<h3 id=\"z\">Z 空間的線性假設</h3>\n\n<p>觀察在 Z 空間的線性方程式，不同的參數在 X 空間會是不同的曲線，有可能是圓、橢圓、雙曲線等等，因此我們了解在 Z 空間的線會是 X 空間的二次曲線。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-5.png\">\n</p>\n\n<h3 id=\"\">一般化二次假設</h3>\n\n<p>我們剛剛是使用 x0, x1^2, X2^2 來簡化理解這個問題，現在將問題更一般化，將原本的 xn 用 Phi 二次展開來一般化剛剛個問題，這樣的 Z 空間學習出來的線性方程式在 X 空間就不一定會是以原點為中心，這樣所有的二次曲線都有辦法在 Z 空間學習到了，而起原本在 X 空間的線性方程式也會包含在按次曲線中。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-6.png\">\n</p>\n\n<h3 id=\"\">好的二次空間假設</h3>\n\n<p>所以原本的問題可以透過這樣的非線性轉換到二次 Z 空間進行機器學習演算法，在 Z 空間的線性可分就可以對應到 X 空間的二次曲線可分。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-7.png\">\n</p>\n\n<h3 id=\"\">非線性轉換的學習步驟</h3>\n\n<p>了解了這樣的思路之後，非線性轉換的學習步驟就是先將資料點透過 Phi 轉換到非線性空間，然後使用之前學過的線性演算法進行機器學習，由於學習出來的 Z 空間線性方程式不一定能轉回 X 空間，我們實務的上做法是將測試資料透過 Phi 轉換到 Z 空間，再進行預測。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-8.png\">\n</p>\n\n<h3 id=\"\">非線性模型是潘朵拉的盒子</h3>\n\n<p>學會了特徵轉換使用非線性模型就像打開了潘朵拉的盒子，我們可以任意的將資料轉換到更高維的空間來進行機器學習，如此可以得到更低的 Ein，但這對機器學習效果不一定好，因此要慎用。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-9.png\">\n</p>\n\n<h3 id=\"\">代價一：更高的計算及儲存代價</h3>\n\n<p>我們可以將資料點進行 Q 次轉換，這樣原本的資料點會有 0 次項、1 次項、2 次項 .... Q 次項，每筆資料的維度都增加了，理所當然計算量及儲存量也都變高了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-10.png\">\n</p>\n\n<h3 id=\"\">代價二：更高的模型複雜度</h3>\n\n<p>進行了 Q 次轉換後，資料的為度更高了，理論上 VC dimention 也跟著增加了， VC dimention 代表著模型複雜度，在之前的課程中我們知道較複雜的模型會讓 Eout 變高。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-11.png\">\n</p>\n\n<h3 id=\"\">可能會面臨的問題</h3>\n\n<p>我們用下圖來說明我們可能會面臨的問題，左圖雖然不是線性可分，但一眼看來其實也是一個不錯的結果，右圖可以得到完美的 Ein，但會覺得有點過頭，我們會面臨的問題就是要如何抉擇左邊或右邊？較高的 Q 次轉換會造成 Eout 與 Ein 不會很接近，但可以得到較小的 Ein，較低的 Q 次轉換可以保證 Eout 跟 Ein 很接近，但 Ein 的效果可能不好，怎麼選 Q 呢？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-12.png\">\n</p>\n\n<h3 id=\"\">用看的選有風險</h3>\n\n<p>就上述的例子我們可以用圖示來觀察，而使用較低的 Q 次轉換，但如果為度很高，我們是無法畫成圖來看的，而且用看圖的方式，我們可能會不小心用我們的人工運算，直接加上圈圈方程式來降低 VC dimention，這也可能會造成 Eout 效果不佳，因為 VC dimention 是經過人腦降低的，會讓我們低估 VC dimention 複雜度，所以我們應該要避免用看資料的方式來調整演算法。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-13.png\">\n</p>\n\n<h3 id=\"\">多項式結構化</h3>\n\n<p>我們將 Q 次轉換用下面的式子及圖示結構化，我們可以發現 0 次轉換的假設會包含在 1 次轉換的假設中，1 次轉換的假設會包含在 2 次轉換的假設中，一直到 Q 次轉換這樣的結構，表示成 H0, H1, H2, ...., Hq。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-14.png\">\n</p>\n\n<h3 id=\"\">假設集合結構化</h3>\n\n<p>從 H0 包含於 H1、H1 包含於 H2 .... Hq-1 包含於 Hq 這樣的關係中，我們可以推論 d<em>vc(H0) &lt;= d</em>vc(H1) ... &lt;= d<em>vc(Hq)，而在理論上 Ein(g0) >= Ein(g1) ... >= Ein(gq)，從之前學過的理論可知，out of sample Eout 在 d</em>vc 很高、Ein 很低的情況下，不一定會是最低點。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-15.png\">\n</p>\n\n<h3 id=\"\">線性模型第一優先</h3>\n\n<p>所以依據理論，我們不該為了追求 Ein 低、訓練效果好來做機器學習，這樣是一種自我欺騙，我們要做的應該是使用線性模型為第一優先，如果 Ein 很差，則考慮做二次轉換，慢慢升高 d_vc，而不是一步登天。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-16.png\">\n</p>\n\n<h3 id=\"\">總結</h3>\n\n<p>在這一講中我們打開了潘朵拉的盒子，學會了使用非線性轉換來得到更好的 Ein，但這會付出一些代價，會讓計算量增加、資料儲存量增加，若一次升高太多模型複雜度，還會造成學習效果不佳，Eout 會比 Ein 高很多，所以要慎用。最好的學習方式就是先從線性模型開始，然後再慢慢升高模型複雜度。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-12-17.png\">\n</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-02-14T12:53:14.000Z","created_by":1,"updated_at":"2016-08-09T05:36:40.000Z","updated_by":1,"published_at":"2016-02-14T15:18:51.000Z","published_by":1},{"id":75,"uuid":"15306ded-bb73-40ea-9280-7156939cb7f5","title":"Test Mail","slug":"test-mail","markdown":"```\nMAIL_DRIVER=log\n```\n\nAuthTest.php\n```\n\tpublic function setUp() {\n\n        parent::setUp();\n\n        Mail::getSwiftMailer()\n            ->registerPlugin(new TestingMailEventListener);\n\n    }\n```\n\nAuthTest.php\n```\nclass TestingMailEventListener implements Swift_Events_EventListener\n{\n    public function beforeSendPerformed($event)\n    {\n        $message = $event->getMessage();\n        dd($message);\n    }\n}\n```\n\nAuthTest.php\n```\n\tpublic function testSendMail() {\n        Mail::raw('Hello World', function($message) {\n            $message->to('foo@bar.com');\n            $message->from('bar@foo.com');\n        });\n    }\n```\n\n```\ndd(get_class_methods($message));\n```\n\n\n```\n\tprotected $mail_messages = [];\n\n\tpublic function setUp() {\n\n        parent::setUp();\n\n        Mail::getSwiftMailer()\n            ->registerPlugin(new TestingMailEventListener($this));\n\n    }\n```\n\n```\n\tpublic function addMailEventMessage(Swift_Message $mail_message)\n    {\n        $this->mail_messages[] = $mail_message;\n    }\n```\n\n```\nclass TestingMailEventListener implements Swift_Events_EventListener\n{\n\n    protected $test_case;\n\n    public function __construct($test_case)\n    {\n        $this->test_case = $test_case;\n    }\n\n    public function beforeSendPerformed($event)\n    {\n        $this->test_case->addMailEventMessage($event->getMessage());\n    }\n}\n```\n\nMialTester.php\n```\n<?php\n\ntrait MailTester\n{\n\n    protected $mail_messages = [];\n\n    /** @before **/\n    public function setUpMailTester()\n    {\n\n        Mail::getSwiftMailer()\n            ->registerPlugin(new TestingMailEventListener($this));\n\n    }\n\n    public function addMailEventMessage(Swift_Message $mail_message)\n    {\n        $this->mail_messages[] = $mail_message;\n    }\n\n}\n\nclass TestingMailEventListener implements Swift_Events_EventListener\n{\n\n    protected $test_case;\n\n    public function __construct($test_case)\n    {\n        $this->test_case = $test_case;\n    }\n\n    public function beforeSendPerformed($event)\n    {\n        $this->test_case->addMailEventMessage($event->getMessage());\n    }\n}\n\n```\n\n```\n\t\"autoload-dev\": {\n        \"classmap\": [\n            \"tests/TestCase.php\"\n        ],\n        \"files\": [\n            \"tests/MailTester.php\"\n        ]\n    },\n```\n\n```\ncomposer dump-autoload\n```\n\n```\n\tpublic function seeEmailTo($email_to)\n    {\n        $mail_message = end($this->mail_messages);\n        $this->assertArrayHasKey($email_to, $mail_message->getTo());\n    }\n```\n\n```\n<div class=\"form-group{{ $errors->has('city') ? ' has-error' : '' }}\">\n    <label class=\"col-md-4 control-label\">City</label>\n\n    <div class=\"col-md-6\">\n        <input type=\"text\" class=\"form-control\" name=\"city\" value=\"{{ old('city') }}\">\n\n        @if ($errors->has('city'))\n            <span class=\"help-block\">\n                <strong>{{ $errors->first('city') }}</strong>\n            </span>\n        @endif\n    </div>\n</div>\n```","html":"<pre><code>MAIL_DRIVER=log  \n</code></pre>\n\n<p>AuthTest.php  </p>\n\n<pre><code>    public function setUp() {\n\n        parent::setUp();\n\n        Mail::getSwiftMailer()\n            -&gt;registerPlugin(new TestingMailEventListener);\n\n    }\n</code></pre>\n\n<p>AuthTest.php  </p>\n\n<pre><code>class TestingMailEventListener implements Swift_Events_EventListener  \n{\n    public function beforeSendPerformed($event)\n    {\n        $message = $event-&gt;getMessage();\n        dd($message);\n    }\n}\n</code></pre>\n\n<p>AuthTest.php  </p>\n\n<pre><code>    public function testSendMail() {\n        Mail::raw('Hello World', function($message) {\n            $message-&gt;to('foo@bar.com');\n            $message-&gt;from('bar@foo.com');\n        });\n    }\n</code></pre>\n\n<pre><code>dd(get_class_methods($message));  \n</code></pre>\n\n<pre><code>    protected $mail_messages = [];\n\n    public function setUp() {\n\n        parent::setUp();\n\n        Mail::getSwiftMailer()\n            -&gt;registerPlugin(new TestingMailEventListener($this));\n\n    }\n</code></pre>\n\n<pre><code>    public function addMailEventMessage(Swift_Message $mail_message)\n    {\n        $this-&gt;mail_messages[] = $mail_message;\n    }\n</code></pre>\n\n<pre><code>class TestingMailEventListener implements Swift_Events_EventListener  \n{\n\n    protected $test_case;\n\n    public function __construct($test_case)\n    {\n        $this-&gt;test_case = $test_case;\n    }\n\n    public function beforeSendPerformed($event)\n    {\n        $this-&gt;test_case-&gt;addMailEventMessage($event-&gt;getMessage());\n    }\n}\n</code></pre>\n\n<p>MialTester.php  </p>\n\n<pre><code>&lt;?php\n\ntrait MailTester  \n{\n\n    protected $mail_messages = [];\n\n    /** @before **/\n    public function setUpMailTester()\n    {\n\n        Mail::getSwiftMailer()\n            -&gt;registerPlugin(new TestingMailEventListener($this));\n\n    }\n\n    public function addMailEventMessage(Swift_Message $mail_message)\n    {\n        $this-&gt;mail_messages[] = $mail_message;\n    }\n\n}\n\nclass TestingMailEventListener implements Swift_Events_EventListener  \n{\n\n    protected $test_case;\n\n    public function __construct($test_case)\n    {\n        $this-&gt;test_case = $test_case;\n    }\n\n    public function beforeSendPerformed($event)\n    {\n        $this-&gt;test_case-&gt;addMailEventMessage($event-&gt;getMessage());\n    }\n}\n</code></pre>\n\n<pre><code>    \"autoload-dev\": {\n        \"classmap\": [\n            \"tests/TestCase.php\"\n        ],\n        \"files\": [\n            \"tests/MailTester.php\"\n        ]\n    },\n</code></pre>\n\n<pre><code>composer dump-autoload  \n</code></pre>\n\n<pre><code>    public function seeEmailTo($email_to)\n    {\n        $mail_message = end($this-&gt;mail_messages);\n        $this-&gt;assertArrayHasKey($email_to, $mail_message-&gt;getTo());\n    }\n</code></pre>\n\n<pre><code>&lt;div class=\"form-group{{ $errors-&gt;has('city') ? ' has-error' : '' }}\"&gt;  \n    &lt;label class=\"col-md-4 control-label\"&gt;City&lt;/label&gt;\n\n    &lt;div class=\"col-md-6\"&gt;\n        &lt;input type=\"text\" class=\"form-control\" name=\"city\" value=\"{{ old('city') }}\"&gt;\n\n        @if ($errors-&gt;has('city'))\n            &lt;span class=\"help-block\"&gt;\n                &lt;strong&gt;{{ $errors-&gt;first('city') }}&lt;/strong&gt;\n            &lt;/span&gt;\n        @endif\n    &lt;/div&gt;\n&lt;/div&gt;  \n</code></pre>","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-02-23T13:11:56.000Z","created_by":1,"updated_at":"2016-02-24T15:28:03.000Z","updated_by":1,"published_at":null,"published_by":null},{"id":76,"uuid":"5fdb65cf-9ae4-4dad-ba1d-66a679760cf0","title":"林軒田教授機器學習基石 Machine Learning Foundations 第十三講學習筆記","slug":"lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-san-jiang-xue-xi-bi-ji","markdown":"### 前言\n\n本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 [第十二講](http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-er-jiang-xue-xi-bi-ji/) 的碼農們，我建議可以先回頭去讀一下再回來喔！\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 熱身回顧一下\n\n在上一講中，我們了解了如何使用非線性轉換來讓我們的機器學習演算法可以學習出非線性分類模型，也了解了這樣的方法可能會讓模型複雜度變高，造成 Overfitting 使未來 Eout 效果不佳的情況，所以要慎用此方法。在這一講中將更進一步說明什麼是 Overfitting，並講解如何避免 Overfitting。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-13-1.png\">\n</p>\n\n### Bad Generalization 無法舉一反三\n\n我們來看個例子，現在我們使用一個二次多項式加上一點 noise 產生資料點，由於有 noise，我們是無法學習出一個二次多項式讓 Ein 為 0。但如果我們使用了非線性轉換到四次多項式來進行學習，我們可以找到一個 w 讓 Ein 為 0，看起來可能會像是圖中的紅線。但可想而知紅線的 Eout 可能會非常高，如此我們的機器學習是失敗的，無法舉一反三。 \n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-13-2.png\">\n</p>\n\n### Overfitting 過度優化\n\n其實這就是一種過度優化，當我們發現 Eout - Ein 很大時，就是發生了 Bad Generalization。\n\n我們觀察圖中的 dvc\\*，當 dvc\\* 越來越高時，Ein 會下降，但 Eout 會上升，這時就是產生了 Overfitting。\n\n當 dvc\\* 往左時，Ein 會上升，Eout 也會上升，這時就是產生了 Underfitting。\n\nUnderfitting 不會很常發生，因為我們會追求低的 Ein，因此可以避免 Underfitting，但 Overfitting 卻常常發生，因為追求低 Ein，會讓我們不小心進入陷阱。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-13-3.png\">\n</p>\n\n### Case Study 1/2\n\n我們再來看一個例子，我們現在有兩個 target function，一個是 10 次多項式加上一些 noise，一個是 50 次多項式然後沒有 noise，現在我們使用一個 2 次多項式及一個 10 次多項式來逼近學習這兩個 target function，以觀察 Overfitting 現象。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-13-4.png\">\n</p>\n\n### Case Study 2/2\n\n結果我們發現，當我們將學習模型由 2 次多項式轉換到 10 次多項式時，無論是在 10 次或 50 次多項式的資料點都會產生 Overfitting，Ein 變小了，但是 Eout 卻變得非常大！\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-13-5.png\">\n</p>\n\n### Learning Curve\n\n我們將 Ein 及 Eout 的變化畫成圖示，我們可以看出 10 次多項式的模型在資料點 N 趨近于無限大時，的確可以得到很低的 Eout，但在資料點不夠多的情況下，Eout 卻會比原來的 2 次多項式模型高很多，圖中灰色的區域會很容易產生 Overfitting。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-13-6.png\">\n</p>\n\n### Noise 的影響\n\n會產生 Overfitting 其實就是 Noise 的影響，尤其當資料點不夠多的情況下影響會很大，在有 Noise 的情況下，複雜的學習模型會去模擬 Noise，因此也就會造成未來在做預測時反而會不準確。所以在有 Noise 的情況下，有時簡單的模型反而會有好的效果。\n\nNoise 除了我們一般所知的 stochastic noise 之外，還有另一種 Noise，當我們要學習的模型越複雜時，這其實對我們的學習演算法也是一種 Noise，這就是 deterministic noise。我們將這兩個 Noise 與 Data N 的數量畫成圖來觀察 Overfitting，我們可以得到四個結論：\n\n1. 當 data 越少時，Overfitting 越容易發生\n2. 當 stochastic noise 越大時，Overfitting 越容易發生\n3. 當 deterministic noist 越大時，Overfitting 越容易發生\n4. 當使用的學習模型越複雜時，因為他會模擬 Noist，Overfitting 越容易發生\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-13-8.png\">\n</p>\n\n### 避免 Overfitting 的方法\n\n我們有幾個方法來避免 Overfitting：\n\n1. 先從簡單的模型開始學習，再慢慢使用複雜的模型，這在上一講有說過了。\n2. 使用資料清洗（Data Cleaning/Pruning），將錯誤的 label 修正，或直接刪除錯誤的數據。\n3. 製造資料（Data Hinting），使用合理的方法將原來手的的資料變得更多，比如在數字識別的這個問題將已有的數字透過平移、旋轉來製造出更多資料。\n4. 正規化（Regularization），下 14 講的主題。\n5. 驗證（Validation），第 15 講的主題。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-13-9.png\">\n</p>\n\n### 總結\n\n在這一講中，我們更了解了什麼是 Overfitting，也觀察到 Overfitting 是很容易發生的，也介紹了一些避免 Overfitting 的方法。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-13-12.png\">\n</p>","html":"<h3 id=\"\">前言</h3>\n\n<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href=\"http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-er-jiang-xue-xi-bi-ji/\">第十二講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">熱身回顧一下</h3>\n\n<p>在上一講中，我們了解了如何使用非線性轉換來讓我們的機器學習演算法可以學習出非線性分類模型，也了解了這樣的方法可能會讓模型複雜度變高，造成 Overfitting 使未來 Eout 效果不佳的情況，所以要慎用此方法。在這一講中將更進一步說明什麼是 Overfitting，並講解如何避免 Overfitting。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-13-1.png\">\n</p>\n\n<h3 id=\"badgeneralization\">Bad Generalization 無法舉一反三</h3>\n\n<p>我們來看個例子，現在我們使用一個二次多項式加上一點 noise 產生資料點，由於有 noise，我們是無法學習出一個二次多項式讓 Ein 為 0。但如果我們使用了非線性轉換到四次多項式來進行學習，我們可以找到一個 w 讓 Ein 為 0，看起來可能會像是圖中的紅線。但可想而知紅線的 Eout 可能會非常高，如此我們的機器學習是失敗的，無法舉一反三。 </p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-13-2.png\">\n</p>\n\n<h3 id=\"overfitting\">Overfitting 過度優化</h3>\n\n<p>其實這就是一種過度優化，當我們發現 Eout - Ein 很大時，就是發生了 Bad Generalization。</p>\n\n<p>我們觀察圖中的 dvc*，當 dvc* 越來越高時，Ein 會下降，但 Eout 會上升，這時就是產生了 Overfitting。</p>\n\n<p>當 dvc* 往左時，Ein 會上升，Eout 也會上升，這時就是產生了 Underfitting。</p>\n\n<p>Underfitting 不會很常發生，因為我們會追求低的 Ein，因此可以避免 Underfitting，但 Overfitting 卻常常發生，因為追求低 Ein，會讓我們不小心進入陷阱。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-13-3.png\">\n</p>\n\n<h3 id=\"casestudy12\">Case Study 1/2</h3>\n\n<p>我們再來看一個例子，我們現在有兩個 target function，一個是 10 次多項式加上一些 noise，一個是 50 次多項式然後沒有 noise，現在我們使用一個 2 次多項式及一個 10 次多項式來逼近學習這兩個 target function，以觀察 Overfitting 現象。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-13-4.png\">\n</p>\n\n<h3 id=\"casestudy22\">Case Study 2/2</h3>\n\n<p>結果我們發現，當我們將學習模型由 2 次多項式轉換到 10 次多項式時，無論是在 10 次或 50 次多項式的資料點都會產生 Overfitting，Ein 變小了，但是 Eout 卻變得非常大！</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-13-5.png\">\n</p>\n\n<h3 id=\"learningcurve\">Learning Curve</h3>\n\n<p>我們將 Ein 及 Eout 的變化畫成圖示，我們可以看出 10 次多項式的模型在資料點 N 趨近于無限大時，的確可以得到很低的 Eout，但在資料點不夠多的情況下，Eout 卻會比原來的 2 次多項式模型高很多，圖中灰色的區域會很容易產生 Overfitting。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-13-6.png\">\n</p>\n\n<h3 id=\"noise\">Noise 的影響</h3>\n\n<p>會產生 Overfitting 其實就是 Noise 的影響，尤其當資料點不夠多的情況下影響會很大，在有 Noise 的情況下，複雜的學習模型會去模擬 Noise，因此也就會造成未來在做預測時反而會不準確。所以在有 Noise 的情況下，有時簡單的模型反而會有好的效果。</p>\n\n<p>Noise 除了我們一般所知的 stochastic noise 之外，還有另一種 Noise，當我們要學習的模型越複雜時，這其實對我們的學習演算法也是一種 Noise，這就是 deterministic noise。我們將這兩個 Noise 與 Data N 的數量畫成圖來觀察 Overfitting，我們可以得到四個結論：</p>\n\n<ol>\n<li>當 data 越少時，Overfitting 越容易發生  </li>\n<li>當 stochastic noise 越大時，Overfitting 越容易發生  </li>\n<li>當 deterministic noist 越大時，Overfitting 越容易發生  </li>\n<li>當使用的學習模型越複雜時，因為他會模擬 Noist，Overfitting 越容易發生</li>\n</ol>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-13-8.png\">\n</p>\n\n<h3 id=\"overfitting\">避免 Overfitting 的方法</h3>\n\n<p>我們有幾個方法來避免 Overfitting：</p>\n\n<ol>\n<li>先從簡單的模型開始學習，再慢慢使用複雜的模型，這在上一講有說過了。  </li>\n<li>使用資料清洗（Data Cleaning/Pruning），將錯誤的 label 修正，或直接刪除錯誤的數據。  </li>\n<li>製造資料（Data Hinting），使用合理的方法將原來手的的資料變得更多，比如在數字識別的這個問題將已有的數字透過平移、旋轉來製造出更多資料。  </li>\n<li>正規化（Regularization），下 14 講的主題。  </li>\n<li>驗證（Validation），第 15 講的主題。</li>\n</ol>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-13-9.png\">\n</p>\n\n<h3 id=\"\">總結</h3>\n\n<p>在這一講中，我們更了解了什麼是 Overfitting，也觀察到 Overfitting 是很容易發生的，也介紹了一些避免 Overfitting 的方法。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-13-12.png\">\n</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-02-28T18:27:22.000Z","created_by":1,"updated_at":"2016-08-09T05:37:04.000Z","updated_by":1,"published_at":"2016-02-29T18:01:32.000Z","published_by":1},{"id":77,"uuid":"8998a6a6-6338-4dbd-9cf3-6f4bddd8bdbd","title":"林軒田教授機器學習基石 Machine Learning Foundations 第十四講學習筆記","slug":"lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-si-jiang-xue-xi-bi-ji","markdown":"### 前言\n\n本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 [第十三講](http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-san-jiang-xue-xi-bi-ji/) 的碼農們，我建議可以先回頭去讀一下再回來喔！\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 熱身回顧一下\n\n在上一講中，我們更進一步的了解了什麼是 Overfitting 是因為 stochastic noise 及 deterministic noise 而造成，與簡易地介紹了幾個簡單的方法來避免 overfitting，這一講將介紹一個比較內行的方法來避免 overfitting，這個方法叫做正規化（Regularization）。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-14-1.png\">\n</p>\n\n### 正規化\n\n正規化（Regularization）的想法，就是我們了解 overfitting 發生時，有可能是因為我們訓練的假設模型本身就過於複雜，因此我們能不能讓複雜的假設模型退回至簡單的假設模型呢？這個退回去的方法就是正規化。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-14-2.png\">\n</p>\n\n### 退回簡單模型就像是加了限制\n\n假設我們現在是一個 10 次多項式的假設集合，我們想要退回成為較為簡單的 2 次多項式假設集合，其實可以想成就像是 2 次以上的項的係數都是 0，也就像是我們為求解的過程加上了一些限制，希望 2 次以上的項的係數都是 0。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-14-3.png\">\n</p>\n\n### 使用較鬆的限制\n\n直接將高維的項次設成 0 可能不是一個好方法，通常我們會希望由學習的過程來決定哪些項次要是 0，這樣的得到的學習效果可能會比較好。所以我們的限制就改成，希望不為 0 的係數不超過三個，由機器從資料來學習出最好的 w，這樣可能會得到比較好的結果。而這樣的限制並不是平滑的函數，所以這是一個 NP Hard 的問題。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-14-4.png\">\n</p>\n\n### 換個方式得出較為平滑的限制\n\n所以我們需要換個方式得出較為平滑的限制，這樣在演算法上會比較容易求解，在 Regression 這個問題上，我們可以把限制改為 ||w^2|| <= C 來代表 w 不超過三個係數不為 0，這個含義就像是讓 w 限制在某些值裡面，也許他不一定代表 w 不超過三個係數不為 0，但它可能可以包含，而且 C 的值是一個連續的數，求解上會比較容易。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-14-5.png\">\n</p>\n\n### Regularized Linear Regression\n\n加上 ||w^2|| <= C 這個限制的線性迴歸（Linear Regression）就是正規化線性迴歸（Regularized Linear Regression），如何求解優化這個問題呢？\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-14-6.png\">\n</p>\n\n### 使用 Lagrange Multiplier\n\n讓我們用微觀的角度來看求解優化這個問題，原來沒有限制的時候，我們使用梯度下降法來求解，只需要讓目標函數沿著提度的反方向走，直到梯度為 0。加入了限制之後，這代表 w 需要在一個紅色的球裡面滾動，如圖所示。由圖來看，我們的解應該都是在求的邊界附近，只要梯度與 w 不是平行的，目標函數就可以再向谷底滾動一點點，可以得到更好的解。如此往下推，最佳的結果就是梯度與 w_reg 是平行的時候。所以使用梯度下降法解這個問題，就是去求解 w_reg 及 lamda，然後讓 w_reg 與梯度平行即為最佳解。（而這個 lamda 就是 Lagrange Multiplier）\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-14-7.png\">\n</p>\n\n### Ridge Regression\n\n有了上式的概念之後，我們只要知道 lamda，就可以很容易地求出 w_reg。這個式子經過整理之後，能夠直接得出最佳解，這個方法在統計上就稱為是 \nRidge Regression。\n\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-14-8.png\">\n</p>\n\n### 擴增錯誤\n\n我們將上式進行積分，可以得到下圖中的式子，在意義上我們要優化的除了 Ein 之外，也要考慮到擴增出來的錯誤。由於 WTW 是正的，lambda 及 N 也是正的，因此在優化求解的時候可以保證 WTW 不能太大。這個方法可以對模型複雜度進行懲罰，讓 Ein(W) 在解空間受到了限制。給定 C 跟給定 lamda 對我們來說可能是一樣的，使用這個角度所推導出來的式子對我們來說更容易求解。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-14-9.png\">\n</p>\n\n### 如何求 lambda\n\n現在就剩下，改如何給定 lambda 呢？總歸一句話，我們可以做實驗來決定。我們只要知道 lambda 的性質就好，選越大的 lambda 代表懲罰越多，這就代表 w 長度值越小，這其實就就代表 C 越小（限制越多）。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-14-10.png\">\n</p>\n\n### Legendre Polynomials\n\n有一個小細節要注意，之前學過將空間轉換到高維度以求得更小 Ein 的方法，都可以配合正規化來避免 overfitting。不過單純轉換到高次，由於高次的維度 xi 值乘很多次，Regularizer 可能會過度懲罰這些高次項，因此我們需要使用 Legendre Polynomials 來進行高次轉換，讓高次項不會在訓練過程中被過度懲罰。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-14-11.png\">\n</p>\n\n### 如何選擇最好的 lambda\n\n如何選擇最好的 lambda？剛剛說要透過實驗，那麼怎麼做實驗呢？這就是下一次的課程了。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-14-16.png\">\n</p>\n\n### 總結\n\n在這一章我們學會了如何使用正規化這個方法來避免 overfitting，在核心概念上就像為解空間加上了限制，也因此可以避免過度優化。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-14-17.png\">\n</p>","html":"<h3 id=\"\">前言</h3>\n\n<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href=\"http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-san-jiang-xue-xi-bi-ji/\">第十三講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">熱身回顧一下</h3>\n\n<p>在上一講中，我們更進一步的了解了什麼是 Overfitting 是因為 stochastic noise 及 deterministic noise 而造成，與簡易地介紹了幾個簡單的方法來避免 overfitting，這一講將介紹一個比較內行的方法來避免 overfitting，這個方法叫做正規化（Regularization）。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-14-1.png\">\n</p>\n\n<h3 id=\"\">正規化</h3>\n\n<p>正規化（Regularization）的想法，就是我們了解 overfitting 發生時，有可能是因為我們訓練的假設模型本身就過於複雜，因此我們能不能讓複雜的假設模型退回至簡單的假設模型呢？這個退回去的方法就是正規化。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-14-2.png\">\n</p>\n\n<h3 id=\"\">退回簡單模型就像是加了限制</h3>\n\n<p>假設我們現在是一個 10 次多項式的假設集合，我們想要退回成為較為簡單的 2 次多項式假設集合，其實可以想成就像是 2 次以上的項的係數都是 0，也就像是我們為求解的過程加上了一些限制，希望 2 次以上的項的係數都是 0。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-14-3.png\">\n</p>\n\n<h3 id=\"\">使用較鬆的限制</h3>\n\n<p>直接將高維的項次設成 0 可能不是一個好方法，通常我們會希望由學習的過程來決定哪些項次要是 0，這樣的得到的學習效果可能會比較好。所以我們的限制就改成，希望不為 0 的係數不超過三個，由機器從資料來學習出最好的 w，這樣可能會得到比較好的結果。而這樣的限制並不是平滑的函數，所以這是一個 NP Hard 的問題。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-14-4.png\">\n</p>\n\n<h3 id=\"\">換個方式得出較為平滑的限制</h3>\n\n<p>所以我們需要換個方式得出較為平滑的限制，這樣在演算法上會比較容易求解，在 Regression 這個問題上，我們可以把限制改為 ||w^2|| &lt;= C 來代表 w 不超過三個係數不為 0，這個含義就像是讓 w 限制在某些值裡面，也許他不一定代表 w 不超過三個係數不為 0，但它可能可以包含，而且 C 的值是一個連續的數，求解上會比較容易。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-14-5.png\">\n</p>\n\n<h3 id=\"regularizedlinearregression\">Regularized Linear Regression</h3>\n\n<p>加上 ||w^2|| &lt;= C 這個限制的線性迴歸（Linear Regression）就是正規化線性迴歸（Regularized Linear Regression），如何求解優化這個問題呢？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-14-6.png\">\n</p>\n\n<h3 id=\"lagrangemultiplier\">使用 Lagrange Multiplier</h3>\n\n<p>讓我們用微觀的角度來看求解優化這個問題，原來沒有限制的時候，我們使用梯度下降法來求解，只需要讓目標函數沿著提度的反方向走，直到梯度為 0。加入了限制之後，這代表 w 需要在一個紅色的球裡面滾動，如圖所示。由圖來看，我們的解應該都是在求的邊界附近，只要梯度與 w 不是平行的，目標函數就可以再向谷底滾動一點點，可以得到更好的解。如此往下推，最佳的結果就是梯度與 w<em>reg 是平行的時候。所以使用梯度下降法解這個問題，就是去求解 w</em>reg 及 lamda，然後讓 w_reg 與梯度平行即為最佳解。（而這個 lamda 就是 Lagrange Multiplier）</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-14-7.png\">\n</p>\n\n<h3 id=\"ridgeregression\">Ridge Regression</h3>\n\n<p>有了上式的概念之後，我們只要知道 lamda，就可以很容易地求出 w_reg。這個式子經過整理之後，能夠直接得出最佳解，這個方法在統計上就稱為是 \nRidge Regression。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-14-8.png\">\n</p>\n\n<h3 id=\"\">擴增錯誤</h3>\n\n<p>我們將上式進行積分，可以得到下圖中的式子，在意義上我們要優化的除了 Ein 之外，也要考慮到擴增出來的錯誤。由於 WTW 是正的，lambda 及 N 也是正的，因此在優化求解的時候可以保證 WTW 不能太大。這個方法可以對模型複雜度進行懲罰，讓 Ein(W) 在解空間受到了限制。給定 C 跟給定 lamda 對我們來說可能是一樣的，使用這個角度所推導出來的式子對我們來說更容易求解。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-14-9.png\">\n</p>\n\n<h3 id=\"lambda\">如何求 lambda</h3>\n\n<p>現在就剩下，改如何給定 lambda 呢？總歸一句話，我們可以做實驗來決定。我們只要知道 lambda 的性質就好，選越大的 lambda 代表懲罰越多，這就代表 w 長度值越小，這其實就就代表 C 越小（限制越多）。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-14-10.png\">\n</p>\n\n<h3 id=\"legendrepolynomials\">Legendre Polynomials</h3>\n\n<p>有一個小細節要注意，之前學過將空間轉換到高維度以求得更小 Ein 的方法，都可以配合正規化來避免 overfitting。不過單純轉換到高次，由於高次的維度 xi 值乘很多次，Regularizer 可能會過度懲罰這些高次項，因此我們需要使用 Legendre Polynomials 來進行高次轉換，讓高次項不會在訓練過程中被過度懲罰。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-14-11.png\">\n</p>\n\n<h3 id=\"lambda\">如何選擇最好的 lambda</h3>\n\n<p>如何選擇最好的 lambda？剛剛說要透過實驗，那麼怎麼做實驗呢？這就是下一次的課程了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-14-16.png\">\n</p>\n\n<h3 id=\"\">總結</h3>\n\n<p>在這一章我們學會了如何使用正規化這個方法來避免 overfitting，在核心概念上就像為解空間加上了限制，也因此可以避免過度優化。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-14-17.png\">\n</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-03-01T19:30:32.000Z","created_by":1,"updated_at":"2016-08-09T05:37:31.000Z","updated_by":1,"published_at":"2016-03-15T14:06:26.000Z","published_by":1},{"id":78,"uuid":"34b15720-1117-4248-a9bd-24d5b542d828","title":"林軒田教授機器學習基石 Machine Learning Foundations 第十五講學習筆記","slug":"lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-wu-jiang-xue-xi-bi-ji","markdown":"### 前言\n\n本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 [第十四講](http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-si-jiang-xue-xi-bi-ji/) 的碼農們，我建議可以先回頭去讀一下再回來喔！\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 熱身回顧一下\n\n在上一講中，我們進一步了解了如何透過正規化（Regularization）來避免 Overfitting，但正規化這個方法會有一個參數 lambda，這個 lambda 我們又要如何選擇呢？在這一講將會學習到使用 Validation 這個方法來幫助我們選擇比較好的 lambda 值，同理，這個方法也可以幫助我們用於選擇各種不同的學習模型。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-1.png?1\">\n</p>\n\n### 許多學習模型可以使用\n\n經過了前面 14 講，我們已經學會了許多學習模型，在演算法上我們有 PLA、Pocket、Linear Regression、Logistic Regression 可以做選擇；然後在模型學習的過程中，我們可以指定演算法要經過幾次的學習，每次學習優化的過程要走多大步；我們也可以有很多種線性轉換的方式將模型轉換到更複雜的空間來進行學習；如果模型太過複雜了，我們也有很多種正規化的方法來讓模型退回叫簡單的模型，並可透過 lambda 這個參數來調整退回的程度。\n\n我們可以任意組合，但組合完之後我們要怎麼判斷哪個組合未來在做預測時效果會比較好呢？\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-2.png\">\n</p>\n\n### 用 Ein 來做選擇\n\n如果我們用 Ein 來做選擇，那就永遠會選擇到比較複雜的模型，這在上一講中我們已經知道這很可能會有 Overfitting 發生。所以用 Ein 來做選擇是很危險的。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-3.png\">\n</p>\n\n### 用 Etest 來做選擇\n\n使用 Etest 來做選擇，基本上理論上是可行的，但 Etest 實際在我們訓練的過程中是不能拿來用的，直接拿 Etest 來幫助我們選擇模型其實是一種作弊行為。所以用 Etest 來做選擇也是不可行的。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-4.png\">\n</p>\n\n### 引進 Eval\n\n既然用 Ein 或 Etest 來做選擇是不可行的，那如果我們把我們手中的資料 D 保留一份下來作為 Dval，然後在訓練的過程中都不使用 Dval，等訓練完之後，在挑選各種模型的時候再用 Dval 來做選擇，那這樣會是一個比較安全的做法。\n\n這有點像是我們在練習時，會把一些練習題留下來，等考試之前再驗證看看自己的成果如何，機器學習也用上了這個概念。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-5.png\">\n</p>\n\n### 進一步了解 Dval\n\n所以有了 Dval 的概念之後，我們就會把拿到的資料先分成 Dtrain 及 Dval，Dtrain 用來做訓練得出 g-，Dval 用來做驗證。在霍夫丁不等式的理論中，我們可以保證 Eout(g-) 會跟 Eval(g-) 很接近。所以用 Eval 來選擇最好的模型是可行的。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-6.png\">\n</p>\n\n### 選出 Eval 表現最好的模型\n\n所以現在當我們有很多個模型需要做選擇時，每個會用訓練資料先得出各自最好的 g-，然後我們再用驗證資料計算 Eval，Eval 表現最好的模型就是我們要的。不過 g- 使用的訓練資料較少，也因此未來的表現也可能因為訓練資料少而受影響。所以我們選出了最好的模型之後，會再將所有的資料使用進去訓練出一個最好的 g。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-7.png\">\n</p>\n\n### 驗證資料的影響\n\n我們用一些實驗來看驗證資料的影響，用 Ein 來選的話，Eout 就會是上面那條黑實線，因為 Ein 永遠會選擇最複雜的模型。然後用 Etest 來選模型，當然會得到最好的 Eout 值，但這是作弊。紅色的線代表我們用 g- 直接來做預測，當驗證資料越多的時候，g- 的 Eout 值就變高了，有時甚至比 Ein 選出來的模型還差。藍色的線代表我們用驗證資料選出模型之後，再將全部的資料丟進去訓練出 g，如此得到的效果都會比用 Ein 來選好。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-8.png\">\n</p>\n\n### 那要保留多少驗證資料\n\n從上面的實驗，我們會知道驗證資料的多寡也會影響選出來的模型，所以我們應怎麼選擇保留多少驗證資料呢？實務上我們目前都是用 1/5 的資料作為驗證資料。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-9.png\">\n</p>\n\n### 一個極端的例子\n\n我們從 Eout(g) 及 Eout(g-) 及 Eval(g-) 的關係中觀察，如果我們的驗證資料 k 越小，那 Eout(g) 與 Eout(g-) 就會越接近；但驗證資料 k 越大，那 Eout(g-) 及 Eval(g-) 就會越接近；有沒有方法可以讓 Eout(g) 跟 Eout(g-) 很接近，卻又可以讓 Eval 可以正確地挑出最好的模型。\n\n這個方法就是 leave-one-out cross validataion，每次只保留一個資料作為驗證資料，重複這個過程，直到所有的資料都做過驗證資料，並將所有的 Eval 做平均之後，Eval 最好的那個模型就會是我們想要的模型。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-10.png\">\n</p>\n\n### 用一個簡單的例子來說明 Leave One Out\n\n我們用一個簡單的例子來說明 Leave One Out Cross Validation 選擇模型的效果。假設現在我們有三個點，現在有兩個模型要做選擇，一個是線性模型，一個是常數模型。從下圖我們可以看出常數模型的 Eloocv 會比較小，所以我們就會用常數模型作為我們最後訓練完的結果。\n\n這也告訴我們，當資料很少的時候，有時選擇簡單的模型效果反而會比較好。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-11.png\">\n</p>\n\n### 理論上也有保證\n\n我們剛才都是以實驗上的角度來說明 cross validation 是有效果的，這邊有一個理論推導也可以支持這個結果。推導 Eloovc 的期望值時，最後可以得到跟 Eout(N-1) 平均相等。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-12.png\">\n</p>\n\n### Leave-One-Out Cross Validation 的缺點\n\n雖然 Leave-One-Out Cross Validation 在理論上的確可以讓我們得到的結果很接近真實得到的 Eout，但這個方法也有缺點。如果我們有 1000 個資料，那我們就每個模型都要訓練一千次來計算出各自的 Eloocv，這樣計算量會非常大。\n\n然後觀察下圖 Eloovc 的曲線，我們可以看出隨著 Feature 值的上升，Eloocv 值不會是一個穩定下降再上升的曲線，它會有跳動的情況發生（例如多個模型表現都很好的情況），所以有時會造成選擇的盲點。\n\n因此實務上我們都不會使用 Leave-One-Out Cross Validation 這個方法來選擇模型。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-13.png\">\n</p>\n\n### 分份數的概念\n\nLeave-One-Out Cross Validation 很像是把 D 個資料分成 D 分來做 cross validation，那我們可以將份數變少，比如說分成 5 份或 10 份做 cross validation，這樣就可以大大減少計算量了。\n\n目前實務上都是分 10 份，使用 10-fold cross validation。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-14.png\">\n</p>\n\n### 一些提醒\n\nCross Validaton 是用來做模型的選擇，基本上也會是用風險，因此 validation 表現的結果很好，也不是百分之百未來做預測時效果都會很好。還是要記得觀察未來的預測情況。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-15.png\">\n</p>\n\n### 總結\n\n由於我們已經學會了很多機器學習的方法，有許多地方可以調整我們學習的模型，所以在眾多的學習模型中哪個是最好的，我們也需要有一個方法來幫助我們做選擇，這個方法就是 Cross Validation。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-16.png\">\n</p>","html":"<h3 id=\"\">前言</h3>\n\n<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href=\"http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-si-jiang-xue-xi-bi-ji/\">第十四講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">熱身回顧一下</h3>\n\n<p>在上一講中，我們進一步了解了如何透過正規化（Regularization）來避免 Overfitting，但正規化這個方法會有一個參數 lambda，這個 lambda 我們又要如何選擇呢？在這一講將會學習到使用 Validation 這個方法來幫助我們選擇比較好的 lambda 值，同理，這個方法也可以幫助我們用於選擇各種不同的學習模型。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-1.png?1\">\n</p>\n\n<h3 id=\"\">許多學習模型可以使用</h3>\n\n<p>經過了前面 14 講，我們已經學會了許多學習模型，在演算法上我們有 PLA、Pocket、Linear Regression、Logistic Regression 可以做選擇；然後在模型學習的過程中，我們可以指定演算法要經過幾次的學習，每次學習優化的過程要走多大步；我們也可以有很多種線性轉換的方式將模型轉換到更複雜的空間來進行學習；如果模型太過複雜了，我們也有很多種正規化的方法來讓模型退回叫簡單的模型，並可透過 lambda 這個參數來調整退回的程度。</p>\n\n<p>我們可以任意組合，但組合完之後我們要怎麼判斷哪個組合未來在做預測時效果會比較好呢？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-2.png\">\n</p>\n\n<h3 id=\"ein\">用 Ein 來做選擇</h3>\n\n<p>如果我們用 Ein 來做選擇，那就永遠會選擇到比較複雜的模型，這在上一講中我們已經知道這很可能會有 Overfitting 發生。所以用 Ein 來做選擇是很危險的。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-3.png\">\n</p>\n\n<h3 id=\"etest\">用 Etest 來做選擇</h3>\n\n<p>使用 Etest 來做選擇，基本上理論上是可行的，但 Etest 實際在我們訓練的過程中是不能拿來用的，直接拿 Etest 來幫助我們選擇模型其實是一種作弊行為。所以用 Etest 來做選擇也是不可行的。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-4.png\">\n</p>\n\n<h3 id=\"eval\">引進 Eval</h3>\n\n<p>既然用 Ein 或 Etest 來做選擇是不可行的，那如果我們把我們手中的資料 D 保留一份下來作為 Dval，然後在訓練的過程中都不使用 Dval，等訓練完之後，在挑選各種模型的時候再用 Dval 來做選擇，那這樣會是一個比較安全的做法。</p>\n\n<p>這有點像是我們在練習時，會把一些練習題留下來，等考試之前再驗證看看自己的成果如何，機器學習也用上了這個概念。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-5.png\">\n</p>\n\n<h3 id=\"dval\">進一步了解 Dval</h3>\n\n<p>所以有了 Dval 的概念之後，我們就會把拿到的資料先分成 Dtrain 及 Dval，Dtrain 用來做訓練得出 g-，Dval 用來做驗證。在霍夫丁不等式的理論中，我們可以保證 Eout(g-) 會跟 Eval(g-) 很接近。所以用 Eval 來選擇最好的模型是可行的。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-6.png\">\n</p>\n\n<h3 id=\"eval\">選出 Eval 表現最好的模型</h3>\n\n<p>所以現在當我們有很多個模型需要做選擇時，每個會用訓練資料先得出各自最好的 g-，然後我們再用驗證資料計算 Eval，Eval 表現最好的模型就是我們要的。不過 g- 使用的訓練資料較少，也因此未來的表現也可能因為訓練資料少而受影響。所以我們選出了最好的模型之後，會再將所有的資料使用進去訓練出一個最好的 g。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-7.png\">\n</p>\n\n<h3 id=\"\">驗證資料的影響</h3>\n\n<p>我們用一些實驗來看驗證資料的影響，用 Ein 來選的話，Eout 就會是上面那條黑實線，因為 Ein 永遠會選擇最複雜的模型。然後用 Etest 來選模型，當然會得到最好的 Eout 值，但這是作弊。紅色的線代表我們用 g- 直接來做預測，當驗證資料越多的時候，g- 的 Eout 值就變高了，有時甚至比 Ein 選出來的模型還差。藍色的線代表我們用驗證資料選出模型之後，再將全部的資料丟進去訓練出 g，如此得到的效果都會比用 Ein 來選好。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-8.png\">\n</p>\n\n<h3 id=\"\">那要保留多少驗證資料</h3>\n\n<p>從上面的實驗，我們會知道驗證資料的多寡也會影響選出來的模型，所以我們應怎麼選擇保留多少驗證資料呢？實務上我們目前都是用 1/5 的資料作為驗證資料。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-9.png\">\n</p>\n\n<h3 id=\"\">一個極端的例子</h3>\n\n<p>我們從 Eout(g) 及 Eout(g-) 及 Eval(g-) 的關係中觀察，如果我們的驗證資料 k 越小，那 Eout(g) 與 Eout(g-) 就會越接近；但驗證資料 k 越大，那 Eout(g-) 及 Eval(g-) 就會越接近；有沒有方法可以讓 Eout(g) 跟 Eout(g-) 很接近，卻又可以讓 Eval 可以正確地挑出最好的模型。</p>\n\n<p>這個方法就是 leave-one-out cross validataion，每次只保留一個資料作為驗證資料，重複這個過程，直到所有的資料都做過驗證資料，並將所有的 Eval 做平均之後，Eval 最好的那個模型就會是我們想要的模型。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-10.png\">\n</p>\n\n<h3 id=\"leaveoneout\">用一個簡單的例子來說明 Leave One Out</h3>\n\n<p>我們用一個簡單的例子來說明 Leave One Out Cross Validation 選擇模型的效果。假設現在我們有三個點，現在有兩個模型要做選擇，一個是線性模型，一個是常數模型。從下圖我們可以看出常數模型的 Eloocv 會比較小，所以我們就會用常數模型作為我們最後訓練完的結果。</p>\n\n<p>這也告訴我們，當資料很少的時候，有時選擇簡單的模型效果反而會比較好。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-11.png\">\n</p>\n\n<h3 id=\"\">理論上也有保證</h3>\n\n<p>我們剛才都是以實驗上的角度來說明 cross validation 是有效果的，這邊有一個理論推導也可以支持這個結果。推導 Eloovc 的期望值時，最後可以得到跟 Eout(N-1) 平均相等。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-12.png\">\n</p>\n\n<h3 id=\"leaveoneoutcrossvalidation\">Leave-One-Out Cross Validation 的缺點</h3>\n\n<p>雖然 Leave-One-Out Cross Validation 在理論上的確可以讓我們得到的結果很接近真實得到的 Eout，但這個方法也有缺點。如果我們有 1000 個資料，那我們就每個模型都要訓練一千次來計算出各自的 Eloocv，這樣計算量會非常大。</p>\n\n<p>然後觀察下圖 Eloovc 的曲線，我們可以看出隨著 Feature 值的上升，Eloocv 值不會是一個穩定下降再上升的曲線，它會有跳動的情況發生（例如多個模型表現都很好的情況），所以有時會造成選擇的盲點。</p>\n\n<p>因此實務上我們都不會使用 Leave-One-Out Cross Validation 這個方法來選擇模型。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-13.png\">\n</p>\n\n<h3 id=\"\">分份數的概念</h3>\n\n<p>Leave-One-Out Cross Validation 很像是把 D 個資料分成 D 分來做 cross validation，那我們可以將份數變少，比如說分成 5 份或 10 份做 cross validation，這樣就可以大大減少計算量了。</p>\n\n<p>目前實務上都是分 10 份，使用 10-fold cross validation。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-14.png\">\n</p>\n\n<h3 id=\"\">一些提醒</h3>\n\n<p>Cross Validaton 是用來做模型的選擇，基本上也會是用風險，因此 validation 表現的結果很好，也不是百分之百未來做預測時效果都會很好。還是要記得觀察未來的預測情況。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-15.png\">\n</p>\n\n<h3 id=\"\">總結</h3>\n\n<p>由於我們已經學會了很多機器學習的方法，有許多地方可以調整我們學習的模型，所以在眾多的學習模型中哪個是最好的，我們也需要有一個方法來幫助我們做選擇，這個方法就是 Cross Validation。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-15-16.png\">\n</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-03-18T18:30:53.000Z","created_by":1,"updated_at":"2016-08-09T05:37:55.000Z","updated_by":1,"published_at":"2016-03-19T18:44:00.000Z","published_by":1},{"id":79,"uuid":"c1b2b7f3-9cc6-4ac9-8b3a-44f5b50f1e61","title":"林軒田教授機器學習基石 Machine Learning Foundations 第十六講學習筆記","slug":"lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-liu-jiang-xue-xi-bi-ji","markdown":"### 前言\n\n本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 [第十五講](http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-wu-jiang-xue-xi-bi-ji/) 的碼農們，我建議可以先回頭去讀一下再回來喔！\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 熱身回顧一下\n\n在上一講中，我們了解了如何使用 Cross Validation 來幫助我們客觀選擇較好的模型，基本上機器學習所有相關的基本知識都已經具備了，這一講是林軒田老師給的三個錦囊妙計，算是一種經驗分享吧～\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-1.png\">\n</p>\n\n### 第一計 奧卡姆剃刀\n\n資料的解釋應該要越簡單越好，我們應該要用剃刀剃掉過分的解釋，據說這句話是愛因斯坦說的。\n\n如下圖，我們在使用機器學習時，也希望學習出來的模型會是左較簡單的模型。在直覺上我們會覺得左圖會比右圖夠有解釋性，當然理論上也證明如此了。\n    \n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-2.png\">\n</p>\n\n### 較簡單的模型\n\n什麼是叫簡單的模型呢？較教簡單的模型，就是看起來很簡單，假設較少、參數較少，假設集合也比較好。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-3.png\">\n</p>\n\n### 簡單比較好\n\n那為什麼簡單會比較好呢？除了之前數學上的解釋之外，我們可以有這樣直觀的解釋：如果一個簡單的模型可以為數據做一個好的鑒別，那就代表這個模型的假設很有解釋性，如果是複雜的模型，由於它永遠都可以把訓練資料分的很好，這樣其實是沒有什麼解釋性的，也因此用簡單的模型會是比較好的。\n\n所以根據這一計的想法，我們應該要先試線性模型，然後盡可能了解自己是不是已經盡可能地用了簡單的模型。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-4.png\">\n</p>\n\n### 第二計 避免取樣偏差\n\n取樣有可能會有偏差，VC 理論其中的一個假設就是訓練資料與測試資料要來自於同一個分佈，否則就無法成立。如果取樣有偏差，那機器學習的效果就會不好。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-5.png\">\n</p>\n\n### 處理取樣偏差\n\n要避免取樣偏差，要好好了解測試環境，讓訓練環境跟測試環境可以儘可能接近。舉例來說，如果測試環境會使用時間軸近期的資料，那訓練時要想辦法對時間軸較近的資料做一些權重的調整，在做 Validation 的時候也應該要選擇時間軸較近的資料。\n\n另一個例子，其實信用卡核卡問題也有取樣偏差的風險，因為銀行只會有錯誤核卡，申請人刷爆卡的記錄，卻沒有錯誤不核卡，但該位申請人信用良好的資料。因此搜集到的資料本身就已經有被篩選過了，也因此可以針對這個部分在做一些優化。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-6.png\">\n</p>\n\n### 第三計 避免偷看資料\n\n之前我們的課程中有說過，我們可能會因為看過資料而猜測圈圈會有最好的效果，但這樣就會造成我們的學習過程沒有考慮到人腦幫忙計算過的 model complexity，所以我們要避免偷看資料。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-7.png\">\n</p>\n\n### 資料重複利用地偷看\n\n其實使用資料的過程中，我們就不斷地偷看資料，甚至看別人論文時，也是在累積偷看資料的過程，所以需要了解到這個概念，有可能讓你的機器學習受到影響。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-8.png\">\n</p>\n\n### 處理資料偷看\n\n實際上偷看資料的情況很容易發生，要做到完全不偷看資料很難，所以我們可以做的就是，一開始就將測試資料鎖起來，學習的過程中完全不用，然後使用 Validation 來避免偷看資料。\n\n如果說希望將自己的 Domain Knowledge 加入假設，應該一開始就加進去，而不是看完資料再加進去。然後，要時時刻刻會實驗的結果存著懷疑之心，要有一種感覺這樣的結果可能受到的資料偷看污染的影響。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-9.png\">\n</p>\n\n### Power of Three\n\n除了三個錦囊妙計，林軒田老師將機器學習的重點整理成 Power of Three，帶我們整個回顧一下。\n\n第一個是機器學習有三個相關領域，Data Mining、Artificial Intelligence、Statistics。\n\n- Data Mining 是從大量的數據裡面找出有趣的特性，它跟 ML 是高度相關的。\n- Artificail Intelligence 是想讓機器做一些有智慧的事，ML 是實現 AI 的一種工具。 \n- Statistics 是從數據裡做一些推論的動作，是 ML 的一種工具。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-10.png\">\n</p>\n\n### 三個理論保證\n\n- Hoeffding 不等式，針對單一個 hypothesis 保證錯誤率在某個上界，我們會用在 Testing。\n- Multi-Bin Hoeffding，針對 M 個 hypothesis 保證錯誤率在某個上界，我們會用在 Validation。\n- VC Bound，針對所有的 hypothesis set 保證錯誤率在某個上界，我們會用在 Training。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-11.png\">\n</p>\n\n### 三個模型\n\n- PLA/Pocket，用在二元分類，由於是 NP-Hard 的問題，我們使用特殊的方法來優化。\n- Linear Regression，線性迴歸很容易優化，可以用公式解。\n- Logistic Regression，用來計算機率，使用遞迴的方式優化。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-12.png\">\n</p>\n\n### 三個重要工具\n\n- Feature Transform，可以轉換到高維空間，將 Ein 變小。\n- Regularization，與 Feature Transform 相反，讓模型變簡單，VC Dimenstion 變小，但 Ein 會變大。\n- Validation，留下一些乾淨的資料來做模型的選擇。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-13.png\">\n</p>\n\n### 未來的方向\n\n底下所有機器學習相關的關鍵字都是未來可以去學習的，將在後續的機器學習技法課程中講解。大致上有三個方向，一個是更多不一樣的轉換方式，不只有多項式的轉換；一個是更多的正規化方式；最後一個是沒有那麼多的 Label，比如說無監督式的學習等等。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-15.png\">\n</p>","html":"<h3 id=\"\">前言</h3>\n\n<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習基石（Machine Learning Foundations）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href=\"http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-wu-jiang-xue-xi-bi-ji/\">第十五講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">熱身回顧一下</h3>\n\n<p>在上一講中，我們了解了如何使用 Cross Validation 來幫助我們客觀選擇較好的模型，基本上機器學習所有相關的基本知識都已經具備了，這一講是林軒田老師給的三個錦囊妙計，算是一種經驗分享吧～</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-1.png\">\n</p>\n\n<h3 id=\"\">第一計 奧卡姆剃刀</h3>\n\n<p>資料的解釋應該要越簡單越好，我們應該要用剃刀剃掉過分的解釋，據說這句話是愛因斯坦說的。</p>\n\n<p>如下圖，我們在使用機器學習時，也希望學習出來的模型會是左較簡單的模型。在直覺上我們會覺得左圖會比右圖夠有解釋性，當然理論上也證明如此了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-2.png\">\n</p>\n\n<h3 id=\"\">較簡單的模型</h3>\n\n<p>什麼是叫簡單的模型呢？較教簡單的模型，就是看起來很簡單，假設較少、參數較少，假設集合也比較好。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-3.png\">\n</p>\n\n<h3 id=\"\">簡單比較好</h3>\n\n<p>那為什麼簡單會比較好呢？除了之前數學上的解釋之外，我們可以有這樣直觀的解釋：如果一個簡單的模型可以為數據做一個好的鑒別，那就代表這個模型的假設很有解釋性，如果是複雜的模型，由於它永遠都可以把訓練資料分的很好，這樣其實是沒有什麼解釋性的，也因此用簡單的模型會是比較好的。</p>\n\n<p>所以根據這一計的想法，我們應該要先試線性模型，然後盡可能了解自己是不是已經盡可能地用了簡單的模型。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-4.png\">\n</p>\n\n<h3 id=\"\">第二計 避免取樣偏差</h3>\n\n<p>取樣有可能會有偏差，VC 理論其中的一個假設就是訓練資料與測試資料要來自於同一個分佈，否則就無法成立。如果取樣有偏差，那機器學習的效果就會不好。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-5.png\">\n</p>\n\n<h3 id=\"\">處理取樣偏差</h3>\n\n<p>要避免取樣偏差，要好好了解測試環境，讓訓練環境跟測試環境可以儘可能接近。舉例來說，如果測試環境會使用時間軸近期的資料，那訓練時要想辦法對時間軸較近的資料做一些權重的調整，在做 Validation 的時候也應該要選擇時間軸較近的資料。</p>\n\n<p>另一個例子，其實信用卡核卡問題也有取樣偏差的風險，因為銀行只會有錯誤核卡，申請人刷爆卡的記錄，卻沒有錯誤不核卡，但該位申請人信用良好的資料。因此搜集到的資料本身就已經有被篩選過了，也因此可以針對這個部分在做一些優化。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-6.png\">\n</p>\n\n<h3 id=\"\">第三計 避免偷看資料</h3>\n\n<p>之前我們的課程中有說過，我們可能會因為看過資料而猜測圈圈會有最好的效果，但這樣就會造成我們的學習過程沒有考慮到人腦幫忙計算過的 model complexity，所以我們要避免偷看資料。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-7.png\">\n</p>\n\n<h3 id=\"\">資料重複利用地偷看</h3>\n\n<p>其實使用資料的過程中，我們就不斷地偷看資料，甚至看別人論文時，也是在累積偷看資料的過程，所以需要了解到這個概念，有可能讓你的機器學習受到影響。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-8.png\">\n</p>\n\n<h3 id=\"\">處理資料偷看</h3>\n\n<p>實際上偷看資料的情況很容易發生，要做到完全不偷看資料很難，所以我們可以做的就是，一開始就將測試資料鎖起來，學習的過程中完全不用，然後使用 Validation 來避免偷看資料。</p>\n\n<p>如果說希望將自己的 Domain Knowledge 加入假設，應該一開始就加進去，而不是看完資料再加進去。然後，要時時刻刻會實驗的結果存著懷疑之心，要有一種感覺這樣的結果可能受到的資料偷看污染的影響。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-9.png\">\n</p>\n\n<h3 id=\"powerofthree\">Power of Three</h3>\n\n<p>除了三個錦囊妙計，林軒田老師將機器學習的重點整理成 Power of Three，帶我們整個回顧一下。</p>\n\n<p>第一個是機器學習有三個相關領域，Data Mining、Artificial Intelligence、Statistics。</p>\n\n<ul>\n<li>Data Mining 是從大量的數據裡面找出有趣的特性，它跟 ML 是高度相關的。</li>\n<li>Artificail Intelligence 是想讓機器做一些有智慧的事，ML 是實現 AI 的一種工具。 </li>\n<li>Statistics 是從數據裡做一些推論的動作，是 ML 的一種工具。</li>\n</ul>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-10.png\">\n</p>\n\n<h3 id=\"\">三個理論保證</h3>\n\n<ul>\n<li>Hoeffding 不等式，針對單一個 hypothesis 保證錯誤率在某個上界，我們會用在 Testing。</li>\n<li>Multi-Bin Hoeffding，針對 M 個 hypothesis 保證錯誤率在某個上界，我們會用在 Validation。</li>\n<li>VC Bound，針對所有的 hypothesis set 保證錯誤率在某個上界，我們會用在 Training。</li>\n</ul>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-11.png\">\n</p>\n\n<h3 id=\"\">三個模型</h3>\n\n<ul>\n<li>PLA/Pocket，用在二元分類，由於是 NP-Hard 的問題，我們使用特殊的方法來優化。</li>\n<li>Linear Regression，線性迴歸很容易優化，可以用公式解。</li>\n<li>Logistic Regression，用來計算機率，使用遞迴的方式優化。</li>\n</ul>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-12.png\">\n</p>\n\n<h3 id=\"\">三個重要工具</h3>\n\n<ul>\n<li>Feature Transform，可以轉換到高維空間，將 Ein 變小。</li>\n<li>Regularization，與 Feature Transform 相反，讓模型變簡單，VC Dimenstion 變小，但 Ein 會變大。</li>\n<li>Validation，留下一些乾淨的資料來做模型的選擇。</li>\n</ul>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-13.png\">\n</p>\n\n<h3 id=\"\">未來的方向</h3>\n\n<p>底下所有機器學習相關的關鍵字都是未來可以去學習的，將在後續的機器學習技法課程中講解。大致上有三個方向，一個是更多不一樣的轉換方式，不只有多項式的轉換；一個是更多的正規化方式；最後一個是沒有那麼多的 Label，比如說無監督式的學習等等。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Foundations-16-15.png\">\n</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-03-20T13:53:15.000Z","created_by":1,"updated_at":"2016-08-09T05:38:26.000Z","updated_by":1,"published_at":"2016-03-20T17:16:59.000Z","published_by":1},{"id":80,"uuid":"a494508e-3676-4840-8039-896c6407e666","title":"林軒田教授機器學習技法 Machine Learning Techniques 第 1 講學習筆記","slug":"lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-1-jiang-xue-xi-bi-ji","markdown":"### 前言\n\n本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 [機器學習基石系列](http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-liu-jiang-xue-xi-bi-ji/) 的碼農們，我建議可以先回頭去讀一下再回來喔！\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 熱身回顧一下\n\n從機器學習基石課程中，我們已經了解了機器學習一些基本的演算法，在機器學習技法課程中我們將介紹更多進階的機器學習演算法。首先登場的就是支持向量機（Support Vector Machine）了，第一講中我們將先介紹最簡單的 Hard Margin Linear Support Vector Machine。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-1.png\">\n</p>\n\n### 線性分類回憶\n\n回憶一下之前的課程中，我們使用 PLA 及 Pocket 來學習出可以分出兩類的線。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-2.png\">\n</p>\n\n### 哪條線最好？\n\n但其實可以將訓練資料分類的線可能會有很多條線，如下圖所示。我們要怎麼選呢？如果用眼睛來看，你或許會覺得右邊的這條線最好。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-3.png\">\n</p>\n\n### 為何右邊這條線最好？\n\n為何會覺得右邊這條線最好呢？假設先在我們再一次取得資料，可以預期資料與訓練資料會有點接近，但並不會完全一樣，這是因為 noise 的原因。所以偏差了一點點的 X 及 O 再左邊這條線可能就會不小心超出現，所以就會被誤判了，但在右邊這條線就可以容忍更多的誤差，也就比較不容易 overfitting，也因此右邊這條線最好。\n\n如何描述這條線？我們可以說這條線與最近的訓練資料距離是所有的線中最大的。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-4.png\">\n</p>\n\n### 胖的線\n\n我們希望得到的線與最近的資料點的距離最大，換的角度，我們也可以說，我們想要得到最胖的線，而且這個胖線還可以將訓練資料分好分類。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-5.png\">\n</p>\n\n### Large-Margin Separating Hyperplane\n\n這種胖的線名稱就叫 Large-Margin Separating Hyperplane，原本的問題就可以定義成要找最大的 margin，而且還要分好類，也就是 yn = sign(wTXn)。\n\n最大的 margin 可以轉換成點與超平面之間最小的距離 distance(Xn, w)，然後 yn = sign(wTXn)，就代表 Yn 與 score 同號，所以可以轉換成 YnwTXn > 0，我們需要求解滿足這些條件的超平面。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-7.png\">\n</p>\n\n### 點與超平面的距離 - 符號解釋\n\n點與超平面的距離怎麼算呢？在這邊的推導，我們需要暫時將 w0 分出來，寫成 b，所以我們之前熟悉的 wTXn 在這邊暫時變成 wTxn + b，以方便推導。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-8.png\">\n</p>\n\n### 點與超平面的距離 - 推導\n\n如果我們現在有一個超平面 wTx + b = 0，假設 x' 與 x'' 都在這個超平面上，也就是 wTx' + b 及 wTx'' + b 都會是 0，如此就會得到 wTx' = -b 及 wTx'' = -b。現在我們將 wT 與 (X''- X') 相乘，由於剛剛的式子，我們會得到 0。(X''- X')是一個在 wTx + b = 0 超平面上的向量，W 與這個向量相乘會是 0 就代表 w 是這個超平面的法向量，要算 x 與超平面的距離，就是將 (x-x') 這個向量投影到 w，就可以算出點與超平面的距離了，公式如下所示。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-9.png\">\n</p>\n\n### 分開訓練資料的超平面\n\n由於我們要求的事可以分開訓練資料的超平面，因此已有 yn(wTXn+ b) > 0 這個條件，也因此距離公式中的 |wTx+b| 可以用 yn(wTXn + b) 來取代，這樣會比較容易求解。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-10.png\">\n</p>\n\n### 減少超平面解的數量\n\n觀察一下下圖中所有求解的條件，我們可以再進一步簡化。假設我們要找的是 wTx + b = 0 這個超平面，我們對這個超平面進行縮放其實是沒有任何影響的，現在我們也將 wTx + b 進行放縮，讓它跟 yn 相乘會是 1，也就是 yn(wTXn + b) = 1，這樣原本的 margin(b,w) 就是可以轉換成 1 除以 w 的長度，我們只要求讓這個值最大的平面就可以了。 \n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-11.png\">\n</p>\n\n### 再次簡化問題\n\n經過上述的推導，我們的問題變成求滿足(1) max 1/||w|| 及 (2) min yn(wTXn + b) = 1 這兩個條件的超平面，但這樣我們好像還是覺得有些複雜不會解，可以再這麼簡化呢？\n\nmin yn(wTXn + b) = 1 這個條件我們可以讓它的限制再鬆一點，只要 yn(wTxn+b)>=1 就好了，理論上保證最後得到的解，一定會有等於 1 的情況，而不會全部都大於 1。\n\n另外 max 1/||w|| 我們改成 min ||w||，||w|| 是 wTw 開根號，我們可以不理根號然後乘上 1/2 以方便後面的推導，所以轉換成 min 1/2(wTw)。 \n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-12.png\">\n</p>\n\n### 解一個簡單的問題來看看\n\n現在我們求解的條件變成求 min 1/2(wTw) 且 yn(wTXn + b)>=1 的超平面，我們用一個簡單的例子來求解看看。如下圖所示，我們可以找出 w1 = 1, w2 = -1, b = -1 滿足我們的條件，這個超平面就是 x1 - x2 - 1，這個超平面也稱為 Support Vector Machine（SVM）。 \n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-13.png\">\n</p>\n\n### 支持向量機（Support Vector Machine）\n\n從上面這個簡單例子，讓我們來了解一下支持向量機。首先，這個向量可以由 margin 的公式得出 marging 的寬度值。我們會發現有些點會剛好在這個 margin 的邊界上，這些點就是所謂的支持向量（Support Vector）。這些支持向量可以標出胖線的位置，其他的點則無法，所以其他的點在這個問題上是不重要的點。\n\n所以 SVM 的意思就是：透過 Support Vector 的協助來學習出最胖的超平面。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-14.png\">\n</p>\n\n### 實際上怎麼解這個問題？\n\n我們剛剛只是從一個簡單的例子來解 SVM，那實際上怎麼解這個問題呢？之前我們學過 gradient descent，在這邊好像沒用，因為有很多限制，我們不能讓演算法自由自在的計算 gradient descent。\n\n但這個問題其實有現存的方法可以解，觀察所有的限制式就會發現 SVM 可以用二次規劃（quadratic programming）來找出最佳解。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-15.png\">\n</p>\n\n### 二次規劃\n\n我們把 SVM 的限制式，跟二次規劃的限制式做一個比較，就發現可以將問題轉換成二次規劃的相關參數，找出 SVM 問題在二次規劃時的 u, Q, p, a, c，我們就可以把這些參數丟到 QP Solver 來幫我們解 SVM。\n\n目前各語言都有提供 QP Solver，但是介面參數可能會不相同，要自己讀文件去了解各個參數，自己做轉換。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-16.png\">\n</p>\n\n### 第一個 SVM\n\n使用 QP Solver 你就可以解第一個學會的 SVM，這個 SVM 是 hard margin（需要訓練資料線性可分），且學習出來的是線性模型，如果要做非線性模型，只要將資料做非線性轉換就可以了。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-17.png\">\n</p>\n\n### 探討一下其中的理論\n\n為何胖的超平面會比較好呢？除了我們前述用例子說明之外，有沒有什麼理論基礎呢？其實如果觀察 SVM 的限制式，我們把它拿來與正規化做比較，會發現 SVM 與正規化實際上做的事情很類似，也因此兩個方法都有避免 Overfitting 的能力。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-18.png\">\n</p>\n\n### SVM 的優勢\n\nSVM 的優勢在哪呢？之前我們學過可以將資料做特徵轉換到高維度，讓假設集合變多，可以學習更複雜的模型，但這很可能會造成 overfitting。但簡單的模型假設集合太少，無法學習複雜的模型。\n\n我們希望可以讓假設集合不是太多，但又可以學習較複雜的模型，SVM 就可以兩全其美，他比起正規化更好的地方就在於正規化需要對原來的演算法作調整，但 SVM 本身就像是一個具備正規化的演算法。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-20.png\">\n</p>\n\n### 總結\n\n在這一講我們了解了 SVM，且知道 SVM 的特性就是去找出可以將訓練資料分好的一條最胖的超平面。而實務上我們會用二次規劃的工具來解 SVM。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-21.png\">\n</p>","html":"<h3 id=\"\">前言</h3>\n\n<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href=\"http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-liu-jiang-xue-xi-bi-ji/\">機器學習基石系列</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">熱身回顧一下</h3>\n\n<p>從機器學習基石課程中，我們已經了解了機器學習一些基本的演算法，在機器學習技法課程中我們將介紹更多進階的機器學習演算法。首先登場的就是支持向量機（Support Vector Machine）了，第一講中我們將先介紹最簡單的 Hard Margin Linear Support Vector Machine。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-1.png\">\n</p>\n\n<h3 id=\"\">線性分類回憶</h3>\n\n<p>回憶一下之前的課程中，我們使用 PLA 及 Pocket 來學習出可以分出兩類的線。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-2.png\">\n</p>\n\n<h3 id=\"\">哪條線最好？</h3>\n\n<p>但其實可以將訓練資料分類的線可能會有很多條線，如下圖所示。我們要怎麼選呢？如果用眼睛來看，你或許會覺得右邊的這條線最好。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-3.png\">\n</p>\n\n<h3 id=\"\">為何右邊這條線最好？</h3>\n\n<p>為何會覺得右邊這條線最好呢？假設先在我們再一次取得資料，可以預期資料與訓練資料會有點接近，但並不會完全一樣，這是因為 noise 的原因。所以偏差了一點點的 X 及 O 再左邊這條線可能就會不小心超出現，所以就會被誤判了，但在右邊這條線就可以容忍更多的誤差，也就比較不容易 overfitting，也因此右邊這條線最好。</p>\n\n<p>如何描述這條線？我們可以說這條線與最近的訓練資料距離是所有的線中最大的。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-4.png\">\n</p>\n\n<h3 id=\"\">胖的線</h3>\n\n<p>我們希望得到的線與最近的資料點的距離最大，換的角度，我們也可以說，我們想要得到最胖的線，而且這個胖線還可以將訓練資料分好分類。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-5.png\">\n</p>\n\n<h3 id=\"largemarginseparatinghyperplane\">Large-Margin Separating Hyperplane</h3>\n\n<p>這種胖的線名稱就叫 Large-Margin Separating Hyperplane，原本的問題就可以定義成要找最大的 margin，而且還要分好類，也就是 yn = sign(wTXn)。</p>\n\n<p>最大的 margin 可以轉換成點與超平面之間最小的距離 distance(Xn, w)，然後 yn = sign(wTXn)，就代表 Yn 與 score 同號，所以可以轉換成 YnwTXn > 0，我們需要求解滿足這些條件的超平面。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-7.png\">\n</p>\n\n<h3 id=\"\">點與超平面的距離 - 符號解釋</h3>\n\n<p>點與超平面的距離怎麼算呢？在這邊的推導，我們需要暫時將 w0 分出來，寫成 b，所以我們之前熟悉的 wTXn 在這邊暫時變成 wTxn + b，以方便推導。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-8.png\">\n</p>\n\n<h3 id=\"\">點與超平面的距離 - 推導</h3>\n\n<p>如果我們現在有一個超平面 wTx + b = 0，假設 x' 與 x'' 都在這個超平面上，也就是 wTx' + b 及 wTx'' + b 都會是 0，如此就會得到 wTx' = -b 及 wTx'' = -b。現在我們將 wT 與 (X''- X') 相乘，由於剛剛的式子，我們會得到 0。(X''- X')是一個在 wTx + b = 0 超平面上的向量，W 與這個向量相乘會是 0 就代表 w 是這個超平面的法向量，要算 x 與超平面的距離，就是將 (x-x') 這個向量投影到 w，就可以算出點與超平面的距離了，公式如下所示。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-9.png\">\n</p>\n\n<h3 id=\"\">分開訓練資料的超平面</h3>\n\n<p>由於我們要求的事可以分開訓練資料的超平面，因此已有 yn(wTXn+ b) > 0 這個條件，也因此距離公式中的 |wTx+b| 可以用 yn(wTXn + b) 來取代，這樣會比較容易求解。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-10.png\">\n</p>\n\n<h3 id=\"\">減少超平面解的數量</h3>\n\n<p>觀察一下下圖中所有求解的條件，我們可以再進一步簡化。假設我們要找的是 wTx + b = 0 這個超平面，我們對這個超平面進行縮放其實是沒有任何影響的，現在我們也將 wTx + b 進行放縮，讓它跟 yn 相乘會是 1，也就是 yn(wTXn + b) = 1，這樣原本的 margin(b,w) 就是可以轉換成 1 除以 w 的長度，我們只要求讓這個值最大的平面就可以了。 </p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-11.png\">\n</p>\n\n<h3 id=\"\">再次簡化問題</h3>\n\n<p>經過上述的推導，我們的問題變成求滿足(1) max 1/||w|| 及 (2) min yn(wTXn + b) = 1 這兩個條件的超平面，但這樣我們好像還是覺得有些複雜不會解，可以再這麼簡化呢？</p>\n\n<p>min yn(wTXn + b) = 1 這個條件我們可以讓它的限制再鬆一點，只要 yn(wTxn+b)>=1 就好了，理論上保證最後得到的解，一定會有等於 1 的情況，而不會全部都大於 1。</p>\n\n<p>另外 max 1/||w|| 我們改成 min ||w||，||w|| 是 wTw 開根號，我們可以不理根號然後乘上 1/2 以方便後面的推導，所以轉換成 min 1/2(wTw)。 </p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-12.png\">\n</p>\n\n<h3 id=\"\">解一個簡單的問題來看看</h3>\n\n<p>現在我們求解的條件變成求 min 1/2(wTw) 且 yn(wTXn + b)>=1 的超平面，我們用一個簡單的例子來求解看看。如下圖所示，我們可以找出 w1 = 1, w2 = -1, b = -1 滿足我們的條件，這個超平面就是 x1 - x2 - 1，這個超平面也稱為 Support Vector Machine（SVM）。 </p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-13.png\">\n</p>\n\n<h3 id=\"supportvectormachine\">支持向量機（Support Vector Machine）</h3>\n\n<p>從上面這個簡單例子，讓我們來了解一下支持向量機。首先，這個向量可以由 margin 的公式得出 marging 的寬度值。我們會發現有些點會剛好在這個 margin 的邊界上，這些點就是所謂的支持向量（Support Vector）。這些支持向量可以標出胖線的位置，其他的點則無法，所以其他的點在這個問題上是不重要的點。</p>\n\n<p>所以 SVM 的意思就是：透過 Support Vector 的協助來學習出最胖的超平面。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-14.png\">\n</p>\n\n<h3 id=\"\">實際上怎麼解這個問題？</h3>\n\n<p>我們剛剛只是從一個簡單的例子來解 SVM，那實際上怎麼解這個問題呢？之前我們學過 gradient descent，在這邊好像沒用，因為有很多限制，我們不能讓演算法自由自在的計算 gradient descent。</p>\n\n<p>但這個問題其實有現存的方法可以解，觀察所有的限制式就會發現 SVM 可以用二次規劃（quadratic programming）來找出最佳解。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-15.png\">\n</p>\n\n<h3 id=\"\">二次規劃</h3>\n\n<p>我們把 SVM 的限制式，跟二次規劃的限制式做一個比較，就發現可以將問題轉換成二次規劃的相關參數，找出 SVM 問題在二次規劃時的 u, Q, p, a, c，我們就可以把這些參數丟到 QP Solver 來幫我們解 SVM。</p>\n\n<p>目前各語言都有提供 QP Solver，但是介面參數可能會不相同，要自己讀文件去了解各個參數，自己做轉換。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-16.png\">\n</p>\n\n<h3 id=\"svm\">第一個 SVM</h3>\n\n<p>使用 QP Solver 你就可以解第一個學會的 SVM，這個 SVM 是 hard margin（需要訓練資料線性可分），且學習出來的是線性模型，如果要做非線性模型，只要將資料做非線性轉換就可以了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-17.png\">\n</p>\n\n<h3 id=\"\">探討一下其中的理論</h3>\n\n<p>為何胖的超平面會比較好呢？除了我們前述用例子說明之外，有沒有什麼理論基礎呢？其實如果觀察 SVM 的限制式，我們把它拿來與正規化做比較，會發現 SVM 與正規化實際上做的事情很類似，也因此兩個方法都有避免 Overfitting 的能力。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-18.png\">\n</p>\n\n<h3 id=\"svm\">SVM 的優勢</h3>\n\n<p>SVM 的優勢在哪呢？之前我們學過可以將資料做特徵轉換到高維度，讓假設集合變多，可以學習更複雜的模型，但這很可能會造成 overfitting。但簡單的模型假設集合太少，無法學習複雜的模型。</p>\n\n<p>我們希望可以讓假設集合不是太多，但又可以學習較複雜的模型，SVM 就可以兩全其美，他比起正規化更好的地方就在於正規化需要對原來的演算法作調整，但 SVM 本身就像是一個具備正規化的演算法。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-20.png\">\n</p>\n\n<h3 id=\"\">總結</h3>\n\n<p>在這一講我們了解了 SVM，且知道 SVM 的特性就是去找出可以將訓練資料分好的一條最胖的超平面。而實務上我們會用二次規劃的工具來解 SVM。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-1-21.png\">\n</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-04-19T09:13:43.000Z","created_by":1,"updated_at":"2016-08-09T05:38:51.000Z","updated_by":1,"published_at":"2016-04-21T09:00:27.000Z","published_by":1},{"id":81,"uuid":"df1e549d-f4ba-4bf7-acdd-d2e65d2efdf6","title":"林軒田教授機器學習技法 Machine Learning Techniques 第 2 講學習筆記","slug":"lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-2-jiang-xue-xi-bi-ji","markdown":"### 前言\n\n本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 [第 1 講](http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-1-jiang-xue-xi-bi-ji/) 的碼農們，我建議可以先回頭去讀一下再回來喔！\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 熱身回顧一下\n\n從機器學習基石課程中，我們已經了解了機器學習一些基本的演算法，在機器學習技法課程中我們將介紹更多進階的機器學習演算法。首先登場的就是支持向量機（Support Vector Machine）了，第一講中我們將先介紹最簡單的 Hard Margin Linear Support Vector Machine。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-1.png\">\n</p>\n\n### 非線性 SVM\n\n學會了 Hard Margin Linear SVM 之後，如果我們想要訓練非線性模型要怎麼做呢？跟之前的學習模型一樣，我們只要將資料點經過非線性轉換之後，在高維空間做訓練就可以了。\n\n非線性的轉換其實可以依我們的需求轉換到非常高維，甚至可能到無限多維，如果是無限多維的話，我們怎麼使用 QP Solver 來解 SVM 呢？如果 SVM 模型可以轉換到與 feature 維度無關，那我們就可以使用無限多維的轉換了。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-2.png\">\n</p>\n\n### 與特徵維度無關的 SVM\n\n為了可以做到無限多維特徵轉換，我們需要將 SVM 轉為另外一個問題，在數學上已證明這兩個問題其實是一樣的，所以又稱為是 SVM 的對偶問題，Dual SVM，由於背後的數學證明很複雜，這門課程只會解釋一些必要的原理來讓我們理解。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-3.png\">\n</p>\n\n### 使用 Lagrange Multipliers 當工具\n\n我們在正規化那一講中曾經使用過 Lagrange Multipliers 來推導正規化的數學式，在推導 Dual SVM 也會使用到 Lagrange Multiplier。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-4.png\">\n</p>\n\n### 將 SVM 的限制條件轉換成無限制條件\n\n在以往的課程中，我們已經了解有限制條件時，會造成我們找最佳解的困難，所以第一步我們先想辦法把 SVM 的限制條件轉換成無限制條件看看。\n\n有了這樣的想法，我們把原本 SVM 的數學式改寫成一個 Lagrange Function，如下圖所示，原本的 N 的限制式改成了 1-yn(wTZn + b)，並用 N 個 Lagrange Multiplier 來做調整（N 個 alpha）。\n\n數學需要證明 SVM 會等於 min (max Largrange Function, all alpha >= 0)，我們可以先看一下 Largrange Function 的意涵。我們希望 SVM 可以完美的分好資料，所以 1-yn(wTZn + b) 應該都是 <= 0，假設現在 1-yn(wTZn + b) 有一些正值的話，那 max Largrange Function 就會趨向無限大，所以如果我們找到正確的 b, w 分好資料，那 max Largrange 就會趨向 1/2wTw，這樣加上前面的 min，就可以知道 Lagrange Function 的轉換解出來的答案會跟原本的 SVM 一樣。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-5.png\">\n</p>\n\n### Max Min 做交換\n\n由於 Lagrange 對偶性質，Max Min 可以透過下圖的關係式做調換，原本的 min(max Lagrange Function) 有大於等於 max(min Lagrange Function) 的關係，在 QP 的性質上，其實又說兩邊解出來的答案會一模一樣。（這邊用單純的說明，沒有數學推導證明）\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-7.png\">\n</p>\n\n### 解 Lagrange Dual (1)\n\n導出目前的 Lagrange Dual 式子 max(min Lagrange Function)之後，我們要來解看看最佳解了。由於 min Lagrange Function 是沒有限制條件的，所以我們可以用偏微分來求極值。\n\n首先我們對 b 做偏微分，會得到 - sigma(anyn) ＝0，負號可以不用管，所以寫成 sigma(anyn) = 0。\n\n將這個限制式代入原本的式子，就可以把原本的式子做一些簡化，如下圖所示。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-8.png\">\n</p>\n\n### 解 Lagrange Dual (2)\n\n接下來我們對 wi 做偏微分，可以得到 w = sigma(anynzn)，一樣將這個限制式代入原本的式子，原本式子中的 w 就都可以換掉。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-9.png\">\n</p>\n\n### KKT Optimality Conditions\n\n將過這些最佳換轉換的式子，導出了一些限制式：\n\n1. yn(wTZn + b)>= 1，這是原本要將資料分好的限制式\n2. an >= 0，對偶問題 Lagrange Multiplier 的條件\n3. sigma(ynan) = 0，w = sigma(anynzn)，這是最佳解時會有的條件\n4. 在最佳解時，an(1-yn(wTZn + b)) = 0\n\n這就是著名的 KKT Optimality Conditions，目前這些 b,w 最佳解時的限制式，其中的變數就只剩下 an，所以實務上我們就要去找出最佳解時的 an 會是什麼，再利用上述的關係解出 b, w。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-10.png\">\n</p>\n\n### Dual SVM\n\n導出最佳解時的所有限制式之後，原來的式子可以改成下圖中的式子，這個式子其實也是一個 QP 問題，我們可以用 QP Solver 來解出最佳解時的 an。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-11.png\">\n</p>\n\n### 用 QP Solver 解 Dual SVM\n\n我們可以用 QP Solver 解 Dual SVM，造上一講的做法去將 QP Solver 所需要的參數找出來，會有下圖中的參數。相等關係的限制式可以改成一個 >= 及 一個 <= 的關係，如果你所使用的 QP Solver 有提供相等關係的參數，那就不用這樣做。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-12.png\">\n</p>\n\n### 特殊的 QP Solver\n\n找出 Dual SVM 的所有 QP Solver 所需參數之後，我們就可以將參數丟進去 QP Solver 讓它幫忙解出最佳解。由於其中的 Q 參數可能會很大，因此使用有對 SVM 問題做特殊處理的 QP Solver 會比較沒問題。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-13.png\">\n</p>\n\n### 找出最佳的 w , b\n\n機器學習演算法最終就是要找出最佳的 w, b 來做未來的預測，不過現在 QP Solver 解出來的只有最佳解時的 an，我們要怎麼求出最佳解時的 w, b 呢？從 KKT Optimality Conditions 我們可以找出 w, b，w ＝sigma(anynzn) 這個條件可以算出 w，an(1-yn(wTZn + b)) = 0 這個條件可以算出 b，因為 an 通常會有大於 0 的情況，所以 1-yn(wTZn + b) 等於 0 才能符合條件，所以 b ＝yn - wTZn。\n\n由於 an 大於 0 的點才能算出 b，這些大於 0 的 an 的資料點其實就是落在胖胖的邊界上。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-14.png\">\n</p>\n\n### Support Vector 的性質\n\n由於 w = sigma(anynzn)，所以其實也只有 an 大於 0 的點會影響到 w 的計算，b 也是只有 an 大於 0 時才有辦法計算，所以 an 大於 0 的資料點其實就是 Support Vector。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-15.png\">\n</p>\n\n### Support Vector 可以呈現胖胖的超平面\n\n我們來看看 w ＝ sigma(anynzn) 的含義，其實他的意思就是 w 可以被 Support Vecotr 線性組合呈現出來。這跟 PLA 也有點像，PLA 的 w 含義是被它犯錯的點的線性組合呈現出來。其實在 Logistic Regression 及 Linear Regression 也可以找到類似的性質，簡而言之，我們最後來出來做預測的 w 其實都可以被我們的訓練資料線性組合呈現出來。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-16.png\">\n</p>\n\n### 比較一下 Primal 及 Dual SVM\n\n我們將 Primal 及 Dual SVM 的式子放在一起比較，其實可以發現 Dual SVM 與資料點的特徵維度 d 已經沒有關係了，因此可以做很高維度的特徵轉換。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-17.png\">\n</p>\n\n### 但其實 Dual SVM 只是把特徵維度藏起來\n\n但其實仔細一看 Dual SVM 只是把特徵維度藏起來，在計算 Q 時，就會與特徵維度牽扯上關係，這樣就還是無法做無限維度的轉換，我們如何真正不需要計算到高維度特徵呢？這是下一講的課程了。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-18.png\">\n</p>\n\n### 總結\n\n在這一講中，我們介紹了 Dual SVM，這是為了讓我們可以對資料點做高維度的特徵轉換，這樣就可以讓 SVM 學習更複雜的非線性模型，但我們又不想要跟高維度的計算牽扯上關係，Dual SVM 將問題作了一些轉換，算是把高維度的計算藏了一半，另一半就是下次的課程了。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-19.png\">\n</p>","html":"<h3 id=\"\">前言</h3>\n\n<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href=\"http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-1-jiang-xue-xi-bi-ji/\">第 1 講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">熱身回顧一下</h3>\n\n<p>從機器學習基石課程中，我們已經了解了機器學習一些基本的演算法，在機器學習技法課程中我們將介紹更多進階的機器學習演算法。首先登場的就是支持向量機（Support Vector Machine）了，第一講中我們將先介紹最簡單的 Hard Margin Linear Support Vector Machine。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-1.png\">\n</p>\n\n<h3 id=\"svm\">非線性 SVM</h3>\n\n<p>學會了 Hard Margin Linear SVM 之後，如果我們想要訓練非線性模型要怎麼做呢？跟之前的學習模型一樣，我們只要將資料點經過非線性轉換之後，在高維空間做訓練就可以了。</p>\n\n<p>非線性的轉換其實可以依我們的需求轉換到非常高維，甚至可能到無限多維，如果是無限多維的話，我們怎麼使用 QP Solver 來解 SVM 呢？如果 SVM 模型可以轉換到與 feature 維度無關，那我們就可以使用無限多維的轉換了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-2.png\">\n</p>\n\n<h3 id=\"svm\">與特徵維度無關的 SVM</h3>\n\n<p>為了可以做到無限多維特徵轉換，我們需要將 SVM 轉為另外一個問題，在數學上已證明這兩個問題其實是一樣的，所以又稱為是 SVM 的對偶問題，Dual SVM，由於背後的數學證明很複雜，這門課程只會解釋一些必要的原理來讓我們理解。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-3.png\">\n</p>\n\n<h3 id=\"lagrangemultipliers\">使用 Lagrange Multipliers 當工具</h3>\n\n<p>我們在正規化那一講中曾經使用過 Lagrange Multipliers 來推導正規化的數學式，在推導 Dual SVM 也會使用到 Lagrange Multiplier。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-4.png\">\n</p>\n\n<h3 id=\"svm\">將 SVM 的限制條件轉換成無限制條件</h3>\n\n<p>在以往的課程中，我們已經了解有限制條件時，會造成我們找最佳解的困難，所以第一步我們先想辦法把 SVM 的限制條件轉換成無限制條件看看。</p>\n\n<p>有了這樣的想法，我們把原本 SVM 的數學式改寫成一個 Lagrange Function，如下圖所示，原本的 N 的限制式改成了 1-yn(wTZn + b)，並用 N 個 Lagrange Multiplier 來做調整（N 個 alpha）。</p>\n\n<p>數學需要證明 SVM 會等於 min (max Largrange Function, all alpha >= 0)，我們可以先看一下 Largrange Function 的意涵。我們希望 SVM 可以完美的分好資料，所以 1-yn(wTZn + b) 應該都是 &lt;= 0，假設現在 1-yn(wTZn + b) 有一些正值的話，那 max Largrange Function 就會趨向無限大，所以如果我們找到正確的 b, w 分好資料，那 max Largrange 就會趨向 1/2wTw，這樣加上前面的 min，就可以知道 Lagrange Function 的轉換解出來的答案會跟原本的 SVM 一樣。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-5.png\">\n</p>\n\n<h3 id=\"maxmin\">Max Min 做交換</h3>\n\n<p>由於 Lagrange 對偶性質，Max Min 可以透過下圖的關係式做調換，原本的 min(max Lagrange Function) 有大於等於 max(min Lagrange Function) 的關係，在 QP 的性質上，其實又說兩邊解出來的答案會一模一樣。（這邊用單純的說明，沒有數學推導證明）</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-7.png\">\n</p>\n\n<h3 id=\"lagrangedual1\">解 Lagrange Dual (1)</h3>\n\n<p>導出目前的 Lagrange Dual 式子 max(min Lagrange Function)之後，我們要來解看看最佳解了。由於 min Lagrange Function 是沒有限制條件的，所以我們可以用偏微分來求極值。</p>\n\n<p>首先我們對 b 做偏微分，會得到 - sigma(anyn) ＝0，負號可以不用管，所以寫成 sigma(anyn) = 0。</p>\n\n<p>將這個限制式代入原本的式子，就可以把原本的式子做一些簡化，如下圖所示。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-8.png\">\n</p>\n\n<h3 id=\"lagrangedual2\">解 Lagrange Dual (2)</h3>\n\n<p>接下來我們對 wi 做偏微分，可以得到 w = sigma(anynzn)，一樣將這個限制式代入原本的式子，原本式子中的 w 就都可以換掉。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-9.png\">\n</p>\n\n<h3 id=\"kktoptimalityconditions\">KKT Optimality Conditions</h3>\n\n<p>將過這些最佳換轉換的式子，導出了一些限制式：</p>\n\n<ol>\n<li>yn(wTZn + b)>= 1，這是原本要將資料分好的限制式  </li>\n<li>an >= 0，對偶問題 Lagrange Multiplier 的條件  </li>\n<li>sigma(ynan) = 0，w = sigma(anynzn)，這是最佳解時會有的條件  </li>\n<li>在最佳解時，an(1-yn(wTZn + b)) = 0</li>\n</ol>\n\n<p>這就是著名的 KKT Optimality Conditions，目前這些 b,w 最佳解時的限制式，其中的變數就只剩下 an，所以實務上我們就要去找出最佳解時的 an 會是什麼，再利用上述的關係解出 b, w。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-10.png\">\n</p>\n\n<h3 id=\"dualsvm\">Dual SVM</h3>\n\n<p>導出最佳解時的所有限制式之後，原來的式子可以改成下圖中的式子，這個式子其實也是一個 QP 問題，我們可以用 QP Solver 來解出最佳解時的 an。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-11.png\">\n</p>\n\n<h3 id=\"qpsolverdualsvm\">用 QP Solver 解 Dual SVM</h3>\n\n<p>我們可以用 QP Solver 解 Dual SVM，造上一講的做法去將 QP Solver 所需要的參數找出來，會有下圖中的參數。相等關係的限制式可以改成一個 >= 及 一個 &lt;= 的關係，如果你所使用的 QP Solver 有提供相等關係的參數，那就不用這樣做。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-12.png\">\n</p>\n\n<h3 id=\"qpsolver\">特殊的 QP Solver</h3>\n\n<p>找出 Dual SVM 的所有 QP Solver 所需參數之後，我們就可以將參數丟進去 QP Solver 讓它幫忙解出最佳解。由於其中的 Q 參數可能會很大，因此使用有對 SVM 問題做特殊處理的 QP Solver 會比較沒問題。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-13.png\">\n</p>\n\n<h3 id=\"wb\">找出最佳的 w , b</h3>\n\n<p>機器學習演算法最終就是要找出最佳的 w, b 來做未來的預測，不過現在 QP Solver 解出來的只有最佳解時的 an，我們要怎麼求出最佳解時的 w, b 呢？從 KKT Optimality Conditions 我們可以找出 w, b，w ＝sigma(anynzn) 這個條件可以算出 w，an(1-yn(wTZn + b)) = 0 這個條件可以算出 b，因為 an 通常會有大於 0 的情況，所以 1-yn(wTZn + b) 等於 0 才能符合條件，所以 b ＝yn - wTZn。</p>\n\n<p>由於 an 大於 0 的點才能算出 b，這些大於 0 的 an 的資料點其實就是落在胖胖的邊界上。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-14.png\">\n</p>\n\n<h3 id=\"supportvector\">Support Vector 的性質</h3>\n\n<p>由於 w = sigma(anynzn)，所以其實也只有 an 大於 0 的點會影響到 w 的計算，b 也是只有 an 大於 0 時才有辦法計算，所以 an 大於 0 的資料點其實就是 Support Vector。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-15.png\">\n</p>\n\n<h3 id=\"supportvector\">Support Vector 可以呈現胖胖的超平面</h3>\n\n<p>我們來看看 w ＝ sigma(anynzn) 的含義，其實他的意思就是 w 可以被 Support Vecotr 線性組合呈現出來。這跟 PLA 也有點像，PLA 的 w 含義是被它犯錯的點的線性組合呈現出來。其實在 Logistic Regression 及 Linear Regression 也可以找到類似的性質，簡而言之，我們最後來出來做預測的 w 其實都可以被我們的訓練資料線性組合呈現出來。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-16.png\">\n</p>\n\n<h3 id=\"primaldualsvm\">比較一下 Primal 及 Dual SVM</h3>\n\n<p>我們將 Primal 及 Dual SVM 的式子放在一起比較，其實可以發現 Dual SVM 與資料點的特徵維度 d 已經沒有關係了，因此可以做很高維度的特徵轉換。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-17.png\">\n</p>\n\n<h3 id=\"dualsvm\">但其實 Dual SVM 只是把特徵維度藏起來</h3>\n\n<p>但其實仔細一看 Dual SVM 只是把特徵維度藏起來，在計算 Q 時，就會與特徵維度牽扯上關係，這樣就還是無法做無限維度的轉換，我們如何真正不需要計算到高維度特徵呢？這是下一講的課程了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-18.png\">\n</p>\n\n<h3 id=\"\">總結</h3>\n\n<p>在這一講中，我們介紹了 Dual SVM，這是為了讓我們可以對資料點做高維度的特徵轉換，這樣就可以讓 SVM 學習更複雜的非線性模型，但我們又不想要跟高維度的計算牽扯上關係，Dual SVM 將問題作了一些轉換，算是把高維度的計算藏了一半，另一半就是下次的課程了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-2-19.png\">\n</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-05-01T17:42:33.000Z","created_by":1,"updated_at":"2016-08-09T05:41:02.000Z","updated_by":1,"published_at":"2016-05-07T14:24:53.000Z","published_by":1},{"id":82,"uuid":"8831ec6e-4238-42cc-a4a3-69a9a3364222","title":"林軒田教授機器學習技法 Machine Learning Techniques 第 3 講學習筆記","slug":"lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-3-jiang-xue-xi-bi-ji","markdown":"### 前言\n\n本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 [第 2 講](http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-2-jiang-xue-xi-bi-ji/) 的碼農們，我建議可以先回頭去讀一下再回來喔！\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 熱身回顧一下\n\n在上一講中，我們介紹了 Dual SVM，這是為了讓我們可以對資料點做高維度的特徵轉換，這樣就可以讓 SVM 學習更複雜的非線性模型，但我們又不想要跟高維度的計算牽扯上關係，Dual SVM 將問題作了一些轉換，能夠將演算法跟高維度的計算脫鉤，但上一講中的 Q 矩陣實際上還是計算了高維度特徵矩陣內積，所以並沒有真的解決問題。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-1.png\">\n</p>\n\n### Dual SVM 仍與高維度 d 有依賴關係\n\n目前推導出來的 Dual SVM 仍與高維度 d 有依賴關係，我們能不能簡化 Q 矩陣的高維度特徵矩陣內積計算呢？\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-2.png\">\n</p>\n\n### 觀察矩陣內積的每一個運算\n\n我們用二次轉換拆開來觀察，發現原本將矩陣進行特徵轉換之後在做矩陣內積，可以分成 0 次項、1 次項、 2 次項分開來計算，結果會是一樣的。而更高維度的轉換也會有相同的性質。如此我們就可以限制計算量只在原本的特徵維度 O(d)。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-3.png\">\n</p>\n\n### Kernel 的概念\n\n有了上述的性質，我們可以引進 Kernel 的概念，之前都是將矩陣進行特徵轉換之後再去計算內積，現在我們可以改成使用 kernel function 來做計算，znTzm 可以改成對應的 kernel function K(xn, xm)。\n\n由於我們都不去 z 空間做計算了，因此也無法得到 z 空間的 w，所以 b 與 w 的式子都要改成使用 kernel function K(xn, xm) 來計算。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-4.png\">\n</p>\n\n### 用 QP 解 Kernel SVM\n\n導出 Kernel function 之後，我們一樣可以將 kernel function 計算出來的 Q 矩陣丟進去 QP Solver 來解 SVM。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-5.png\">\n</p>\n\n### Polynomial Kernel\n\n我們可以將 Kernel Function 整理成更一般化的形式，這樣就可以推導出各式各樣的 Polynomail Kernel。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-6.png\">\n</p>\n\n### Poly-2 Kernel 圖示\n\n我們使用一個例子來看一下 Poly-2 Kernel 的效果，我們可以調整 gamma 參數得出不一樣的分類曲線。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-7.png\">\n</p>\n\n### Polynomial Kernel 一般化\n\n由 Poly-2 Kernel，我們可以再做更多變化，常數項用 zeta 當參數、特徵空間轉換用 Q 當參數，加上原本的 gamma 參數，Polynomial SVM 可以很自由地調整 Kernel 參數來得到更好的分類效果。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-8.png\">\n</p>\n\n### 無限多維轉換的 Kernel\n\n由於我們的演算法已經跟高維度的空間脫鉤了，所以我們有了這樣的想法，我們是不是可以使用無限多維轉換的 kernel 呢？\n\n我們觀察一下指數函數 exp(-(x-x')^2)，結果發現就是一個對 X 的無限多維轉換，由此我們可以推導出下式的無限多維轉換 Kernel，也稱為 Gaussian kernel。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-9.png\">\n</p>\n\n### Gaussian SVM\n\n推導出 Gaussian Kernel 之後，使用 Gaussian Kernel 的 SVM 就是 Gaussian SVM。Gaussian SVM 演算法會與 Polynomial SVM 一樣，只是 Kernel 不一樣，由於是無限多維轉換，我們也不用再去煩腦要用幾次的轉換。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-10.png\">\n</p>\n\n### 觀察 Gaussian SVM 的效果\n\nGaussian SVM 是無限多維的轉換，因此可以預期它有很強的 power 可以做好分類，同時又保證 mergin 最大可以避免 overfit。但下圖中實驗調整 Gaussian Kernel 的 gamma 參數，其實還是有可能會產生 overfit，所以 Gaussian SVM 也不是萬能的，還是要謹慎驗證計算出來的結果。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-12.png\">\n</p>\n\n### 回顧 Linear Kernel 優缺點\n\n我們來回顧一下目前學過的 Kernel。首先是 Linear Kernel，優點是模型較為簡單，也因此比較安全，不容易 overfit；可以算出確切的 W 及 Support Vectors，解釋性較好。缺點就是，限制會較多，如果資料點非線性可分就沒用。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-13.png\">\n</p>\n\n### 回顧 Polynomial Kernel 優缺點\n\n再來是 Polynomial Kernel，由於可以進行 Q 次轉換，分類能力會比 Linear Kernel 好。缺點就是高次轉換可能會有一些數字問題產生，造成計算結果怪異。然後太多參數要選，比較難使用。也因此 Polynomial Kernel 可能會用在比較低次轉換的 SVM 問題上，但這樣也許就可以用 Linear SVM 取代。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-14.png\">\n</p>\n\n### 回顧 Gaussian Kernel 優缺點\n\n最後是 Gaussian Kernel，優點就是無限多維的轉換，分類能力當然更好，而且需要選擇的參數的較少。但缺點就是無法計算出確切的 w 及 support vectors，預測時都要透過 kernel function 來計算，也因此比較沒有解釋性，而且也是會發生 overfit。比起 Polynomail SVM，Gaussian SVM 比較常用。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-15.png\">\n</p>\n\n### 其他 Kernel\n\n除了目前介紹的 Kernel 之外是否還有其他 Kernel 呢？當然有，你也可以自己定義自己的 Kernel，只要符合 Mercer's condition 就是一個合法的 Kernel。不過要定義自己的 Kernel 並不是件容易的事。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-16.png\">\n</p>\n\n### 總結\n\n在這一講中，我們終於脫離了高維度空間的計算依賴，使用 Kernel Funciton 來解 Dual SVM，因此引進了 Polynomial Kernel SVM 的概念，最後甚至推導出了無限多維轉換的 Gaussian Kernel SVM。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-17.png\">\n</p>","html":"<h3 id=\"\">前言</h3>\n\n<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href=\"http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-2-jiang-xue-xi-bi-ji/\">第 2 講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">熱身回顧一下</h3>\n\n<p>在上一講中，我們介紹了 Dual SVM，這是為了讓我們可以對資料點做高維度的特徵轉換，這樣就可以讓 SVM 學習更複雜的非線性模型，但我們又不想要跟高維度的計算牽扯上關係，Dual SVM 將問題作了一些轉換，能夠將演算法跟高維度的計算脫鉤，但上一講中的 Q 矩陣實際上還是計算了高維度特徵矩陣內積，所以並沒有真的解決問題。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-1.png\">\n</p>\n\n<h3 id=\"dualsvmd\">Dual SVM 仍與高維度 d 有依賴關係</h3>\n\n<p>目前推導出來的 Dual SVM 仍與高維度 d 有依賴關係，我們能不能簡化 Q 矩陣的高維度特徵矩陣內積計算呢？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-2.png\">\n</p>\n\n<h3 id=\"\">觀察矩陣內積的每一個運算</h3>\n\n<p>我們用二次轉換拆開來觀察，發現原本將矩陣進行特徵轉換之後在做矩陣內積，可以分成 0 次項、1 次項、 2 次項分開來計算，結果會是一樣的。而更高維度的轉換也會有相同的性質。如此我們就可以限制計算量只在原本的特徵維度 O(d)。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-3.png\">\n</p>\n\n<h3 id=\"kernel\">Kernel 的概念</h3>\n\n<p>有了上述的性質，我們可以引進 Kernel 的概念，之前都是將矩陣進行特徵轉換之後再去計算內積，現在我們可以改成使用 kernel function 來做計算，znTzm 可以改成對應的 kernel function K(xn, xm)。</p>\n\n<p>由於我們都不去 z 空間做計算了，因此也無法得到 z 空間的 w，所以 b 與 w 的式子都要改成使用 kernel function K(xn, xm) 來計算。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-4.png\">\n</p>\n\n<h3 id=\"qpkernelsvm\">用 QP 解 Kernel SVM</h3>\n\n<p>導出 Kernel function 之後，我們一樣可以將 kernel function 計算出來的 Q 矩陣丟進去 QP Solver 來解 SVM。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-5.png\">\n</p>\n\n<h3 id=\"polynomialkernel\">Polynomial Kernel</h3>\n\n<p>我們可以將 Kernel Function 整理成更一般化的形式，這樣就可以推導出各式各樣的 Polynomail Kernel。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-6.png\">\n</p>\n\n<h3 id=\"poly2kernel\">Poly-2 Kernel 圖示</h3>\n\n<p>我們使用一個例子來看一下 Poly-2 Kernel 的效果，我們可以調整 gamma 參數得出不一樣的分類曲線。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-7.png\">\n</p>\n\n<h3 id=\"polynomialkernel\">Polynomial Kernel 一般化</h3>\n\n<p>由 Poly-2 Kernel，我們可以再做更多變化，常數項用 zeta 當參數、特徵空間轉換用 Q 當參數，加上原本的 gamma 參數，Polynomial SVM 可以很自由地調整 Kernel 參數來得到更好的分類效果。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-8.png\">\n</p>\n\n<h3 id=\"kernel\">無限多維轉換的 Kernel</h3>\n\n<p>由於我們的演算法已經跟高維度的空間脫鉤了，所以我們有了這樣的想法，我們是不是可以使用無限多維轉換的 kernel 呢？</p>\n\n<p>我們觀察一下指數函數 exp(-(x-x')^2)，結果發現就是一個對 X 的無限多維轉換，由此我們可以推導出下式的無限多維轉換 Kernel，也稱為 Gaussian kernel。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-9.png\">\n</p>\n\n<h3 id=\"gaussiansvm\">Gaussian SVM</h3>\n\n<p>推導出 Gaussian Kernel 之後，使用 Gaussian Kernel 的 SVM 就是 Gaussian SVM。Gaussian SVM 演算法會與 Polynomial SVM 一樣，只是 Kernel 不一樣，由於是無限多維轉換，我們也不用再去煩腦要用幾次的轉換。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-10.png\">\n</p>\n\n<h3 id=\"gaussiansvm\">觀察 Gaussian SVM 的效果</h3>\n\n<p>Gaussian SVM 是無限多維的轉換，因此可以預期它有很強的 power 可以做好分類，同時又保證 mergin 最大可以避免 overfit。但下圖中實驗調整 Gaussian Kernel 的 gamma 參數，其實還是有可能會產生 overfit，所以 Gaussian SVM 也不是萬能的，還是要謹慎驗證計算出來的結果。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-12.png\">\n</p>\n\n<h3 id=\"linearkernel\">回顧 Linear Kernel 優缺點</h3>\n\n<p>我們來回顧一下目前學過的 Kernel。首先是 Linear Kernel，優點是模型較為簡單，也因此比較安全，不容易 overfit；可以算出確切的 W 及 Support Vectors，解釋性較好。缺點就是，限制會較多，如果資料點非線性可分就沒用。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-13.png\">\n</p>\n\n<h3 id=\"polynomialkernel\">回顧 Polynomial Kernel 優缺點</h3>\n\n<p>再來是 Polynomial Kernel，由於可以進行 Q 次轉換，分類能力會比 Linear Kernel 好。缺點就是高次轉換可能會有一些數字問題產生，造成計算結果怪異。然後太多參數要選，比較難使用。也因此 Polynomial Kernel 可能會用在比較低次轉換的 SVM 問題上，但這樣也許就可以用 Linear SVM 取代。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-14.png\">\n</p>\n\n<h3 id=\"gaussiankernel\">回顧 Gaussian Kernel 優缺點</h3>\n\n<p>最後是 Gaussian Kernel，優點就是無限多維的轉換，分類能力當然更好，而且需要選擇的參數的較少。但缺點就是無法計算出確切的 w 及 support vectors，預測時都要透過 kernel function 來計算，也因此比較沒有解釋性，而且也是會發生 overfit。比起 Polynomail SVM，Gaussian SVM 比較常用。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-15.png\">\n</p>\n\n<h3 id=\"kernel\">其他 Kernel</h3>\n\n<p>除了目前介紹的 Kernel 之外是否還有其他 Kernel 呢？當然有，你也可以自己定義自己的 Kernel，只要符合 Mercer's condition 就是一個合法的 Kernel。不過要定義自己的 Kernel 並不是件容易的事。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-16.png\">\n</p>\n\n<h3 id=\"\">總結</h3>\n\n<p>在這一講中，我們終於脫離了高維度空間的計算依賴，使用 Kernel Funciton 來解 Dual SVM，因此引進了 Polynomial Kernel SVM 的概念，最後甚至推導出了無限多維轉換的 Gaussian Kernel SVM。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-3-17.png\">\n</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-05-23T17:39:46.000Z","created_by":1,"updated_at":"2016-08-09T05:41:19.000Z","updated_by":1,"published_at":"2016-05-23T18:58:10.000Z","published_by":1},{"id":83,"uuid":"e156c8f4-ca0b-4ede-9ded-4216895dc149","title":"林軒田教授機器學習技法 Machine Learning Techniques 第 4 講學習筆記","slug":"lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-4-jiang-xue-xi-bi-ji","markdown":"### 前言\n\n本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 [第 3 講](http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-3-jiang-xue-xi-bi-ji/) 的碼農們，我建議可以先回頭去讀一下再回來喔！\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 熱身回顧一下\n\n在上一講中，我們介紹了使用 kernel 這樣的方法來處理高維度特徵的轉換，如此我們就能省下在高維度空間進行的運算，也因此無限多維的轉換也能輕易做到，讓 SVM 可能有更強的效果。\n\n截至目前為止所學的 SVM 模型都是 Hard Margin SVM，這樣的 SVM 就是會將資料完美的分好，也因此在越強的學習模型中越可能會有 Overfiting 的情況發生（雖然 fat margin 有避免一些，但可能還是會發生）。所以這一講我們希望能允許 SVM 能容忍一些小錯誤（雜訊），這樣的 SVM 就是 Soft Margin SVM。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-1.png\">\n</p>\n\n### Hard-Margin SVM 的缺點\n\n由於 Hard Margin SVM 堅持分好資料，所以在高維 Polynomial 及 Gaussian SVM 的學習模型可能會有 Overfitting 的現象，即使 SVM 的 Fat Margin 性質可以避掉一些，但 Overfitting 還是有可能發生，如下圖。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-2.png\">\n</p>\n\n### 放棄一些小錯\n\n回想之前學過的 Pocket PLA，我們是否也可以讓 SVM 也可以放棄一些小錯？在這邊我們將 SVM 要最佳化的式子加上了容錯項，在做錯的點上面允許 y_n(W^TZ_n + b) >= 負無限大，也就是說錯了也沒關係，如此最佳化時就能允許錯誤了。其中 C 可以調整 large margin 跟容錯項，C 越大代表容錯越小，C 越小代表容錯越大。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-3.png\">\n</p>\n\n### Soft Margin SVM 數學式（1）\n\n整理一下 Soft Margin SVM 的數學式，兩個限制式可以再合起來如下圖。但這個數學式不再是原來的 QP 問題了，而且目前的數學式是無法記錄犯了小錯或是大錯。\n\n所以我們想辦法講原來的數學式轉換成可以記錄犯了多少錯，將犯了多少錯記錄在 xi 裡，如此就將原來的數學式轉換成線性的形式了。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-4.png\">\n</p>\n\n### Soft Margin SVM 數學式（2）\n\nxi 記錄的錯誤如下圖所示，表示 xi 違反了 margin 多少量，離 margin 越遠的，xi 值會越大，所以在 Soft-Margin SVM 我們除了最佳化 w, b，也要最佳化 xi。其中 C 可以調整 large margin 跟容錯項，C 越大代表容錯越小，C 越小代表容錯越大，margin 也就越大。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-5.png\">\n</p>\n\n### Largrange Dual 解 Soft Margin SVM\n\n我們可以仿造上一講的 Dual 方法解 Soft Margin SVM。由於我們多了 xi 這 N 個變數，所以我們必須多出 N 個 b_n Largrange Multiplier。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-6.png\">\n</p>\n\n### 簡化 xi 及 b_n\n\n然後對 xi 偏微分，得到在最佳解時 0 = C - a_n - b_n。將最佳解時的條件帶回原式，我們會得到更簡化的式子如下圖所示。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-7.png\">\n</p>\n\n### 再簡化\n\n我們繼續對數學式簡化，對 b 進行偏微分、對 w 進行偏微分，都可以得到在最佳解時的限制式。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-8.png\">\n</p>\n\n### Soft Margin SVM 的標準對偶數學式\n\n經過上述處理之後，Soft Margin SVM 的標準對偶數學式如下圖所示，藍色的部分是跟 Hard Margin 不一樣的地方，這是一個 QP 問題，只要帶入 QP Solver 就可以解出來。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-9.png\">\n</p>\n\n### Kernel Soft Margin SVM\n\n仿造上一講介紹的 Kernel 方法，我們可以輕易做到 Kernel Soft Margin SVM，如此就可以進行無限多維轉換，alpah 可以使用 QP 算出來，整個算法幾乎跟 Hard Margin SVM 一摸一樣，只是多了一個上界的條件。現在問題只剩下 b 如何求得？\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-10.png\">\n</p>\n\n### 求得 b\n\n如何求得 b 的方法跟 Hard Margin 一樣都是使用 complementary slackness 的性質。在 alpha 大於 0 的這些點可以用來求得 b。但這個式子還要先求得 xi。xi 則是可以在 alpah < C 的這些點求得，因此 b 就可以求得了。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-11.png\">\n</p>\n\n### 觀察 Soft Margin Gaussian SVM\n\n我們來觀察一下 Soft Margin Gaussian SVM，其實如果使用不當還是會有 Overfitting 的現象產生。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-12.png\">\n</p>\n\n### alpha 的物理意義\n\n我們使用特出的 alpha 來計算出 w 及 b，這些 alpha 隱含著什麼物理意義呢？其中那些 alpha=0 的，就是對於 margin 沒有意義的點。alpha 大於 0 小於 C 的就是在邊界上的點。alpha = C 的，就是代表 xi 有值的點，就代表有違反邊界的點。我們可以利用 alpha 的性質來做一些資料分析。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-13.png\">\n</p>\n\n### 選擇 Model\n\n我們可以使用 C 及 gamma 來挑整 Soft Gaussian SVM，那怎麼挑選 C 及 gamma 參數呢？\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-14.png\">\n</p>\n\n### 使用 Cross Validation 來選\n\n當然還是可以用之前學過的 cross validation 來挑選。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-15.png\">\n</p>\n\n### Leave-One-Out CV Error 與 SVM 的關係\n\n這邊有一個 SVM 的特殊性質，SVM 的 Support Vector 數量除以 dataset 數量會是 Leave-One-Out Cross Validation Error 的上界。（non-SV 的 leave-one-out error 是 0，SV 的 leave-one out error 是 0 或 1，所以整體的 error 就是 #SV/N）\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-16.png\">\n</p>\n\n### 用 #SV 來選擇\n\n了解了上述的性質之後，我們也可以利用這個性質來選擇模型，如果 Cross Validation 會計算很久，我們可以簡單的利用 Support Vector 的數量來選擇模型，理論上可以選到 Leave-One-Out CV Error 的上限。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-17.png\">\n</p>\n\n### 總結\n\n在這一講我們了解了 Soft-Margin SVM 的演算法，並且了解了各種 Support Vector 所代表的物理意義，Support Vector 跟 Leave-One-Out Cross Validation 之間的關係也可以讓我們用來選擇模型。\n\nSoft-Margin SVM 也是大家一般口中所說的 SVM，一般都使用在分類問題上，之後的課程我們將介紹如何運用 SVM 相關的技巧來解不是二元分類的問題。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-18.png\">\n</p>\n","html":"<h3 id=\"\">前言</h3>\n\n<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href=\"http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-3-jiang-xue-xi-bi-ji/\">第 3 講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">熱身回顧一下</h3>\n\n<p>在上一講中，我們介紹了使用 kernel 這樣的方法來處理高維度特徵的轉換，如此我們就能省下在高維度空間進行的運算，也因此無限多維的轉換也能輕易做到，讓 SVM 可能有更強的效果。</p>\n\n<p>截至目前為止所學的 SVM 模型都是 Hard Margin SVM，這樣的 SVM 就是會將資料完美的分好，也因此在越強的學習模型中越可能會有 Overfiting 的情況發生（雖然 fat margin 有避免一些，但可能還是會發生）。所以這一講我們希望能允許 SVM 能容忍一些小錯誤（雜訊），這樣的 SVM 就是 Soft Margin SVM。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-1.png\">\n</p>\n\n<h3 id=\"hardmarginsvm\">Hard-Margin SVM 的缺點</h3>\n\n<p>由於 Hard Margin SVM 堅持分好資料，所以在高維 Polynomial 及 Gaussian SVM 的學習模型可能會有 Overfitting 的現象，即使 SVM 的 Fat Margin 性質可以避掉一些，但 Overfitting 還是有可能發生，如下圖。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-2.png\">\n</p>\n\n<h3 id=\"\">放棄一些小錯</h3>\n\n<p>回想之前學過的 Pocket PLA，我們是否也可以讓 SVM 也可以放棄一些小錯？在這邊我們將 SVM 要最佳化的式子加上了容錯項，在做錯的點上面允許 y<em>n(W^TZ</em>n + b) >= 負無限大，也就是說錯了也沒關係，如此最佳化時就能允許錯誤了。其中 C 可以調整 large margin 跟容錯項，C 越大代表容錯越小，C 越小代表容錯越大。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-3.png\">\n</p>\n\n<h3 id=\"softmarginsvm1\">Soft Margin SVM 數學式（1）</h3>\n\n<p>整理一下 Soft Margin SVM 的數學式，兩個限制式可以再合起來如下圖。但這個數學式不再是原來的 QP 問題了，而且目前的數學式是無法記錄犯了小錯或是大錯。</p>\n\n<p>所以我們想辦法講原來的數學式轉換成可以記錄犯了多少錯，將犯了多少錯記錄在 xi 裡，如此就將原來的數學式轉換成線性的形式了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-4.png\">\n</p>\n\n<h3 id=\"softmarginsvm2\">Soft Margin SVM 數學式（2）</h3>\n\n<p>xi 記錄的錯誤如下圖所示，表示 xi 違反了 margin 多少量，離 margin 越遠的，xi 值會越大，所以在 Soft-Margin SVM 我們除了最佳化 w, b，也要最佳化 xi。其中 C 可以調整 large margin 跟容錯項，C 越大代表容錯越小，C 越小代表容錯越大，margin 也就越大。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-5.png\">\n</p>\n\n<h3 id=\"largrangedualsoftmarginsvm\">Largrange Dual 解 Soft Margin SVM</h3>\n\n<p>我們可以仿造上一講的 Dual 方法解 Soft Margin SVM。由於我們多了 xi 這 N 個變數，所以我們必須多出 N 個 b_n Largrange Multiplier。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-6.png\">\n</p>\n\n<h3 id=\"xib_n\">簡化 xi 及 b_n</h3>\n\n<p>然後對 xi 偏微分，得到在最佳解時 0 = C - a<em>n - b</em>n。將最佳解時的條件帶回原式，我們會得到更簡化的式子如下圖所示。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-7.png\">\n</p>\n\n<h3 id=\"\">再簡化</h3>\n\n<p>我們繼續對數學式簡化，對 b 進行偏微分、對 w 進行偏微分，都可以得到在最佳解時的限制式。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-8.png\">\n</p>\n\n<h3 id=\"softmarginsvm\">Soft Margin SVM 的標準對偶數學式</h3>\n\n<p>經過上述處理之後，Soft Margin SVM 的標準對偶數學式如下圖所示，藍色的部分是跟 Hard Margin 不一樣的地方，這是一個 QP 問題，只要帶入 QP Solver 就可以解出來。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-9.png\">\n</p>\n\n<h3 id=\"kernelsoftmarginsvm\">Kernel Soft Margin SVM</h3>\n\n<p>仿造上一講介紹的 Kernel 方法，我們可以輕易做到 Kernel Soft Margin SVM，如此就可以進行無限多維轉換，alpah 可以使用 QP 算出來，整個算法幾乎跟 Hard Margin SVM 一摸一樣，只是多了一個上界的條件。現在問題只剩下 b 如何求得？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-10.png\">\n</p>\n\n<h3 id=\"b\">求得 b</h3>\n\n<p>如何求得 b 的方法跟 Hard Margin 一樣都是使用 complementary slackness 的性質。在 alpha 大於 0 的這些點可以用來求得 b。但這個式子還要先求得 xi。xi 則是可以在 alpah &lt; C 的這些點求得，因此 b 就可以求得了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-11.png\">\n</p>\n\n<h3 id=\"softmargingaussiansvm\">觀察 Soft Margin Gaussian SVM</h3>\n\n<p>我們來觀察一下 Soft Margin Gaussian SVM，其實如果使用不當還是會有 Overfitting 的現象產生。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-12.png\">\n</p>\n\n<h3 id=\"alpha\">alpha 的物理意義</h3>\n\n<p>我們使用特出的 alpha 來計算出 w 及 b，這些 alpha 隱含著什麼物理意義呢？其中那些 alpha=0 的，就是對於 margin 沒有意義的點。alpha 大於 0 小於 C 的就是在邊界上的點。alpha = C 的，就是代表 xi 有值的點，就代表有違反邊界的點。我們可以利用 alpha 的性質來做一些資料分析。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-13.png\">\n</p>\n\n<h3 id=\"model\">選擇 Model</h3>\n\n<p>我們可以使用 C 及 gamma 來挑整 Soft Gaussian SVM，那怎麼挑選 C 及 gamma 參數呢？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-14.png\">\n</p>\n\n<h3 id=\"crossvalidation\">使用 Cross Validation 來選</h3>\n\n<p>當然還是可以用之前學過的 cross validation 來挑選。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-15.png\">\n</p>\n\n<h3 id=\"leaveoneoutcverrorsvm\">Leave-One-Out CV Error 與 SVM 的關係</h3>\n\n<p>這邊有一個 SVM 的特殊性質，SVM 的 Support Vector 數量除以 dataset 數量會是 Leave-One-Out Cross Validation Error 的上界。（non-SV 的 leave-one-out error 是 0，SV 的 leave-one out error 是 0 或 1，所以整體的 error 就是 #SV/N）</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-16.png\">\n</p>\n\n<h3 id=\"sv\">用 #SV 來選擇</h3>\n\n<p>了解了上述的性質之後，我們也可以利用這個性質來選擇模型，如果 Cross Validation 會計算很久，我們可以簡單的利用 Support Vector 的數量來選擇模型，理論上可以選到 Leave-One-Out CV Error 的上限。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-17.png\">\n</p>\n\n<h3 id=\"\">總結</h3>\n\n<p>在這一講我們了解了 Soft-Margin SVM 的演算法，並且了解了各種 Support Vector 所代表的物理意義，Support Vector 跟 Leave-One-Out Cross Validation 之間的關係也可以讓我們用來選擇模型。</p>\n\n<p>Soft-Margin SVM 也是大家一般口中所說的 SVM，一般都使用在分類問題上，之後的課程我們將介紹如何運用 SVM 相關的技巧來解不是二元分類的問題。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-4-18.png\">\n</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-06-13T11:50:21.000Z","created_by":1,"updated_at":"2016-08-09T05:41:38.000Z","updated_by":1,"published_at":"2016-06-13T13:06:59.000Z","published_by":1},{"id":84,"uuid":"21b6a19a-5c26-4fd3-aee5-d3d767dfa5a3","title":"林軒田教授機器學習技法 Machine Learning Techniques 第 5 講學習筆記","slug":"lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-5-jiang-xue-xi-bi-ji","markdown":"### 前言\n\n本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 [第 4 講](http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-4-jiang-xue-xi-bi-ji/) 的碼農們，我建議可以先回頭去讀一下再回來喔！\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 熱身回顧一下\n\n在上一講中，我們介紹了 Soft Margin SVM，讓 SVM 可以容忍一些小錯以避免 Overfitting，由於強度與容忍度兼具，Soft Margin SVM 比較通用，其實大家平常口中所說的 SVM 就是指 Soft Margin SVM。\n\n前面四講我們都在討論 SVM 這個分類演算法，那我們有可能用 SVM 來做 Logistic Regression 或是 Regression 嗎？在這一講中我們將介紹如何使用 SVM 的方法來做 Logistic Regression。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-5-1.png\">\n</p>\n\n### 觀察 SVM 的容錯項\n\n我觀察一下 Soft Margin SVM 的容錯項，我們可以把原本的限制式整合到要最小化的式子裡來看看，如下圖所示，如此就沒有限制式了。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-5-2.png\">\n</p>\n\n### 觀察沒有限制式的 SVM 數學式\n\n我們再仔細觀察一下沒有限制式的 SVM 數學式，發現形式跟 L2 regularized Logistic Regression 有點像，只是沒有限制式的 SVM 數學式有個 max 的函數在裡面，這樣的數學式不再是一個 QP 問題了，然後也不是一個可以微分的式子，因此很難最佳化。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-5-3.png\">\n</p>\n\n### 觀察 SVM 錯誤衡量 function\n\n我們再仔細觀察一下 SVM 錯誤衡量 function，其實 err_svm 跟 err_0/1 在數線圖上 err_svm 會是 err_0/1 的上界，且邊界也很接近，所以我們可以說 SVM 與 L2-regularized logistic regression 是很接近的。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-5-6.png\">\n</p>\n\n### 第一個方法 Two-Level Learning\n\n怎麼讓 SVM 做 Logistic Regression 呢？一個做法是使用 Two-Level Learning，也就是先做 SVM，然後將原來的 X 計算分數（轉換到 SVM 的空間）之後，再對新的 X 以及 Y 做 Logistic Regression 學習 A 與 B。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-5-8.png\">\n</p>\n\n### Probabilistic SVM\n\n這就是 Probabilistic SVM，具體演算法如下，但仔細研究這個算法的背後意涵，這樣的做法並不是讓 Logistic Regression 在 z 空間做最佳解，有其它方法可以讓 Logistic 真正在 z 空間算最佳解嗎？\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-5-9.png\">\n</p>\n\n### Kernel Trick 背後的關鍵\n\n我們了解一下 SVM 使用的 Kernel Trick，SVM 其實有在 z 空間算最佳解，只是用了 Kernel Trick 來省下計算時間，然後算出的 w 其實就是某種 z 空間的資料線性組合。SVM 是取 support vector 的線性組合、PLA 是取錯誤資料的線性組合、Logistic Regression 是取梯度下降的線性組合，所以只要 w 是一種 z 空間的線性組合的形式，那就可以使用 Kernel Trick。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-5-10.png\">\n</p>\n\n### Kernel Logistic Regression\n\n我們將原本的 L2-Regularized Logistic Regression 數學式使用 w 是一種 z 空間線性組合的形式帶進去，得到如下圖數學式，而這數學式是可以最佳化的，所以我們可以使用之前的梯度下降法、隨機梯度下降法來求得最佳的 beta。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-5-11.png\">\n</p>\n\n### 總結\n\n在這一講中，我們了解了如何使用 SVM 來解 Logistic Regression 的問題，一個是使用 SVM 做轉換的 Probabilistic SVM，一個是使用 SVM  Kernel Trick 所啟發的 Kernel Logistic Rregression。下一講我們將繼續介紹如何延伸到解 Regression 的問題。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-5-13.png\">\n</p>\n","html":"<h3 id=\"\">前言</h3>\n\n<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href=\"http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-4-jiang-xue-xi-bi-ji/\">第 4 講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">熱身回顧一下</h3>\n\n<p>在上一講中，我們介紹了 Soft Margin SVM，讓 SVM 可以容忍一些小錯以避免 Overfitting，由於強度與容忍度兼具，Soft Margin SVM 比較通用，其實大家平常口中所說的 SVM 就是指 Soft Margin SVM。</p>\n\n<p>前面四講我們都在討論 SVM 這個分類演算法，那我們有可能用 SVM 來做 Logistic Regression 或是 Regression 嗎？在這一講中我們將介紹如何使用 SVM 的方法來做 Logistic Regression。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-5-1.png\">\n</p>\n\n<h3 id=\"svm\">觀察 SVM 的容錯項</h3>\n\n<p>我觀察一下 Soft Margin SVM 的容錯項，我們可以把原本的限制式整合到要最小化的式子裡來看看，如下圖所示，如此就沒有限制式了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-5-2.png\">\n</p>\n\n<h3 id=\"svm\">觀察沒有限制式的 SVM 數學式</h3>\n\n<p>我們再仔細觀察一下沒有限制式的 SVM 數學式，發現形式跟 L2 regularized Logistic Regression 有點像，只是沒有限制式的 SVM 數學式有個 max 的函數在裡面，這樣的數學式不再是一個 QP 問題了，然後也不是一個可以微分的式子，因此很難最佳化。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-5-3.png\">\n</p>\n\n<h3 id=\"svmfunction\">觀察 SVM 錯誤衡量 function</h3>\n\n<p>我們再仔細觀察一下 SVM 錯誤衡量 function，其實 err<em>svm 跟 err</em>0/1 在數線圖上 err<em>svm 會是 err</em>0/1 的上界，且邊界也很接近，所以我們可以說 SVM 與 L2-regularized logistic regression 是很接近的。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-5-6.png\">\n</p>\n\n<h3 id=\"twolevellearning\">第一個方法 Two-Level Learning</h3>\n\n<p>怎麼讓 SVM 做 Logistic Regression 呢？一個做法是使用 Two-Level Learning，也就是先做 SVM，然後將原來的 X 計算分數（轉換到 SVM 的空間）之後，再對新的 X 以及 Y 做 Logistic Regression 學習 A 與 B。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-5-8.png\">\n</p>\n\n<h3 id=\"probabilisticsvm\">Probabilistic SVM</h3>\n\n<p>這就是 Probabilistic SVM，具體演算法如下，但仔細研究這個算法的背後意涵，這樣的做法並不是讓 Logistic Regression 在 z 空間做最佳解，有其它方法可以讓 Logistic 真正在 z 空間算最佳解嗎？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-5-9.png\">\n</p>\n\n<h3 id=\"kerneltrick\">Kernel Trick 背後的關鍵</h3>\n\n<p>我們了解一下 SVM 使用的 Kernel Trick，SVM 其實有在 z 空間算最佳解，只是用了 Kernel Trick 來省下計算時間，然後算出的 w 其實就是某種 z 空間的資料線性組合。SVM 是取 support vector 的線性組合、PLA 是取錯誤資料的線性組合、Logistic Regression 是取梯度下降的線性組合，所以只要 w 是一種 z 空間的線性組合的形式，那就可以使用 Kernel Trick。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-5-10.png\">\n</p>\n\n<h3 id=\"kernellogisticregression\">Kernel Logistic Regression</h3>\n\n<p>我們將原本的 L2-Regularized Logistic Regression 數學式使用 w 是一種 z 空間線性組合的形式帶進去，得到如下圖數學式，而這數學式是可以最佳化的，所以我們可以使用之前的梯度下降法、隨機梯度下降法來求得最佳的 beta。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-5-11.png\">\n</p>\n\n<h3 id=\"\">總結</h3>\n\n<p>在這一講中，我們了解了如何使用 SVM 來解 Logistic Regression 的問題，一個是使用 SVM 做轉換的 Probabilistic SVM，一個是使用 SVM  Kernel Trick 所啟發的 Kernel Logistic Rregression。下一講我們將繼續介紹如何延伸到解 Regression 的問題。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-5-13.png\">\n</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-06-28T07:00:36.000Z","created_by":1,"updated_at":"2016-08-09T05:42:03.000Z","updated_by":1,"published_at":"2016-06-28T07:41:37.000Z","published_by":1},{"id":86,"uuid":"979b078c-8617-466f-8d20-589c91823260","title":"林軒田教授機器學習技法 Machine Learning Techniques 第 6 講學習筆記","slug":"lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-6-jiang-xue-xi-bi-ji-2","markdown":"### 前言\n\n本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 [第 5 講](http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-5-jiang-xue-xi-bi-ji/) 的碼農們，我建議可以先回頭去讀一下再回來喔！\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 熱身回顧一下\n\n在上一講中，我們了解了如何使用 SVM 來解 Logistic Regression 的問題，一個是使用 SVM 做轉換的 Probabilistic SVM，一個是使用 SVM  Kernel Trick 所啟發的 Kernel Logistic Rregression。這一講我們將繼續介紹如何延伸到解 Regression 的問題。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-1.png\">\n</p>\n\n### 利用 Representer Theorem 延伸\n\n從數學模型上，我們發現 L2-regularized 線性模型都可以轉換成 Kernel 形式，而 Linear/Ridge Regression 都有公式解，那麼 Kernel Ridge Regession 也可以推導出公式解嗎？\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-2.png\">\n</p>\n\n### Kernel Ridge Regression 數學式\n\n我們使用 Representer Theorem 將 Kernel 應用至 Ridge Regression 的數學式上，得到以下 Kernel Ridge Regression 數學式。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-3.png\">\n</p>\n\n### 解 Kernel Ridge Regression 最佳化\n\n接下來我們利用偏微分來計算 Kernel Ridge Regreesion 數學式的最佳化，對 beta 進行偏微分為 0 時即可求得 beta 最佳解。我們可以很簡易的用這個式子做到非線性的 Regression。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-4.png\">\n</p>\n\n### Linear 及 Kernel Ridge Regression 的比較\n\nLinear 及 Kernel Ridge Regression 比較起來，當然 Linear 模型較簡易，因此計算效能較好，而 Kernel Ridge Regression 由於可以做非線性轉換，因此有更強的彈性。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-5.png\">\n</p>\n\n### Soft-Margin SVM 與 Least-Squares SVM 的比較\n\n當我們使用 Kernel Ridge Regression 做分類時，這就是 Least-Squares SVM，Least-Squares SVM 與 Soft-Margin SVM 比較起來，他們的邊界會很接近，但會有更多的 Support Vector，如此在做預測時會慢一些。Suppport Vector 的數量跟 beta 有關，我們可以讓 beta 變得跟標準的 SVM 一樣稀疏嗎？\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-6.png\">\n</p>\n\n### Tube Regression 模型\n\n我們重新思考一個新模型 Tube Regression，讓錯誤在一定範圍內為 0，當錯誤超過界線時，我們再以錯誤的點與邊界的距離當作錯誤值。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-7.png\">\n</p>\n\n### L2-Regularized Tube Regression\n\n將 Tube Regression 的性質帶入 L2-Regularized，與 SVM 對照一下，目前的 L2-Regularized Tube Regression 並不能微分，雖然可以 Kernel 化，但卻不知有沒有跟 SVM 一樣有稀疏 Support Vector 的性質。\n\n我們將 L2-Regularized Tube Regression 改成跟 SVM 幾乎一樣的形式來求解看看，這就是 Support Vector Regression。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-8.png\">\n</p>\n\n### Support Vector Regression Primal 形式\n\n首先我們來推導一下 Support Vector Regression 的 Primal 形式，由於目前的數學式有絕對值，所以我們使用上界的錯誤及下界的錯誤做展開，如下數學式。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-9.png\">\n</p>\n\n### 帶入 Lagrange Multiplier\n\n如同解 SVM 的對偶問題，我們使用了 Lagrange Multiplier 的技巧，這邊也是一樣，但由於有上界的錯誤及下界的錯誤，我們也需要有上界的 Lagrange Multiplier 及 下界的 Lagrange Multiplier。\n\n如此就可以仿造解 Dual SVM 一樣，去解出最佳解時 SVR 的 KKT Conditions。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-11.png\">\n</p>\n\n### 比較一下 SVM Dual 及 SVR Dual\n\nSVM Dual 及 SVR Dual 數學式比較如下圖所示，因此如同之前使用 QP Solver 解 SVM Dual，我們可以將 SVR Dual 對應的變數帶入 QP Solver 來解 SVR Dual。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-12.png\">\n</p>\n\n### SVR 的稀疏性質\n\n從 SVR 最佳解的條件中，當錯誤值在 tube 範圍內時，我們會訂為 0，然後因為點在 Tube 裡面，所以 y 跟分數的差值是不等於 0 的，依照 complementary slackness，如此 alpha 上界跟 alpha 下界就都是 0，alpha 上界與 alpha 下界相減是 beta，這樣就代表 beta 也是 0。而 Support Vetor 是 beta 不等於 0 的點，所以這就代表 SVR 有稀疏 Support Vetor 的性質。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-13.png\">\n</p>\n\n### 總結\n\n在這一講中我介紹了 Kernel Ridge Regression 及 Support Vetor Regression，有關 SVM 的相關模型已經都介紹完畢了，之後的課程將介紹如何像雞尾酒那樣結合各種學習模型。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-16.png\">\n</p>","html":"<h3 id=\"\">前言</h3>\n\n<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href=\"http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-5-jiang-xue-xi-bi-ji/\">第 5 講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">熱身回顧一下</h3>\n\n<p>在上一講中，我們了解了如何使用 SVM 來解 Logistic Regression 的問題，一個是使用 SVM 做轉換的 Probabilistic SVM，一個是使用 SVM  Kernel Trick 所啟發的 Kernel Logistic Rregression。這一講我們將繼續介紹如何延伸到解 Regression 的問題。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-1.png\">\n</p>\n\n<h3 id=\"representertheorem\">利用 Representer Theorem 延伸</h3>\n\n<p>從數學模型上，我們發現 L2-regularized 線性模型都可以轉換成 Kernel 形式，而 Linear/Ridge Regression 都有公式解，那麼 Kernel Ridge Regession 也可以推導出公式解嗎？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-2.png\">\n</p>\n\n<h3 id=\"kernelridgeregression\">Kernel Ridge Regression 數學式</h3>\n\n<p>我們使用 Representer Theorem 將 Kernel 應用至 Ridge Regression 的數學式上，得到以下 Kernel Ridge Regression 數學式。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-3.png\">\n</p>\n\n<h3 id=\"kernelridgeregression\">解 Kernel Ridge Regression 最佳化</h3>\n\n<p>接下來我們利用偏微分來計算 Kernel Ridge Regreesion 數學式的最佳化，對 beta 進行偏微分為 0 時即可求得 beta 最佳解。我們可以很簡易的用這個式子做到非線性的 Regression。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-4.png\">\n</p>\n\n<h3 id=\"linearkernelridgeregression\">Linear 及 Kernel Ridge Regression 的比較</h3>\n\n<p>Linear 及 Kernel Ridge Regression 比較起來，當然 Linear 模型較簡易，因此計算效能較好，而 Kernel Ridge Regression 由於可以做非線性轉換，因此有更強的彈性。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-5.png\">\n</p>\n\n<h3 id=\"softmarginsvmleastsquaressvm\">Soft-Margin SVM 與 Least-Squares SVM 的比較</h3>\n\n<p>當我們使用 Kernel Ridge Regression 做分類時，這就是 Least-Squares SVM，Least-Squares SVM 與 Soft-Margin SVM 比較起來，他們的邊界會很接近，但會有更多的 Support Vector，如此在做預測時會慢一些。Suppport Vector 的數量跟 beta 有關，我們可以讓 beta 變得跟標準的 SVM 一樣稀疏嗎？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-6.png\">\n</p>\n\n<h3 id=\"tuberegression\">Tube Regression 模型</h3>\n\n<p>我們重新思考一個新模型 Tube Regression，讓錯誤在一定範圍內為 0，當錯誤超過界線時，我們再以錯誤的點與邊界的距離當作錯誤值。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-7.png\">\n</p>\n\n<h3 id=\"l2regularizedtuberegression\">L2-Regularized Tube Regression</h3>\n\n<p>將 Tube Regression 的性質帶入 L2-Regularized，與 SVM 對照一下，目前的 L2-Regularized Tube Regression 並不能微分，雖然可以 Kernel 化，但卻不知有沒有跟 SVM 一樣有稀疏 Support Vector 的性質。</p>\n\n<p>我們將 L2-Regularized Tube Regression 改成跟 SVM 幾乎一樣的形式來求解看看，這就是 Support Vector Regression。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-8.png\">\n</p>\n\n<h3 id=\"supportvectorregressionprimal\">Support Vector Regression Primal 形式</h3>\n\n<p>首先我們來推導一下 Support Vector Regression 的 Primal 形式，由於目前的數學式有絕對值，所以我們使用上界的錯誤及下界的錯誤做展開，如下數學式。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-9.png\">\n</p>\n\n<h3 id=\"lagrangemultiplier\">帶入 Lagrange Multiplier</h3>\n\n<p>如同解 SVM 的對偶問題，我們使用了 Lagrange Multiplier 的技巧，這邊也是一樣，但由於有上界的錯誤及下界的錯誤，我們也需要有上界的 Lagrange Multiplier 及 下界的 Lagrange Multiplier。</p>\n\n<p>如此就可以仿造解 Dual SVM 一樣，去解出最佳解時 SVR 的 KKT Conditions。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-11.png\">\n</p>\n\n<h3 id=\"svmdualsvrdual\">比較一下 SVM Dual 及 SVR Dual</h3>\n\n<p>SVM Dual 及 SVR Dual 數學式比較如下圖所示，因此如同之前使用 QP Solver 解 SVM Dual，我們可以將 SVR Dual 對應的變數帶入 QP Solver 來解 SVR Dual。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-12.png\">\n</p>\n\n<h3 id=\"svr\">SVR 的稀疏性質</h3>\n\n<p>從 SVR 最佳解的條件中，當錯誤值在 tube 範圍內時，我們會訂為 0，然後因為點在 Tube 裡面，所以 y 跟分數的差值是不等於 0 的，依照 complementary slackness，如此 alpha 上界跟 alpha 下界就都是 0，alpha 上界與 alpha 下界相減是 beta，這樣就代表 beta 也是 0。而 Support Vetor 是 beta 不等於 0 的點，所以這就代表 SVR 有稀疏 Support Vetor 的性質。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-13.png\">\n</p>\n\n<h3 id=\"\">總結</h3>\n\n<p>在這一講中我介紹了 Kernel Ridge Regression 及 Support Vetor Regression，有關 SVM 的相關模型已經都介紹完畢了，之後的課程將介紹如何像雞尾酒那樣結合各種學習模型。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-6-16.png\">\n</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-08-06T05:51:47.000Z","created_by":1,"updated_at":"2016-08-09T05:42:27.000Z","updated_by":1,"published_at":"2016-08-06T06:53:29.000Z","published_by":1},{"id":87,"uuid":"a04aa1fc-ea81-449a-a8fd-f920d8932f8d","title":"林軒田教授機器學習技法 Machine Learning Techniques 第 7 講學習筆記","slug":"lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-7-jiang-xue-xi-bi-ji","markdown":"### 前言\n\n本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 [第 6 講](http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-6-jiang-xue-xi-bi-ji/) 的碼農們，我建議可以先回頭去讀一下再回來喔！\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 熱身回顧一下\n\n前 6 講我們對 SVM 做了完整的介紹，從基本的 SVM 分類器到使用 Support Vector 性質發展出來的 regression 演算法 SVR，在機器學習基石中學過的各種問題，SVM 都有對應的演算法可以解。\n\n第 7 講我們要介紹 Aggregation Models，顧名思義就是要講多種模型結合起來，看能不能在機器學習上有更好的效果。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-1.png\">\n</p>\n\n### Aggregation 的故事\n\n我們用一個簡單的故事來說明 Aggregation，假設現在你有很多個朋友可以預測股票會漲還是會跌，那你要選擇相信誰的說法呢？這就像我們有很多個機器學習預測模型，我們要選擇哪一個來做預測。\n\n一個方式是選擇裡面最準的那一個人的說法，在機器學習就是使用 Validation 來做選擇。\n\n一個方式是綜合所有人的意見，每個人代表一票，然後選擇票數最多的預測。在機器學習也可以用這樣的方法綜合所有模型的預測。\n\n另一個方式也是綜合所有人的意見，只是每個人的票數不一樣，比較準的人票數較多，比較沒那麼準的人票數較少。在機器學習上，我們也可以為每個模型放上不同的權重來做到這樣的效果。\n\n最後一個方式就是會依據條件來選擇相信誰的說法，因為每個人擅長的預測可能不多，有的人擅長科技類股，有的人擅長傳統類股，所以我們需要依據條件來做調整。在機器學習上也會有類似的演算法來整合各個預測模型。\n\nAggregation 大致就是依照上述方式來整合各個模型。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-2.png\">\n</p>\n\n### 用 Validation 選擇預測模型\n\n我們已經學過如何使用 Validation 來選擇預測模型，這個方式有一個問題就是，需要其中有一個強的預設模型才會有用，如果所有的預設模型都不準確，那也只是從廢渣裡面選一個比較不廢的而已。\n\n所以 Validation 在機器學習上還是有一些限制的，那我們有辦法透過 Aggregation 來讓所有的廢渣整合起來，然後變強，讓預測變得更準確嗎？\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-3.png\">\n</p>\n\n### 為何 Aggregation 會有用？\n\n首先我們看左圖，如果現在預設模型只能切垂直線或水平線，其實預測效果可想而知是不會好到哪裡去的。但是如果我們將多個垂直線或水平線的預測模型整合起來，就有辦法做好一些更複雜的分類。這某種程度像是做了特徵轉換到高維度，讓預測模型變得更強、更準確。\n\n再來我們看右圖，我們知道 PLA 因為隨機的性質，每次算出來的分類線可能會不太一樣，如果我們將所有的分類線平均整合起來，那就會得到比較中間的線，得到的線會跟 SVM 的 large margin 分類線有點類似。這某種程度就像是做了正規化，能夠避免 Overfitting。\n\n這給了我們一個啟示，以往的學習模型，只要越強大，那就越容易發生 Overfitting，但如果越正規畫，有時可能會有 Underfitting。而 Aggregatin 卻可以一邊變強又一邊有正規化的效果。\n\n所以適當的使用 Aggregation 的技巧是可以讓機器學習的效果更好的。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-4.png\">\n</p>\n\n### Uniform Blending 用在分類上\n\n現在我們介紹一個最簡單的 Aggregation 方法 - Uniform Blending，在分類問題上，我們只要很單純地將所有的預測模型的預測結果加起來，然後看正負號就可以做到分類的 Uniform Blending。\n\n這個方法要注意整合的預測模型需要有差異性，不能每個模型預測結果都是一致的，這樣就會沒有效果。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-5.png\">\n</p>\n\n### Uniform Blending 用在迴歸上\n\n在迴歸問題使用 Uniform Blending 就是將所有預設模型的預測結果加起來，然後除上模型的個數，也就是做平均的意思，這樣就可以做到 Aggregation 了。\n\n但同樣也要注意整合的預設模型需要有差異性才會有效果。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-6.png\">\n</p>\n\n### Linear Blending\n\nUniform Bleding 每個預測模型的是一視同仁，可能不夠強大，我們可以使用 Linear Blending 來給每個預設模型不一樣的權重，數學式如下圖所示，其中權重是大於等於 0 的。\n\n演算法的調整方法，其實就是先將資料透過每個模型做預測當成是一種特徵轉換，然後將轉換過後的資料當成是新的資料來做訓練，再使用 Linear Regressiong 算出每個預測模型的權重。未來預測時也是用每個模型轉換過後的資料再依權重做計算。\n\n不過數學式上的條件還有權重都要大於等於 0，這在我們目前的演算法並沒有考慮進去。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-7.png\">\n</p>\n\n### 權重大於等於 0 可以忽略\n\n在 Linear Regression 算權重時，權重可能會有小於 0 的情況，在物理意義上就代表這個模型猜得很不準，所以物理意義上就像是我們把它用來當成反指標，所以它也是對預設有幫助的。因此權重大於等於 0 這個條件我們可以忽略，\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-8.png\">\n</p>\n\n### 差異化的模型\n\n目前我們已經大致學會了幾種 Aggregation 的方法，都需要整合的模型之間有差異化。那我們怎麼得到有差異化的模型呢？\n\n一個就是本來就是演算法哲學不同的模型；一個是從參數來調出差異化；而有隨機性的模型其實每次得到的預測模型也會有差異化；另外一個方式我們可以從資料面（每次訓練餵不一樣的模型）來做出模型的差異化。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-9.png\">\n</p>\n\n### Bootstrap Aggregation 資料差異化的整合\n\n我們如何做到資料差異化的整合呢？如果我們能夠一直不斷的得到不同的 N 筆資料來做訓練，那就可以很容易地做到了。但我們手上只有原本的 N 筆訓練資料，不可能再拿到其他訓練資料，實務上我們的做法就會是從這 N 筆資料中做取後放回的抽樣抽出 N 筆資料，這樣我們每一輪都會得到不同訓練資料（由於是取後放回，N 筆資料中會有重複的資料），這樣就可以用來訓練有差異化的預測模型。\n\n這種 Bootstrap Aggregation 也叫做 Bagging。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-10.png\">\n</p>\n\n### Pocket 使用 Bagging Aggregation 的效果\n\n下圖是 Pocket 使用 Bagging Aggregation 的實例效果，可以看出每個分類線是有差異的，而整合起來又有非線性分類的效果，Bagging 這個方法在有隨機性質的算法上理論上都是有用的。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-11.png\">\n</p>\n\n### 總結\n\n這一講就是介紹如何整合很多個預測模型的預測結果，理論上是可以帶來更好的效果的，所以如果訓練出來的模型都是廢渣的話，也許用 Aggregation 的技巧就會讓效果變好。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-12.png\">\n</p>\n","html":"<h3 id=\"\">前言</h3>\n\n<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href=\"http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-6-jiang-xue-xi-bi-ji/\">第 6 講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">熱身回顧一下</h3>\n\n<p>前 6 講我們對 SVM 做了完整的介紹，從基本的 SVM 分類器到使用 Support Vector 性質發展出來的 regression 演算法 SVR，在機器學習基石中學過的各種問題，SVM 都有對應的演算法可以解。</p>\n\n<p>第 7 講我們要介紹 Aggregation Models，顧名思義就是要講多種模型結合起來，看能不能在機器學習上有更好的效果。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-1.png\">\n</p>\n\n<h3 id=\"aggregation\">Aggregation 的故事</h3>\n\n<p>我們用一個簡單的故事來說明 Aggregation，假設現在你有很多個朋友可以預測股票會漲還是會跌，那你要選擇相信誰的說法呢？這就像我們有很多個機器學習預測模型，我們要選擇哪一個來做預測。</p>\n\n<p>一個方式是選擇裡面最準的那一個人的說法，在機器學習就是使用 Validation 來做選擇。</p>\n\n<p>一個方式是綜合所有人的意見，每個人代表一票，然後選擇票數最多的預測。在機器學習也可以用這樣的方法綜合所有模型的預測。</p>\n\n<p>另一個方式也是綜合所有人的意見，只是每個人的票數不一樣，比較準的人票數較多，比較沒那麼準的人票數較少。在機器學習上，我們也可以為每個模型放上不同的權重來做到這樣的效果。</p>\n\n<p>最後一個方式就是會依據條件來選擇相信誰的說法，因為每個人擅長的預測可能不多，有的人擅長科技類股，有的人擅長傳統類股，所以我們需要依據條件來做調整。在機器學習上也會有類似的演算法來整合各個預測模型。</p>\n\n<p>Aggregation 大致就是依照上述方式來整合各個模型。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-2.png\">\n</p>\n\n<h3 id=\"validation\">用 Validation 選擇預測模型</h3>\n\n<p>我們已經學過如何使用 Validation 來選擇預測模型，這個方式有一個問題就是，需要其中有一個強的預設模型才會有用，如果所有的預設模型都不準確，那也只是從廢渣裡面選一個比較不廢的而已。</p>\n\n<p>所以 Validation 在機器學習上還是有一些限制的，那我們有辦法透過 Aggregation 來讓所有的廢渣整合起來，然後變強，讓預測變得更準確嗎？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-3.png\">\n</p>\n\n<h3 id=\"aggregation\">為何 Aggregation 會有用？</h3>\n\n<p>首先我們看左圖，如果現在預設模型只能切垂直線或水平線，其實預測效果可想而知是不會好到哪裡去的。但是如果我們將多個垂直線或水平線的預測模型整合起來，就有辦法做好一些更複雜的分類。這某種程度像是做了特徵轉換到高維度，讓預測模型變得更強、更準確。</p>\n\n<p>再來我們看右圖，我們知道 PLA 因為隨機的性質，每次算出來的分類線可能會不太一樣，如果我們將所有的分類線平均整合起來，那就會得到比較中間的線，得到的線會跟 SVM 的 large margin 分類線有點類似。這某種程度就像是做了正規化，能夠避免 Overfitting。</p>\n\n<p>這給了我們一個啟示，以往的學習模型，只要越強大，那就越容易發生 Overfitting，但如果越正規畫，有時可能會有 Underfitting。而 Aggregatin 卻可以一邊變強又一邊有正規化的效果。</p>\n\n<p>所以適當的使用 Aggregation 的技巧是可以讓機器學習的效果更好的。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-4.png\">\n</p>\n\n<h3 id=\"uniformblending\">Uniform Blending 用在分類上</h3>\n\n<p>現在我們介紹一個最簡單的 Aggregation 方法 - Uniform Blending，在分類問題上，我們只要很單純地將所有的預測模型的預測結果加起來，然後看正負號就可以做到分類的 Uniform Blending。</p>\n\n<p>這個方法要注意整合的預測模型需要有差異性，不能每個模型預測結果都是一致的，這樣就會沒有效果。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-5.png\">\n</p>\n\n<h3 id=\"uniformblending\">Uniform Blending 用在迴歸上</h3>\n\n<p>在迴歸問題使用 Uniform Blending 就是將所有預設模型的預測結果加起來，然後除上模型的個數，也就是做平均的意思，這樣就可以做到 Aggregation 了。</p>\n\n<p>但同樣也要注意整合的預設模型需要有差異性才會有效果。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-6.png\">\n</p>\n\n<h3 id=\"linearblending\">Linear Blending</h3>\n\n<p>Uniform Bleding 每個預測模型的是一視同仁，可能不夠強大，我們可以使用 Linear Blending 來給每個預設模型不一樣的權重，數學式如下圖所示，其中權重是大於等於 0 的。</p>\n\n<p>演算法的調整方法，其實就是先將資料透過每個模型做預測當成是一種特徵轉換，然後將轉換過後的資料當成是新的資料來做訓練，再使用 Linear Regressiong 算出每個預測模型的權重。未來預測時也是用每個模型轉換過後的資料再依權重做計算。</p>\n\n<p>不過數學式上的條件還有權重都要大於等於 0，這在我們目前的演算法並沒有考慮進去。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-7.png\">\n</p>\n\n<h3 id=\"0\">權重大於等於 0 可以忽略</h3>\n\n<p>在 Linear Regression 算權重時，權重可能會有小於 0 的情況，在物理意義上就代表這個模型猜得很不準，所以物理意義上就像是我們把它用來當成反指標，所以它也是對預設有幫助的。因此權重大於等於 0 這個條件我們可以忽略，</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-8.png\">\n</p>\n\n<h3 id=\"\">差異化的模型</h3>\n\n<p>目前我們已經大致學會了幾種 Aggregation 的方法，都需要整合的模型之間有差異化。那我們怎麼得到有差異化的模型呢？</p>\n\n<p>一個就是本來就是演算法哲學不同的模型；一個是從參數來調出差異化；而有隨機性的模型其實每次得到的預測模型也會有差異化；另外一個方式我們可以從資料面（每次訓練餵不一樣的模型）來做出模型的差異化。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-9.png\">\n</p>\n\n<h3 id=\"bootstrapaggregation\">Bootstrap Aggregation 資料差異化的整合</h3>\n\n<p>我們如何做到資料差異化的整合呢？如果我們能夠一直不斷的得到不同的 N 筆資料來做訓練，那就可以很容易地做到了。但我們手上只有原本的 N 筆訓練資料，不可能再拿到其他訓練資料，實務上我們的做法就會是從這 N 筆資料中做取後放回的抽樣抽出 N 筆資料，這樣我們每一輪都會得到不同訓練資料（由於是取後放回，N 筆資料中會有重複的資料），這樣就可以用來訓練有差異化的預測模型。</p>\n\n<p>這種 Bootstrap Aggregation 也叫做 Bagging。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-10.png\">\n</p>\n\n<h3 id=\"pocketbaggingaggregation\">Pocket 使用 Bagging Aggregation 的效果</h3>\n\n<p>下圖是 Pocket 使用 Bagging Aggregation 的實例效果，可以看出每個分類線是有差異的，而整合起來又有非線性分類的效果，Bagging 這個方法在有隨機性質的算法上理論上都是有用的。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-11.png\">\n</p>\n\n<h3 id=\"\">總結</h3>\n\n<p>這一講就是介紹如何整合很多個預測模型的預測結果，理論上是可以帶來更好的效果的，所以如果訓練出來的模型都是廢渣的話，也許用 Aggregation 的技巧就會讓效果變好。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-7-12.png\">\n</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-09-13T17:18:10.000Z","created_by":1,"updated_at":"2016-09-14T09:06:10.000Z","updated_by":1,"published_at":"2016-09-14T09:05:58.000Z","published_by":1},{"id":88,"uuid":"8bddbd12-af5b-4699-8537-7c0e54c7388a","title":"林軒田教授機器學習技法 Machine Learning Techniques 第 8 講學習筆記","slug":"lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-8-jiang-xue-xi-bi-ji","markdown":"### 前言\n\n本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 [第 7 講](http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-7-jiang-xue-xi-bi-ji/) 的碼農們，我建議可以先回頭去讀一下再回來喔！\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 熱身回顧一下\n\n上一講我們介紹了如何使用 Blending 及 Bagging 的技巧來做到 Aggregation Model，可以使用 Uniform 及 Linear 的方式融合不同的 Model。至於以 Non-linear 的方式融合 Model 就需要依據想展現的特性去調整演算法來做到，這一講將介紹 Adaptive Boosting 這種特別的演算法。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-1.png\">\n</p>\n\n### 幼稚園學生學認識蘋果的故事（一）\n\n我們用一個幼稚園學生在課堂上學認識蘋果的故事來作為開頭說明，在課堂上老師問 Michael 說「上面的圖片哪些是蘋果呢？」，Michael 回答「蘋果是圓的」，的確蘋果很多是圓的，但是有些水果是圓的但不是蘋果，有些蘋果也不一定是圓的，因此 Michael 的回答在藍色的這些圖片犯了錯誤。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-2.png\">\n</p>\n\n### 幼稚園學生學認識蘋果的故事（二）\n\n於是老師為了讓學生可以更精確地回答，將 Michael 犯錯的圖片放大了，答對的圖片則縮小了，讓學生的可針對這些錯誤再修正答案。於是 Tina 回答「蘋果是紅的」，這的確是一個很好的觀察，但一樣在底下藍色標示的這是個圖片犯了錯，番茄跟草莓也是紅的、青蘋果的話就是綠的。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-3.png\">\n</p>\n\n### 幼稚園學生學認識蘋果的故事（三）\n\n於是老師又將 Tina 犯錯的圖片放大了，答對的圖片縮小，讓學生繼續精確的回覆蘋果的特徵。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-4.png\">\n</p>\n\n### 動機\n\n這樣的教學過程也是一種可以用來教機器如何學習的過程，每個學生都只會一些簡單的假設 gt（蘋果是紅的），綜合所有學生的假設就可以好好地認識出蘋果的特徵形成 G，而老師則像是一個演算法一樣指導學生方向，讓錯誤越來越少。\n\n接下來我們就要介紹如何用演算法來模擬這樣的學習過程。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-5.png\">\n</p>\n\n### 有權重的 Ein\n\n老師調整圖片放大縮小的教學方式，在數學上我們可以為每個點犯錯時加上一個權重來表示。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-6.png\">\n</p>\n\n### 每一回合調整權重\n\n那我們如何調整權重呢？我們每次調整權重，是希望每個學生能學出不一樣的觀點，這樣才能配合所有學生的觀點做出對蘋果完整的認識，因此挑整權重時應該要讓第一個學習到的 gt 在 u(t+1) 時這樣的權重下表現很差。\n\n所謂的表現差就是表現跟丟銅板沒兩樣，也就是猜錯的情況為 1/2。（全猜錯其實算是一個好的表現）\n\n我們讓演算法根據新的權重 u(t+1)（會使 gt 表現很差）再去學習出新的 g(t+1)，這個 g(t+1) 就是一個觀點與 gt 不同，但表現卻也不錯的新假設了。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-8.png\">\n</p>\n\n### 調整權重的數學式\n\n調整權重的數學式如下，我們想讓 gt 在 u(t+1) 的情況下猜對跟猜錯的情況為 1/2，那其實就是將原本的 ut 在猜對時乘上錯誤率(incorrect rate)，在猜錯時乘上答對率（correct rate），這樣 gt 在 u(t+1) 的 Ein 就會是 1/2 了。\n\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-9.png\">\n</p>\n\n### 縮放因數\n\n演算法的作者在上述的概念上選擇了一個特別的縮放因數，但基本上就是剛剛上述所說的概念，這個縮放因數我們用 方塊t 來表示（如下圖），這在物理意義上有特別的意義，當 錯誤率 <= 1/2 時，縮放因數 方塊t 才會 >= 1，然後在計算下一輪的 ut 時，答錯的點會乘上 方塊t，答對的點會除上 方塊t，這就像老師在小朋友答錯的圖片放大、答對的圖片縮小那樣的教學過程。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-10.png\">\n</p>\n\n### 初步的演算法\n\n初步的演算法如下，首先第一輪的 u 對每個點都是相同的權重，這時我們學出第一個假設 gt，接下來使用 方塊t 調整權重得出 u(t+1)，然後繼續學出第二個假設 g(t+1)，我們可以繼續這樣的過程學出更多假設，現在只剩下一個問題，如何融合所有的假設呢？\n\n我們可以用之前學過的 blending 使用 uniform 或是 linear 來融合，但這邊作者選擇了一個特別的演算法在計算過程中融合所有的假設。\n\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-11.png\">\n</p>\n\n### 演算過程中就算出融合假設的權重\n\n這邊作者想了一個方法可以在演算過程中就算出融合假設的權重，這邊權重 alpha t = ln(方塊t)，這在物理意義上也有特別的意義。當錯誤率是 1/2 跟丟銅板沒兩樣時，那 方塊t 會等於 1，這時 ln(方塊t) 就會是 0，代表這個假設的權重是 0，一點用都沒有。當錯誤率是 0 的時候，那 方塊t 會等於無限大，這時 ln(方塊t) 就會是無限大，我們可以只看這個超強的假設 gt 就好。\n\n綜合以上，這個演算法就是 Adptive Boosting。我們有 weak learning 就上課堂上的幼稚園學生；我們有一個演算法調整權重就像老師會調整圖片大小引領學生學習；我們有一個融合假設的方法就像我們將課堂上所有學生的觀點融合起來一樣。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-12.png\">\n</p>\n\n### Adaptive Boosting(AdaBoost) 演算法\n\n詳細 Adaptive Boosting 演算法如下所示，中文又稱這個演算法就皮匠法。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-13.png\">\n</p>\n\n### 理論上的保證\n\n跟去理論上的保證只要 weak learner 比亂猜還好，那 AdaBoost 可以很容易地將 Ein 降到 0，而 dvc 的成長也相對慢，因此當資料量夠多時，Eout 理論保證會很小。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-14.png\">\n</p>\n\n### Weak Learner：Decision Stump\n\nAdaBoost 使用的 Weak Learner 是 Decision Stump，是一個只能在平面上切一條水平線或垂直線的 Weak Learner，詳細演算法如下，每個假設就是要學習出是一條水平線或垂直線（direction s），切在哪個 feature（feature i），切在那個值（threshold theta）。如果想要看程式碼可以參考：https://github.com/fukuball/fuku-ml/blob/master/FukuML/DecisionStump.py\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-15.png\">\n</p>\n\n### 我們用一個簡單的例子說明 AdaBoost\n\n我們用一個簡單的例子來說明 AdaBoost 的演算過程。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-16.png\">\n</p>\n\n### 第一輪\n\n第一輪先學出一個 weak learner 切了一個垂直線，這時犯錯的點會放大、答對的點縮小。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-17.png\">\n</p>\n\n### 第二輪\n\n根據犯錯放大的權重，再去學出一個不同觀點的 weak learner 再切了一條垂直線，根據答案再對點的權重調整。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-18.png\">\n</p>\n\n### 第三輪\n\n第三輪又學出了一個不同觀點的 weak learner 切了一條水平線，現在已經可以看出分界線慢慢變得複雜了。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-19.png\">\n</p>\n\n### 第五輪\n\n持續這樣的過程，AdaBoost 不斷地得出不同的 weak learner，綜合 weak learner 的答案便可以回答一些較複雜的問題。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-21.png\">\n</p>\n\n### 總結\n\n這一講介紹了 AdaBoost 這個特殊的機器學習演算法，能夠將表現只比丟銅板好一些的 Weak Learner 融合起來去得到更好的學習效果，算是從 Blending 技巧中衍伸出來的一個特殊演算法。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-22.png\">\n</p>","html":"<h3 id=\"\">前言</h3>\n\n<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href=\"http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-7-jiang-xue-xi-bi-ji/\">第 7 講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">熱身回顧一下</h3>\n\n<p>上一講我們介紹了如何使用 Blending 及 Bagging 的技巧來做到 Aggregation Model，可以使用 Uniform 及 Linear 的方式融合不同的 Model。至於以 Non-linear 的方式融合 Model 就需要依據想展現的特性去調整演算法來做到，這一講將介紹 Adaptive Boosting 這種特別的演算法。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-1.png\">\n</p>\n\n<h3 id=\"\">幼稚園學生學認識蘋果的故事（一）</h3>\n\n<p>我們用一個幼稚園學生在課堂上學認識蘋果的故事來作為開頭說明，在課堂上老師問 Michael 說「上面的圖片哪些是蘋果呢？」，Michael 回答「蘋果是圓的」，的確蘋果很多是圓的，但是有些水果是圓的但不是蘋果，有些蘋果也不一定是圓的，因此 Michael 的回答在藍色的這些圖片犯了錯誤。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-2.png\">\n</p>\n\n<h3 id=\"\">幼稚園學生學認識蘋果的故事（二）</h3>\n\n<p>於是老師為了讓學生可以更精確地回答，將 Michael 犯錯的圖片放大了，答對的圖片則縮小了，讓學生的可針對這些錯誤再修正答案。於是 Tina 回答「蘋果是紅的」，這的確是一個很好的觀察，但一樣在底下藍色標示的這是個圖片犯了錯，番茄跟草莓也是紅的、青蘋果的話就是綠的。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-3.png\">\n</p>\n\n<h3 id=\"\">幼稚園學生學認識蘋果的故事（三）</h3>\n\n<p>於是老師又將 Tina 犯錯的圖片放大了，答對的圖片縮小，讓學生繼續精確的回覆蘋果的特徵。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-4.png\">\n</p>\n\n<h3 id=\"\">動機</h3>\n\n<p>這樣的教學過程也是一種可以用來教機器如何學習的過程，每個學生都只會一些簡單的假設 gt（蘋果是紅的），綜合所有學生的假設就可以好好地認識出蘋果的特徵形成 G，而老師則像是一個演算法一樣指導學生方向，讓錯誤越來越少。</p>\n\n<p>接下來我們就要介紹如何用演算法來模擬這樣的學習過程。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-5.png\">\n</p>\n\n<h3 id=\"ein\">有權重的 Ein</h3>\n\n<p>老師調整圖片放大縮小的教學方式，在數學上我們可以為每個點犯錯時加上一個權重來表示。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-6.png\">\n</p>\n\n<h3 id=\"\">每一回合調整權重</h3>\n\n<p>那我們如何調整權重呢？我們每次調整權重，是希望每個學生能學出不一樣的觀點，這樣才能配合所有學生的觀點做出對蘋果完整的認識，因此挑整權重時應該要讓第一個學習到的 gt 在 u(t+1) 時這樣的權重下表現很差。</p>\n\n<p>所謂的表現差就是表現跟丟銅板沒兩樣，也就是猜錯的情況為 1/2。（全猜錯其實算是一個好的表現）</p>\n\n<p>我們讓演算法根據新的權重 u(t+1)（會使 gt 表現很差）再去學習出新的 g(t+1)，這個 g(t+1) 就是一個觀點與 gt 不同，但表現卻也不錯的新假設了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-8.png\">\n</p>\n\n<h3 id=\"\">調整權重的數學式</h3>\n\n<p>調整權重的數學式如下，我們想讓 gt 在 u(t+1) 的情況下猜對跟猜錯的情況為 1/2，那其實就是將原本的 ut 在猜對時乘上錯誤率(incorrect rate)，在猜錯時乘上答對率（correct rate），這樣 gt 在 u(t+1) 的 Ein 就會是 1/2 了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-9.png\">\n</p>\n\n<h3 id=\"\">縮放因數</h3>\n\n<p>演算法的作者在上述的概念上選擇了一個特別的縮放因數，但基本上就是剛剛上述所說的概念，這個縮放因數我們用 方塊t 來表示（如下圖），這在物理意義上有特別的意義，當 錯誤率 &lt;= 1/2 時，縮放因數 方塊t 才會 >= 1，然後在計算下一輪的 ut 時，答錯的點會乘上 方塊t，答對的點會除上 方塊t，這就像老師在小朋友答錯的圖片放大、答對的圖片縮小那樣的教學過程。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-10.png\">\n</p>\n\n<h3 id=\"\">初步的演算法</h3>\n\n<p>初步的演算法如下，首先第一輪的 u 對每個點都是相同的權重，這時我們學出第一個假設 gt，接下來使用 方塊t 調整權重得出 u(t+1)，然後繼續學出第二個假設 g(t+1)，我們可以繼續這樣的過程學出更多假設，現在只剩下一個問題，如何融合所有的假設呢？</p>\n\n<p>我們可以用之前學過的 blending 使用 uniform 或是 linear 來融合，但這邊作者選擇了一個特別的演算法在計算過程中融合所有的假設。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-11.png\">\n</p>\n\n<h3 id=\"\">演算過程中就算出融合假設的權重</h3>\n\n<p>這邊作者想了一個方法可以在演算過程中就算出融合假設的權重，這邊權重 alpha t = ln(方塊t)，這在物理意義上也有特別的意義。當錯誤率是 1/2 跟丟銅板沒兩樣時，那 方塊t 會等於 1，這時 ln(方塊t) 就會是 0，代表這個假設的權重是 0，一點用都沒有。當錯誤率是 0 的時候，那 方塊t 會等於無限大，這時 ln(方塊t) 就會是無限大，我們可以只看這個超強的假設 gt 就好。</p>\n\n<p>綜合以上，這個演算法就是 Adptive Boosting。我們有 weak learning 就上課堂上的幼稚園學生；我們有一個演算法調整權重就像老師會調整圖片大小引領學生學習；我們有一個融合假設的方法就像我們將課堂上所有學生的觀點融合起來一樣。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-12.png\">\n</p>\n\n<h3 id=\"adaptiveboostingadaboost\">Adaptive Boosting(AdaBoost) 演算法</h3>\n\n<p>詳細 Adaptive Boosting 演算法如下所示，中文又稱這個演算法就皮匠法。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-13.png\">\n</p>\n\n<h3 id=\"\">理論上的保證</h3>\n\n<p>跟去理論上的保證只要 weak learner 比亂猜還好，那 AdaBoost 可以很容易地將 Ein 降到 0，而 dvc 的成長也相對慢，因此當資料量夠多時，Eout 理論保證會很小。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-14.png\">\n</p>\n\n<h3 id=\"weaklearnerdecisionstump\">Weak Learner：Decision Stump</h3>\n\n<p>AdaBoost 使用的 Weak Learner 是 Decision Stump，是一個只能在平面上切一條水平線或垂直線的 Weak Learner，詳細演算法如下，每個假設就是要學習出是一條水平線或垂直線（direction s），切在哪個 feature（feature i），切在那個值（threshold theta）。如果想要看程式碼可以參考：<a href='https://github.com/fukuball/fuku-ml/blob/master/FukuML/DecisionStump.py'>https://github.com/fukuball/fuku-ml/blob/master/FukuML/DecisionStump.py</a></p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-15.png\">\n</p>\n\n<h3 id=\"adaboost\">我們用一個簡單的例子說明 AdaBoost</h3>\n\n<p>我們用一個簡單的例子來說明 AdaBoost 的演算過程。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-16.png\">\n</p>\n\n<h3 id=\"\">第一輪</h3>\n\n<p>第一輪先學出一個 weak learner 切了一個垂直線，這時犯錯的點會放大、答對的點縮小。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-17.png\">\n</p>\n\n<h3 id=\"\">第二輪</h3>\n\n<p>根據犯錯放大的權重，再去學出一個不同觀點的 weak learner 再切了一條垂直線，根據答案再對點的權重調整。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-18.png\">\n</p>\n\n<h3 id=\"\">第三輪</h3>\n\n<p>第三輪又學出了一個不同觀點的 weak learner 切了一條水平線，現在已經可以看出分界線慢慢變得複雜了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-19.png\">\n</p>\n\n<h3 id=\"\">第五輪</h3>\n\n<p>持續這樣的過程，AdaBoost 不斷地得出不同的 weak learner，綜合 weak learner 的答案便可以回答一些較複雜的問題。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-21.png\">\n</p>\n\n<h3 id=\"\">總結</h3>\n\n<p>這一講介紹了 AdaBoost 這個特殊的機器學習演算法，能夠將表現只比丟銅板好一些的 Weak Learner 融合起來去得到更好的學習效果，算是從 Blending 技巧中衍伸出來的一個特殊演算法。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-8-22.png\">\n</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-11-07T11:24:46.000Z","created_by":1,"updated_at":"2016-11-08T10:26:48.000Z","updated_by":1,"published_at":"2016-11-08T10:26:42.000Z","published_by":1},{"id":89,"uuid":"c301a09f-a89c-4b76-861d-eb80b059899b","title":"林軒田教授機器學習技法 Machine Learning Techniques 第 9 講學習筆記","slug":"lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-9-jiang-xue-xi-bi-ji","markdown":"### 前言\n\n本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 [第 8 講](http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-8-jiang-xue-xi-bi-ji/) 的碼農們，我建議可以先回頭去讀一下再回來喔！\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 熱身回顧一下\n\n上一講我們介紹了 Adaptive Boosting 這種可以結合多個 Weak Learner 的 Linear Aggregation Model，這一講將介紹另一種 Aggregation Model - Decision Tree，Decision Tree 其實就是一種 Non-Linear 的 Aggregation Model。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-01.png\">\n</p>\n\n### Aggregation Model 表格\n\n我們可以將這幾講種所介紹的 Aggregation Model 用這個表格整理出來，我們可以知道 AdaBoost 跟 Decision Tree 都是 Weak Learner Aggregation 的模型，只是 AdaBoost 是 Linear Aggregation，會讓所有的 Weak Learner 一起發揮作用，但 Decision Tree 則是 Non-Linear 的 Conditional Aggregation，會每次根據 condition 讓某個 Weak Learner 發揮作用。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-02.png\">\n</p>\n\n### Decision Tree 遞迴式\n\nDecision Tree 的內涵其實就是去模仿人類的決策過程，我們可以用底下這個樹狀圖說明，人類簡化的決策過程大致就是依據很多種情況及條件，最後才去下決定。Decision Tree 會根據訓練資料去長成符合訓練資料所下決定的樹狀圖，而數學式可以用一個遞迴式來表示，父節點就是由分支條件及子樹所組成，我們可以遞迴地分支下去。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-04.png\">\n</p>\n\n### Decision Tree 的特點\n\n由於 Decision Tree 大體上的內涵就是想要模仿人類決策的過程，因此模型也會比較具有解釋性，我們可以從模型中了解當某個指數（特徵值）高於多少時可能會有什麼結果，比起 SVM 我們很難解釋是哪個特徵值影響了分類結果，所以 Decision Tree 在直接面對客戶的產業上如商業及醫藥上運用也較為廣泛。\n\n但 Decision Tree 有個缺點就是沒有什麼完備的理論保證，各種 Decision Tree 的發明都有演算法各自的小巧思，也難說哪種 Decision Tree 的表現會比較好。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-05.png\">\n</p>\n\n### Decision Tree 基礎演算法\n\n根據剛剛說的遞迴式我們可以大致寫出 Decision Tree 的基礎演算法如下：\n\n輸入 N 個樣本點，如果分支達到條件（無法再分支），就回傳 gt(x)，如果還沒達到條件就繼續：1. 獲得分支條件 2. 根據分之條件將資料分成 C 份 3. 由這 C 份資料生成 C 棵子樹 4. 將 C 顆子樹及分支條件一起回傳。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-06.png\">\n</p>\n\n### CART 演算法\n\n只要符合 Decision Tree 的基礎演算法都是一種 Decision Tree 演算法，我們這邊要介紹的是 CART 這種 Decision Tree。\n\nCART 的特點就是可以同時處理分類及迴歸的問題，而演算法的細節就是每次進行分支的時候會將資料分成兩份，也就是 Binary Tree。然後選擇分支條件的準則就是經過這個分支後底下的子樹資料變得更「純」了。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-07.png\">\n</p>\n\n### CART 分支判斷：純不純\n\nCART 分支判斷我們是使用子樹資料變得更「純」了來做判斷，在演算法訓練的過程中我們會反過來計算純度，所以會使用代表不純度 impurity 的函式來做計算，我們需要找出不純度最低的分支。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-08.png\">\n</p>\n\n### Impurity 函式\n\n那麼 Impurity 如何計算呢？根據不同的問題我們可以設計不同的 impurity 函式，如果是迴歸問題，CART 用均方差來衡量，如果是分類問題，CART 用 Gini Index 來衡量（子樹都是同一類的話，Gini Index 為 0，即不純度為 0）。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-09.png\">\n</p>\n\n### 終止條件\n\nCART 的終止條件有兩種情況：1. 如果 impurity 為 0，那代表子樹只剩下相同的分類資料了，所以就不用再分下去了 2. 如果所有的 xn 都相同了，也就是所有資料的 xn 特徵值都相同，也就沒有辦法劃分了。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-10.png\">\n</p>\n\n### CART 詳細演算法\n\n綜合上述 CART 的設計巧思，我們可以把 CART 的詳細演算法表示成下圖，CART 只要做一些些小小的修改就可以很方便處理二元分類、多元分類、迴歸問題，但我們現在的算法會長成一個完全成長樹，因此會有 overfitting 的問題。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-11.png\">\n</p>\n\n### 使用 Pruning 正規化\n\nCART 設計了 Pruning（剪枝）這個方法來處理 overfitting 這個問題，每次去除一個葉子，然後看看剪去哪個葉子時 Ein 最小。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-12.png\">\n</p>\n\n### 枚舉特徵的處理\n\n然後 CART 在處理實際情況下我們會面臨到的問題，當我們的資料特徵有些並不是數值而是枚舉類型特徵（categorical features）時，CART 也可以很容易地處理這樣的特徵。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-13.png\">\n</p>\n\n### Missing Feature 的處理\n\n有時我們的測試資料可能缺少某些特徵，CART 可以透過 Surrogate Brach 這樣的方法來處理 Missing Feature 的問題，一樣可以預測出結果。直觀的想法就是，某些特徵分支的情況可能類似，就可以使用某特特徵來替代遺失特徵的資料，由於 Decision Tree 訓練都會記下分支的結果，所以才可以做到這一點。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-14.png\">\n</p>\n\n### 簡單的 CART 分支訓練過程範例\n\n我們看一個簡單的 CART 分支訓練過程範例\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-15.png\">\n</p>\n\n每次分出一個簡單的分支\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-16.png\">\n</p>\n\n然後遞迴往下分\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-17.png\">\n</p>\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-18.png\">\n</p>\n\n直到不能分了之後，回傳分類的結果（塗上顏色）\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-19.png\">\n</p>\n\n遞迴往上回傳\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-20.png\">\n</p>\n\n最後分完所有資料\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-21.png\">\n</p>\n\n比較 CART 與 AdaBoost 分出來的結果\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-22.png\">\n</p>\n\n### CART 在實務上的優勢\n\nCART 在實務上的優勢如下：1. 比較容易解釋 2. 多元分類也很容易處理，幾乎不用改演算法 3. 枚舉特徵也可以處理 4. 遺失特徵也可以進行預測 5. 相對來說在訓練上很有效率\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-23.png\">\n</p>\n\n### 總結\n\n在介紹了各種 Aggregation 模型之後，我們介紹了第一個 Non-Linear 的 Aggregation 模型 Decision Tree，Decision Tree 其實有各種不同的變形，我們這邊介紹的是一個算是相當有名的 Decision Tree CART，下一講我們將介紹，如果我們要使用 Aggregation Model 的 Aggregation Model，也就是兩層 Aggregation Model，我們要怎麼做呢？\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-24.png\">\n</p>","html":"<h3 id=\"\">前言</h3>\n\n<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href=\"http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-8-jiang-xue-xi-bi-ji/\">第 8 講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">熱身回顧一下</h3>\n\n<p>上一講我們介紹了 Adaptive Boosting 這種可以結合多個 Weak Learner 的 Linear Aggregation Model，這一講將介紹另一種 Aggregation Model - Decision Tree，Decision Tree 其實就是一種 Non-Linear 的 Aggregation Model。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-01.png\">\n</p>\n\n<h3 id=\"aggregationmodel\">Aggregation Model 表格</h3>\n\n<p>我們可以將這幾講種所介紹的 Aggregation Model 用這個表格整理出來，我們可以知道 AdaBoost 跟 Decision Tree 都是 Weak Learner Aggregation 的模型，只是 AdaBoost 是 Linear Aggregation，會讓所有的 Weak Learner 一起發揮作用，但 Decision Tree 則是 Non-Linear 的 Conditional Aggregation，會每次根據 condition 讓某個 Weak Learner 發揮作用。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-02.png\">\n</p>\n\n<h3 id=\"decisiontree\">Decision Tree 遞迴式</h3>\n\n<p>Decision Tree 的內涵其實就是去模仿人類的決策過程，我們可以用底下這個樹狀圖說明，人類簡化的決策過程大致就是依據很多種情況及條件，最後才去下決定。Decision Tree 會根據訓練資料去長成符合訓練資料所下決定的樹狀圖，而數學式可以用一個遞迴式來表示，父節點就是由分支條件及子樹所組成，我們可以遞迴地分支下去。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-04.png\">\n</p>\n\n<h3 id=\"decisiontree\">Decision Tree 的特點</h3>\n\n<p>由於 Decision Tree 大體上的內涵就是想要模仿人類決策的過程，因此模型也會比較具有解釋性，我們可以從模型中了解當某個指數（特徵值）高於多少時可能會有什麼結果，比起 SVM 我們很難解釋是哪個特徵值影響了分類結果，所以 Decision Tree 在直接面對客戶的產業上如商業及醫藥上運用也較為廣泛。</p>\n\n<p>但 Decision Tree 有個缺點就是沒有什麼完備的理論保證，各種 Decision Tree 的發明都有演算法各自的小巧思，也難說哪種 Decision Tree 的表現會比較好。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-05.png\">\n</p>\n\n<h3 id=\"decisiontree\">Decision Tree 基礎演算法</h3>\n\n<p>根據剛剛說的遞迴式我們可以大致寫出 Decision Tree 的基礎演算法如下：</p>\n\n<p>輸入 N 個樣本點，如果分支達到條件（無法再分支），就回傳 gt(x)，如果還沒達到條件就繼續：1. 獲得分支條件 2. 根據分之條件將資料分成 C 份 3. 由這 C 份資料生成 C 棵子樹 4. 將 C 顆子樹及分支條件一起回傳。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-06.png\">\n</p>\n\n<h3 id=\"cart\">CART 演算法</h3>\n\n<p>只要符合 Decision Tree 的基礎演算法都是一種 Decision Tree 演算法，我們這邊要介紹的是 CART 這種 Decision Tree。</p>\n\n<p>CART 的特點就是可以同時處理分類及迴歸的問題，而演算法的細節就是每次進行分支的時候會將資料分成兩份，也就是 Binary Tree。然後選擇分支條件的準則就是經過這個分支後底下的子樹資料變得更「純」了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-07.png\">\n</p>\n\n<h3 id=\"cart\">CART 分支判斷：純不純</h3>\n\n<p>CART 分支判斷我們是使用子樹資料變得更「純」了來做判斷，在演算法訓練的過程中我們會反過來計算純度，所以會使用代表不純度 impurity 的函式來做計算，我們需要找出不純度最低的分支。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-08.png\">\n</p>\n\n<h3 id=\"impurity\">Impurity 函式</h3>\n\n<p>那麼 Impurity 如何計算呢？根據不同的問題我們可以設計不同的 impurity 函式，如果是迴歸問題，CART 用均方差來衡量，如果是分類問題，CART 用 Gini Index 來衡量（子樹都是同一類的話，Gini Index 為 0，即不純度為 0）。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-09.png\">\n</p>\n\n<h3 id=\"\">終止條件</h3>\n\n<p>CART 的終止條件有兩種情況：1. 如果 impurity 為 0，那代表子樹只剩下相同的分類資料了，所以就不用再分下去了 2. 如果所有的 xn 都相同了，也就是所有資料的 xn 特徵值都相同，也就沒有辦法劃分了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-10.png\">\n</p>\n\n<h3 id=\"cart\">CART 詳細演算法</h3>\n\n<p>綜合上述 CART 的設計巧思，我們可以把 CART 的詳細演算法表示成下圖，CART 只要做一些些小小的修改就可以很方便處理二元分類、多元分類、迴歸問題，但我們現在的算法會長成一個完全成長樹，因此會有 overfitting 的問題。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-11.png\">\n</p>\n\n<h3 id=\"pruning\">使用 Pruning 正規化</h3>\n\n<p>CART 設計了 Pruning（剪枝）這個方法來處理 overfitting 這個問題，每次去除一個葉子，然後看看剪去哪個葉子時 Ein 最小。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-12.png\">\n</p>\n\n<h3 id=\"\">枚舉特徵的處理</h3>\n\n<p>然後 CART 在處理實際情況下我們會面臨到的問題，當我們的資料特徵有些並不是數值而是枚舉類型特徵（categorical features）時，CART 也可以很容易地處理這樣的特徵。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-13.png\">\n</p>\n\n<h3 id=\"missingfeature\">Missing Feature 的處理</h3>\n\n<p>有時我們的測試資料可能缺少某些特徵，CART 可以透過 Surrogate Brach 這樣的方法來處理 Missing Feature 的問題，一樣可以預測出結果。直觀的想法就是，某些特徵分支的情況可能類似，就可以使用某特特徵來替代遺失特徵的資料，由於 Decision Tree 訓練都會記下分支的結果，所以才可以做到這一點。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-14.png\">\n</p>\n\n<h3 id=\"cart\">簡單的 CART 分支訓練過程範例</h3>\n\n<p>我們看一個簡單的 CART 分支訓練過程範例</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-15.png\">\n</p>\n\n<p>每次分出一個簡單的分支</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-16.png\">\n</p>\n\n<p>然後遞迴往下分</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-17.png\">\n</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-18.png\">\n</p>\n\n<p>直到不能分了之後，回傳分類的結果（塗上顏色）</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-19.png\">\n</p>\n\n<p>遞迴往上回傳</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-20.png\">\n</p>\n\n<p>最後分完所有資料</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-21.png\">\n</p>\n\n<p>比較 CART 與 AdaBoost 分出來的結果</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-22.png\">\n</p>\n\n<h3 id=\"cart\">CART 在實務上的優勢</h3>\n\n<p>CART 在實務上的優勢如下：1. 比較容易解釋 2. 多元分類也很容易處理，幾乎不用改演算法 3. 枚舉特徵也可以處理 4. 遺失特徵也可以進行預測 5. 相對來說在訓練上很有效率</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-23.png\">\n</p>\n\n<h3 id=\"\">總結</h3>\n\n<p>在介紹了各種 Aggregation 模型之後，我們介紹了第一個 Non-Linear 的 Aggregation 模型 Decision Tree，Decision Tree 其實有各種不同的變形，我們這邊介紹的是一個算是相當有名的 Decision Tree CART，下一講我們將介紹，如果我們要使用 Aggregation Model 的 Aggregation Model，也就是兩層 Aggregation Model，我們要怎麼做呢？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-09-24.png\">\n</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-11-30T08:23:49.000Z","created_by":1,"updated_at":"2016-11-30T09:41:03.000Z","updated_by":1,"published_at":"2016-11-30T09:40:52.000Z","published_by":1},{"id":90,"uuid":"071a8b49-4ae4-4834-b9ed-fb15d62f2c17","title":"林軒田教授機器學習技法 Machine Learning Techniques 第 10 講學習筆記","slug":"lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-10-jiang-xue-xi-bi-ji","markdown":"### 前言\n\n本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 [第 9 講](http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-9-jiang-xue-xi-bi-ji/) 的碼農們，我建議可以先回頭去讀一下再回來喔！\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 熱身回顧一下\n\n上一講介紹了 Decision Tree，如同之前介紹的 blending 算法，我們也可以進一步使用在 Decision Tree，這就是這一講要介紹的 Random Forest。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-10-01.png\">\n</p>\n\n### 回憶 Bagging 與 Decision Tree\n\n回憶一下 Bagging 與 Decision Tree 的特點，Bagging 的結合 weak learner 的方式主要是為何減少差異化，讓未來的預測可以更好，Decision Tree 結合 weak learner 的方式則是著重差異化，讓 modle 在訓練時得到的預測效果更好，我們有辦法結合這兩個特點嗎？\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-10-02.png\">\n</p>\n\n### Random Forest\n\nRandom Forest 就可以達到上述的目的，每次會用類似 Bagging 的方法取得一個新的 Decision Tree，再將所有的 Decision Tree 結合起來。這個方法可以很容易地平行化運算，且不僅能夠保持 Decision Tree 的差異行，還能減少 Decision Tree 的 fully grown 的 overfitting。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-10-03.png\">\n</p>\n\n### 透過特徵選取增加變異\n\nRandom Forest 在取得 Decision Tree 時會希望盡量取得更多不一樣的 Decision Tree，以增加分類的效果，這邊 Random Forest 的作者提出了一些方法在每次取得 Decision Tree 時過特徵選取或是特徵轉換來取得不一樣的 Decision Tree，通常會使用 low dimension 的方式進行特徵轉換，這樣運算速度可以提昇。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-10-04.png\">\n</p>\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-10-05.png\">\n</p>\n\n### Bagging 在 Random Forest 的特點\n\n由於 Random Forest 在每一輪取得 Decision Tree 時，都會進行一下 Bagging，這時會有一些沒有被抽到的 data，這些就是 out-of-bag。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-10-06.png\">\n</p>\n\n### 使用 Out of Bag Error 取得最後的 model\n\n數學上證明使用 out of bag error 來取得最後的 model，未來在預測時的 error 會跟 out of bag error 非常接近，因此我們可以在訓練的過程中就順便計算 out of bag error 來進行 model 的選取。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-10-09.png\">\n</p>\n\n### 特徵選取\n\nRandom Forest 每次選取特徵進行訓練時最簡單的方式就是隨機選取，我們也可以進一步讓演算法去根據特徵的「重要性」來進行特徵選取。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-10-11.png\">\n</p>\n\n### 利用 Permutation Test 進行特徵選取\n\n我們可以利用 Permutation Test 這個方法來進行特徵選取，比如 N 個樣本，每個樣本有 d 維度特徵，想要衡量其中第 i 維特徵的重要性，可以把這 N 個樣本的第 i 維特徵都洗牌打亂，再評估洗牌前跟洗牌後 Model 的 performance，如此就可以知道 i 維特徵的重要性。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-10-12.png\">\n</p>\n\n### 一個例子\n\n我們看一個例子，左圖是使用 Decision Tree 來做分類，右圖是使用 Random Forest 來做分類，我們可以看到 Random Forest 的邊界比較平滑。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-10-14.png\">\n</p>\n\n### 要多少棵樹呢\n\n那訓練 Random Forest 時要娶多少棵樹呢？簡單來說就是越多棵樹越好！\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-10-16.png\">\n</p>\n\n### 總結\n\n這一講我們介紹了 Random Forest，下一講將繼續介紹 Boosted Decision Tree。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-10-17.png\">\n</p>","html":"<h3 id=\"\">前言</h3>\n\n<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href=\"http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-9-jiang-xue-xi-bi-ji/\">第 9 講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">熱身回顧一下</h3>\n\n<p>上一講介紹了 Decision Tree，如同之前介紹的 blending 算法，我們也可以進一步使用在 Decision Tree，這就是這一講要介紹的 Random Forest。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-10-01.png\">\n</p>\n\n<h3 id=\"baggingdecisiontree\">回憶 Bagging 與 Decision Tree</h3>\n\n<p>回憶一下 Bagging 與 Decision Tree 的特點，Bagging 的結合 weak learner 的方式主要是為何減少差異化，讓未來的預測可以更好，Decision Tree 結合 weak learner 的方式則是著重差異化，讓 modle 在訓練時得到的預測效果更好，我們有辦法結合這兩個特點嗎？</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-10-02.png\">\n</p>\n\n<h3 id=\"randomforest\">Random Forest</h3>\n\n<p>Random Forest 就可以達到上述的目的，每次會用類似 Bagging 的方法取得一個新的 Decision Tree，再將所有的 Decision Tree 結合起來。這個方法可以很容易地平行化運算，且不僅能夠保持 Decision Tree 的差異行，還能減少 Decision Tree 的 fully grown 的 overfitting。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-10-03.png\">\n</p>\n\n<h3 id=\"\">透過特徵選取增加變異</h3>\n\n<p>Random Forest 在取得 Decision Tree 時會希望盡量取得更多不一樣的 Decision Tree，以增加分類的效果，這邊 Random Forest 的作者提出了一些方法在每次取得 Decision Tree 時過特徵選取或是特徵轉換來取得不一樣的 Decision Tree，通常會使用 low dimension 的方式進行特徵轉換，這樣運算速度可以提昇。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-10-04.png\">\n</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-10-05.png\">\n</p>\n\n<h3 id=\"baggingrandomforest\">Bagging 在 Random Forest 的特點</h3>\n\n<p>由於 Random Forest 在每一輪取得 Decision Tree 時，都會進行一下 Bagging，這時會有一些沒有被抽到的 data，這些就是 out-of-bag。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-10-06.png\">\n</p>\n\n<h3 id=\"outofbagerrormodel\">使用 Out of Bag Error 取得最後的 model</h3>\n\n<p>數學上證明使用 out of bag error 來取得最後的 model，未來在預測時的 error 會跟 out of bag error 非常接近，因此我們可以在訓練的過程中就順便計算 out of bag error 來進行 model 的選取。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-10-09.png\">\n</p>\n\n<h3 id=\"\">特徵選取</h3>\n\n<p>Random Forest 每次選取特徵進行訓練時最簡單的方式就是隨機選取，我們也可以進一步讓演算法去根據特徵的「重要性」來進行特徵選取。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-10-11.png\">\n</p>\n\n<h3 id=\"permutationtest\">利用 Permutation Test 進行特徵選取</h3>\n\n<p>我們可以利用 Permutation Test 這個方法來進行特徵選取，比如 N 個樣本，每個樣本有 d 維度特徵，想要衡量其中第 i 維特徵的重要性，可以把這 N 個樣本的第 i 維特徵都洗牌打亂，再評估洗牌前跟洗牌後 Model 的 performance，如此就可以知道 i 維特徵的重要性。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-10-12.png\">\n</p>\n\n<h3 id=\"\">一個例子</h3>\n\n<p>我們看一個例子，左圖是使用 Decision Tree 來做分類，右圖是使用 Random Forest 來做分類，我們可以看到 Random Forest 的邊界比較平滑。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-10-14.png\">\n</p>\n\n<h3 id=\"\">要多少棵樹呢</h3>\n\n<p>那訓練 Random Forest 時要娶多少棵樹呢？簡單來說就是越多棵樹越好！</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-10-16.png\">\n</p>\n\n<h3 id=\"\">總結</h3>\n\n<p>這一講我們介紹了 Random Forest，下一講將繼續介紹 Boosted Decision Tree。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-10-17.png\">\n</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2016-12-01T17:43:56.000Z","created_by":1,"updated_at":"2017-03-28T06:15:51.000Z","updated_by":1,"published_at":"2017-03-28T06:15:43.000Z","published_by":1},{"id":91,"uuid":"f1efa76d-78b5-4338-b623-e260217a9bc5","title":"林軒田教授機器學習技法 Machine Learning Techniques 第 11 講學習筆記","slug":"lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-11-jiang-xue-xi-bi-ji","markdown":"### 前言\n\n本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 [第 10 講](http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-10-jiang-xue-xi-bi-ji/) 的碼農們，我建議可以先回頭去讀一下再回來喔！\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 熱身回顧一下\n\n上一講的 Random Forest 演算核心主要就是利用 bootstrap data 的方式訓練出許多不同的 Decision Trees 再 uniform 結合起來。\n\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-01.png\">\n</p>\n\n### AdaBoost Decision Tree\n\n這一講接下來要介紹的 AdaBoost Decision Tree 其實乍看有些類似，但它的訓練資料集並不是透過 bootstrap 來打亂，而是使用之前 AdaBoost 的方式再每一輪資料計算加權 u(t) 去訓練出許多不同的 Decision Tree，最後再以 alpha(t) 的權重將所有的 Decision Tree 結合起來。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-02.png\">\n</p>\n\n### 權重會影響演算法\n\n由於 AdaBoost Decision Tree 會考慮到權重，因此應該要像之前介紹過的 AdaBoost 會將權重傳進 Decision Stump 一樣，AdaBoost Decision Tree 應該也要將權重傳進 Decision Tree 裡做訓練，但這樣就需要調整 Decision Tree 原本的演算法，我們不喜歡這樣。\n\n轉換一個方式，也許我們可以一樣使用抽樣的方式來將訓練資料依造 u(t) 的權重做抽樣，這樣就可以直接將用權重抽樣玩的訓練資料集傳進 Decision Tree 做訓練，達到相同的效果，如此就不用改原本 Decision Tree 的演算法了。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-03.png\">\n</p>\n\n### 要使用 Weak Decision Tree\n\n另外要注意的是，如果 AdaBoost Decision Tree 使用了 fully grown 的 Decision Tree，這樣 alpha(t) 就會變得無限大，如此訓練完的 AdaBoost Decision Tree 做預測時就只會參考這個權重無限大的 Decision Tree，這樣就沒有 Aggregation Model 的效果了，我們應該要避免這個問題，所以要使用弱一點的 Decision Tree，比如透過 pruned 來避免 Decision Tree fully grown。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-04.png\">\n</p>\n\n### 特例：使用 Extremely Pruned Tree\n\n如果 AdaBoost Decision Tree 使用了 Extremely Pruned Tree，比如限制樹的高度只有 1，那這樣其實就是之前學過的 AdaBoost，這是 AdaBoost Decision Tree 中的一個特例。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-05.png\">\n</p>\n\n### Gradien Boost\n\n這邊的數學演算太過複雜，大家可以直接觀看影片學習，我這邊直接說數學推導最後得出來的結論。\n\nAdaBoost 透過一些數學特性的推導之後，可得出圖中的式子，代表要最佳化 binary-output Error，這個式子可以換成是要算 real-output Error，這樣就是所謂的 Gradien Boost。\n\n由於 real-output Error 的最佳化是一個連續函數，我們可以使用跟之前的 logistic regression 一樣的方式使用 gradient decent 找出最佳的 ita 及 h(x)。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-06.png\">\n</p>\n\n### Gradien Boost 演算法\n\nGradien Boost 演算法的數學推導這邊也請大家去看影片，我直接講數學推導完之後得到的結論，整個演算法看起來很簡單，第一步先使用 xn 與餘數 (yn - sn) 做訓練，得出 gt，第二步再使用 gt 對 xn 做資料轉換，再使用 gt(xn) 與餘數 (yn - sn) 做 linear regression 算出 alphat，最後使用 sn + alphat * gt(xn) 得出新的 sn，重複這個過程，將各個 Decision Tree 結合起來就是 Gradien Boost Decision Tree 了！\n\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-07.png\">\n</p>\n\n### Blending Models\n\n課程到這邊已經介紹完了 Blending Models 的各種形式，有 uniform、non-uniform、conditional 的形式，uniform 可以帶來穩定性，non-uniform 及 conditional 可以帶來模型複雜度，但要小心 overfiting。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-08.png\">\n</p>\n\n### Aggregation Learning Model\n\n從上述的 blending 方式，我們可以發展出不同的 Aggregation Model，如 Badding 使用 uniform vote、AdaBoost 使用 linear vote by reweighting、Decision Tree 使用 conditional vote、GradientBoost 使用 linear vote by residual（餘數） fitting。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-09.png\">\n</p>\n\n### Aggregation Model 的好處\n\nAggregation Model 可以避免 underfitting，讓 weak learner 結合起來也可以做複雜的預測，其實跟 feature transform 的效果很類似。Aggregation Model 也可以避免 overfitting，因為 Aggregation 會選出比較中庸的結果，這其實跟 regularization 的效果類似。所以使用了 Aggregation 也就是 Ensemble 方法通常也就代表了更好的效果。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-11.png\">\n</p>\n\n### 總結\n\n在這一講，我們從 Random Forest 延伸到了 AdaBoost Decision Tree，再從 AdaBoost Decision Tree 延伸到 Gradient Boosting，基本上對大部分的 Aggregation Model 都有一些認識了。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-12.png\">\n</p>","html":"<h3 id=\"\">前言</h3>\n\n<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href=\"http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-10-jiang-xue-xi-bi-ji/\">第 10 講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">熱身回顧一下</h3>\n\n<p>上一講的 Random Forest 演算核心主要就是利用 bootstrap data 的方式訓練出許多不同的 Decision Trees 再 uniform 結合起來。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-01.png\">\n</p>\n\n<h3 id=\"adaboostdecisiontree\">AdaBoost Decision Tree</h3>\n\n<p>這一講接下來要介紹的 AdaBoost Decision Tree 其實乍看有些類似，但它的訓練資料集並不是透過 bootstrap 來打亂，而是使用之前 AdaBoost 的方式再每一輪資料計算加權 u(t) 去訓練出許多不同的 Decision Tree，最後再以 alpha(t) 的權重將所有的 Decision Tree 結合起來。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-02.png\">\n</p>\n\n<h3 id=\"\">權重會影響演算法</h3>\n\n<p>由於 AdaBoost Decision Tree 會考慮到權重，因此應該要像之前介紹過的 AdaBoost 會將權重傳進 Decision Stump 一樣，AdaBoost Decision Tree 應該也要將權重傳進 Decision Tree 裡做訓練，但這樣就需要調整 Decision Tree 原本的演算法，我們不喜歡這樣。</p>\n\n<p>轉換一個方式，也許我們可以一樣使用抽樣的方式來將訓練資料依造 u(t) 的權重做抽樣，這樣就可以直接將用權重抽樣玩的訓練資料集傳進 Decision Tree 做訓練，達到相同的效果，如此就不用改原本 Decision Tree 的演算法了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-03.png\">\n</p>\n\n<h3 id=\"weakdecisiontree\">要使用 Weak Decision Tree</h3>\n\n<p>另外要注意的是，如果 AdaBoost Decision Tree 使用了 fully grown 的 Decision Tree，這樣 alpha(t) 就會變得無限大，如此訓練完的 AdaBoost Decision Tree 做預測時就只會參考這個權重無限大的 Decision Tree，這樣就沒有 Aggregation Model 的效果了，我們應該要避免這個問題，所以要使用弱一點的 Decision Tree，比如透過 pruned 來避免 Decision Tree fully grown。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-04.png\">\n</p>\n\n<h3 id=\"extremelyprunedtree\">特例：使用 Extremely Pruned Tree</h3>\n\n<p>如果 AdaBoost Decision Tree 使用了 Extremely Pruned Tree，比如限制樹的高度只有 1，那這樣其實就是之前學過的 AdaBoost，這是 AdaBoost Decision Tree 中的一個特例。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-05.png\">\n</p>\n\n<h3 id=\"gradienboost\">Gradien Boost</h3>\n\n<p>這邊的數學演算太過複雜，大家可以直接觀看影片學習，我這邊直接說數學推導最後得出來的結論。</p>\n\n<p>AdaBoost 透過一些數學特性的推導之後，可得出圖中的式子，代表要最佳化 binary-output Error，這個式子可以換成是要算 real-output Error，這樣就是所謂的 Gradien Boost。</p>\n\n<p>由於 real-output Error 的最佳化是一個連續函數，我們可以使用跟之前的 logistic regression 一樣的方式使用 gradient decent 找出最佳的 ita 及 h(x)。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-06.png\">\n</p>\n\n<h3 id=\"gradienboost\">Gradien Boost 演算法</h3>\n\n<p>Gradien Boost 演算法的數學推導這邊也請大家去看影片，我直接講數學推導完之後得到的結論，整個演算法看起來很簡單，第一步先使用 xn 與餘數 (yn - sn) 做訓練，得出 gt，第二步再使用 gt 對 xn 做資料轉換，再使用 gt(xn) 與餘數 (yn - sn) 做 linear regression 算出 alphat，最後使用 sn + alphat * gt(xn) 得出新的 sn，重複這個過程，將各個 Decision Tree 結合起來就是 Gradien Boost Decision Tree 了！</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-07.png\">\n</p>\n\n<h3 id=\"blendingmodels\">Blending Models</h3>\n\n<p>課程到這邊已經介紹完了 Blending Models 的各種形式，有 uniform、non-uniform、conditional 的形式，uniform 可以帶來穩定性，non-uniform 及 conditional 可以帶來模型複雜度，但要小心 overfiting。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-08.png\">\n</p>\n\n<h3 id=\"aggregationlearningmodel\">Aggregation Learning Model</h3>\n\n<p>從上述的 blending 方式，我們可以發展出不同的 Aggregation Model，如 Badding 使用 uniform vote、AdaBoost 使用 linear vote by reweighting、Decision Tree 使用 conditional vote、GradientBoost 使用 linear vote by residual（餘數） fitting。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-09.png\">\n</p>\n\n<h3 id=\"aggregationmodel\">Aggregation Model 的好處</h3>\n\n<p>Aggregation Model 可以避免 underfitting，讓 weak learner 結合起來也可以做複雜的預測，其實跟 feature transform 的效果很類似。Aggregation Model 也可以避免 overfitting，因為 Aggregation 會選出比較中庸的結果，這其實跟 regularization 的效果類似。所以使用了 Aggregation 也就是 Ensemble 方法通常也就代表了更好的效果。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-11.png\">\n</p>\n\n<h3 id=\"\">總結</h3>\n\n<p>在這一講，我們從 Random Forest 延伸到了 AdaBoost Decision Tree，再從 AdaBoost Decision Tree 延伸到 Gradient Boosting，基本上對大部分的 Aggregation Model 都有一些認識了。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-11-12.png\">\n</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2017-04-25T08:46:15.000Z","created_by":1,"updated_at":"2017-04-25T09:36:07.000Z","updated_by":1,"published_at":"2017-04-25T09:35:59.000Z","published_by":1},{"id":92,"uuid":"7013701e-f4a6-4920-bb14-2bf8f58bcb90","title":"林軒田教授機器學習技法 Machine Learning Techniques 第 12 講學習筆記","slug":"lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-12-jiang-xue-xi-bi-ji","markdown":"### 前言\n\n本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 [第 11 講](https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-11-jiang-xue-xi-bi-ji/) 的碼農們，我建議可以先回頭去讀一下再回來喔！\n\n### 範例原始碼：[FukuML - 簡單易用的機器學習套件](https://github.com/fukuball/fuku-ml)\n\n我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 [GitHub](https://github.com/fukuball/fuku-ml) 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。\n\n如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 [FukuML](https://github.com/fukuball/fuku-ml) 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 [FukuML](https://github.com/fukuball/fuku-ml) 了。不過我還是有寫 [Tutorial](https://github.com/fukuball/FukuML-Tutorial) 啦，之後會不定期更新，讓大家可以容易上手比較重要！\n\n### 熱身回顧一下\n\n上一講我們從 Random Forest 延伸到了 AdaBoost Decision Tree，再從 AdaBoost Decision Tree 延伸到 Gradient Boosting，大家不一定要記住所有演算法的細節，但大致上對 Aggregation 的方式有些概念就可以啦！\n\n如果要記，就要記住這個核心概念：Aggregation Model 可以避免 underfitting，讓 weak learner 結合起來也可以做複雜的預測，其實跟 feature transform 的效果很類似。Aggregation Model 也可以避免 overfitting，因為 Aggregation 會選出比較中庸的結果，這其實跟 regularization 的效果類似。所以使用了 Aggregation 也就是 Ensemble 方法通常也就代表了更好的效果。\n\n這一講我們將開始介紹現在很紅的類神經網路機器學習演算法。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-01.png\">\n</p>\n\n### Perceptron 的線性組合\n\n我們先看一下最簡單的類神經網路，其實可以看成是多個 Perceptron 的線性組合，如果之前學過的 Aggregation，這樣組合多個 Perceptron 就能帶來更複雜的學習效果。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-02.png\">\n</p>\n\n### 單層類神經網路的限制\n\n單層類神經網路可以透過組合越多的神經元來模擬曲線的邊界，用以解決更複雜的問題，但還是有其限制，比如 XOR 及雙曲線這樣的邊界，無論如何都無法透過單層的線性組合來做到，因此我們需要想辦法再延伸單層類神經網路。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-04.png\">\n</p>\n\n### 多層類神經網路\n\n所以就延伸出了多層類神經網路，就跟邏輯閘設計一樣，多層的架構就可以做出 XOR 這樣的邏輯，多層類神經網路就可以模擬各種各樣的邊界了，一般人家在說的類神經網路其實也就是說多層類神經網路。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-05.png\">\n</p>\n\n### 在生物上的關係\n\n類神經網路與生物上的神經網路有什麼關係呢？其實類神經網路的架構就是有想要模擬生物上的神經網路，但不完全就跟生物上的神經網路一樣，就跟飛機是模擬鳥類，但跟鳥類飛行實際如何運作並不完全一致。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-06.png\">\n</p>\n\n### 類神經網路的 output\n\n我們從這個圖探討類神經網路的 output，在 output 之前我們可以把它看成是一個對 x 資料的轉換，x 資料經過各個神經元結合轉換之後，再透過 output 層的線性組合做 voting，如果要做分類就對最後的 output 加上 sign 函數，要做迴歸就直接輸出 output 結果，如果要輸出機率值，就對最後的 output 加上 theta 函數，如此就可以用來解各種常見的 Machine Learning 問題。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-07.png\">\n</p>\n\n### 特徵轉換\n\n我們再往前看一下 x 在 output 前經過的特徵轉換層，這些轉換層是由許多神經元組成，用來計算複雜的特徵轉換，每個神經元也會做組合與輸出，這邊的組合如果是用線性組合，那在數學意義上，所有的神經網路就是單純的在做線性組合，所以並無法做到複雜的特徵轉換。\n\n所以這邊的組合要跟邏輯閘一樣使用類似 sign 函數來組合，才能組合出複雜的邊界，但 sign 函數組合並不是一個連續函數，因此比較難優化，所以我們會永 tanh 函數來逼近 sign 函數，如此就不僅可以模擬複雜邊界，然後計算也比較容易優化。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-08.png\">\n</p>\n\n### 類神經網路假設\n\n如上述去界定類神經網路的架構後，我們可以把類神經網路畫成如下圖，我們會有 x 作為輸入，然後經過各層的 weight 計算後，再透過神經元的 tanh 組合，最後再輸出結果，如此我們只剩下如何去計算出各層的 weight 就可以訓練出類神經網路了。 \n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-09.png\">\n</p>\n\n### 神經元的物理意義解釋\n\n這邊我們可以探討一下神經元的物理意義，我們可以看到前一層的輸出會作為後一層的輸入，如果達到一定程度神經元就會輸出結果，所以其實他就是在做兩層之間的匹配程度，越匹配就越能輸出結果，也就是每個神經元都是在學習某一種 pattern。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-10.png\">\n</p>\n\n### 如何學習各層之間的 weight？\n\n之前學過的 Gradient Boosting 可以用來解單層的類神經網路，但多層的類神經網路不容易用 Gradient Boosting 來解，這邊需要用 Stochastic Gradient Decent 來解會比較簡單一些。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-11.png\">\n</p>\n\n### Backpropagation 演算法\n\n計算類神經網路的 Gradient Decent 需要使用到 Backpropagation 演算法，這邊我跳過了數學推導過程，想要詳細了解我另外推薦[李宏毅老師的講解](https://www.youtube.com/watch?v=ibJpTrp5mcE)，演算法概觀如下：首先，需要先設定整個類神經網路的 weight value，然後 1. 隨機選取一個資料點 x，2. 計算 forward pass，3 計算 backward pass，4 調整 weight。\n\n（forward 跟 backward pass 分別怎麼計算請看[李宏毅老師的講解](https://www.youtube.com/watch?v=ibJpTrp5mcE)）\n\n通常實務上我們會重複 1 - 3 很多次之後做一個平均再去 update weight，這樣的做法叫做 mini batch。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-14.png\">\n</p>\n\n### NN 最佳化的問題\n\n由於類神經網路非常複雜，有很多個凸點，所以很容易在學習過程中得到一個 local minimum 的結果。因此不同的起始 weight 可能會得到不同的最佳化結果，在訓練類神經網路時可以挑不同的起始 weight 來做訓練。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-15.png\">\n</p>\n\n### VC Dimension \n\n類神將網路的 VC Dimension 為 V\\*D，V 是神經元數量，D 是權重的數量，所以如果神經網路的層數跟神經元多起來那 VC Dimension 就會很高，所以可能就會有 overfitting 的現象，這樣就需要做 regularization 來防止 overfitting。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-16.png\">\n</p>\n\n### Early Stopping\n\n除了使用一般的逞罰項來做 regularization 之外，類神經網路還是用了另一個方式來做到 regularization，這個方法叫 Early Stopping，至於哪時要 stop 呢？那就要做 validation。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-18.png\">\n</p>\n\n### 總結\n\n這一講說明了什麼是類神經網路，以及類神經網路的核心演算法 Backpropagation，下一講將介紹類神經網路的延伸 - 深度學習。\n\n<p style=\"text-align:center\">\n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-19.png\">\n</p>","html":"<h3 id=\"\">前言</h3>\n\n<p>本系列部落格文章將分享我在 Coursera 上台灣大學林軒田教授所教授的機器學習技法（Machine Learning Techniques）課程整理成的心得，並對照林教授的投影片作說明。若還沒有閱讀過 <a href=\"https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-11-jiang-xue-xi-bi-ji/\">第 11 講</a> 的碼農們，我建議可以先回頭去讀一下再回來喔！</p>\n\n<h3 id=\"fukumlhttpsgithubcomfukuballfukuml\">範例原始碼：<a href=\"https://github.com/fukuball/fuku-ml\">FukuML - 簡單易用的機器學習套件</a></h3>\n\n<p>我在分享機器學習基石課程時，也跟著把每個介紹過的機器學習演算法都實作了一遍，原始碼都放在 <a href=\"https://github.com/fukuball/fuku-ml\">GitHub</a> 上了，所以大家可以去參考看看每個演算法的實作細節，看完原始碼會對課程中的數學式更容易理解。</p>\n\n<p>如果大家對實作沒有興趣，只想知道怎麼使用機器學習演算法，那 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 絕對會比起其他機器學習套件簡單易用，且方法及變數都會跟林軒田教授的課程類似，有看過課程的話，說不定連文件都不用看就會使用 <a href=\"https://github.com/fukuball/fuku-ml\">FukuML</a> 了。不過我還是有寫 <a href=\"https://github.com/fukuball/FukuML-Tutorial\">Tutorial</a> 啦，之後會不定期更新，讓大家可以容易上手比較重要！</p>\n\n<h3 id=\"\">熱身回顧一下</h3>\n\n<p>上一講我們從 Random Forest 延伸到了 AdaBoost Decision Tree，再從 AdaBoost Decision Tree 延伸到 Gradient Boosting，大家不一定要記住所有演算法的細節，但大致上對 Aggregation 的方式有些概念就可以啦！</p>\n\n<p>如果要記，就要記住這個核心概念：Aggregation Model 可以避免 underfitting，讓 weak learner 結合起來也可以做複雜的預測，其實跟 feature transform 的效果很類似。Aggregation Model 也可以避免 overfitting，因為 Aggregation 會選出比較中庸的結果，這其實跟 regularization 的效果類似。所以使用了 Aggregation 也就是 Ensemble 方法通常也就代表了更好的效果。</p>\n\n<p>這一講我們將開始介紹現在很紅的類神經網路機器學習演算法。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-01.png\">\n</p>\n\n<h3 id=\"perceptron\">Perceptron 的線性組合</h3>\n\n<p>我們先看一下最簡單的類神經網路，其實可以看成是多個 Perceptron 的線性組合，如果之前學過的 Aggregation，這樣組合多個 Perceptron 就能帶來更複雜的學習效果。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-02.png\">\n</p>\n\n<h3 id=\"\">單層類神經網路的限制</h3>\n\n<p>單層類神經網路可以透過組合越多的神經元來模擬曲線的邊界，用以解決更複雜的問題，但還是有其限制，比如 XOR 及雙曲線這樣的邊界，無論如何都無法透過單層的線性組合來做到，因此我們需要想辦法再延伸單層類神經網路。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-04.png\">\n</p>\n\n<h3 id=\"\">多層類神經網路</h3>\n\n<p>所以就延伸出了多層類神經網路，就跟邏輯閘設計一樣，多層的架構就可以做出 XOR 這樣的邏輯，多層類神經網路就可以模擬各種各樣的邊界了，一般人家在說的類神經網路其實也就是說多層類神經網路。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-05.png\">\n</p>\n\n<h3 id=\"\">在生物上的關係</h3>\n\n<p>類神經網路與生物上的神經網路有什麼關係呢？其實類神經網路的架構就是有想要模擬生物上的神經網路，但不完全就跟生物上的神經網路一樣，就跟飛機是模擬鳥類，但跟鳥類飛行實際如何運作並不完全一致。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-06.png\">\n</p>\n\n<h3 id=\"output\">類神經網路的 output</h3>\n\n<p>我們從這個圖探討類神經網路的 output，在 output 之前我們可以把它看成是一個對 x 資料的轉換，x 資料經過各個神經元結合轉換之後，再透過 output 層的線性組合做 voting，如果要做分類就對最後的 output 加上 sign 函數，要做迴歸就直接輸出 output 結果，如果要輸出機率值，就對最後的 output 加上 theta 函數，如此就可以用來解各種常見的 Machine Learning 問題。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-07.png\">\n</p>\n\n<h3 id=\"\">特徵轉換</h3>\n\n<p>我們再往前看一下 x 在 output 前經過的特徵轉換層，這些轉換層是由許多神經元組成，用來計算複雜的特徵轉換，每個神經元也會做組合與輸出，這邊的組合如果是用線性組合，那在數學意義上，所有的神經網路就是單純的在做線性組合，所以並無法做到複雜的特徵轉換。</p>\n\n<p>所以這邊的組合要跟邏輯閘一樣使用類似 sign 函數來組合，才能組合出複雜的邊界，但 sign 函數組合並不是一個連續函數，因此比較難優化，所以我們會永 tanh 函數來逼近 sign 函數，如此就不僅可以模擬複雜邊界，然後計算也比較容易優化。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-08.png\">\n</p>\n\n<h3 id=\"\">類神經網路假設</h3>\n\n<p>如上述去界定類神經網路的架構後，我們可以把類神經網路畫成如下圖，我們會有 x 作為輸入，然後經過各層的 weight 計算後，再透過神經元的 tanh 組合，最後再輸出結果，如此我們只剩下如何去計算出各層的 weight 就可以訓練出類神經網路了。 </p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-09.png\">\n</p>\n\n<h3 id=\"\">神經元的物理意義解釋</h3>\n\n<p>這邊我們可以探討一下神經元的物理意義，我們可以看到前一層的輸出會作為後一層的輸入，如果達到一定程度神經元就會輸出結果，所以其實他就是在做兩層之間的匹配程度，越匹配就越能輸出結果，也就是每個神經元都是在學習某一種 pattern。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-10.png\">\n</p>\n\n<h3 id=\"weight\">如何學習各層之間的 weight？</h3>\n\n<p>之前學過的 Gradient Boosting 可以用來解單層的類神經網路，但多層的類神經網路不容易用 Gradient Boosting 來解，這邊需要用 Stochastic Gradient Decent 來解會比較簡單一些。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-11.png\">\n</p>\n\n<h3 id=\"backpropagation\">Backpropagation 演算法</h3>\n\n<p>計算類神經網路的 Gradient Decent 需要使用到 Backpropagation 演算法，這邊我跳過了數學推導過程，想要詳細了解我另外推薦<a href=\"https://www.youtube.com/watch?v=ibJpTrp5mcE\">李宏毅老師的講解</a>，演算法概觀如下：首先，需要先設定整個類神經網路的 weight value，然後 1. 隨機選取一個資料點 x，2. 計算 forward pass，3 計算 backward pass，4 調整 weight。</p>\n\n<p>（forward 跟 backward pass 分別怎麼計算請看<a href=\"https://www.youtube.com/watch?v=ibJpTrp5mcE\">李宏毅老師的講解</a>）</p>\n\n<p>通常實務上我們會重複 1 - 3 很多次之後做一個平均再去 update weight，這樣的做法叫做 mini batch。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-14.png\">\n</p>\n\n<h3 id=\"nn\">NN 最佳化的問題</h3>\n\n<p>由於類神經網路非常複雜，有很多個凸點，所以很容易在學習過程中得到一個 local minimum 的結果。因此不同的起始 weight 可能會得到不同的最佳化結果，在訓練類神經網路時可以挑不同的起始 weight 來做訓練。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-15.png\">\n</p>\n\n<h3 id=\"vcdimension\">VC Dimension</h3>\n\n<p>類神將網路的 VC Dimension 為 V*D，V 是神經元數量，D 是權重的數量，所以如果神經網路的層數跟神經元多起來那 VC Dimension 就會很高，所以可能就會有 overfitting 的現象，這樣就需要做 regularization 來防止 overfitting。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-16.png\">\n</p>\n\n<h3 id=\"earlystopping\">Early Stopping</h3>\n\n<p>除了使用一般的逞罰項來做 regularization 之外，類神經網路還是用了另一個方式來做到 regularization，這個方法叫 Early Stopping，至於哪時要 stop 呢？那就要做 validation。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-18.png\">\n</p>\n\n<h3 id=\"\">總結</h3>\n\n<p>這一講說明了什麼是類神經網路，以及類神經網路的核心演算法 Backpropagation，下一講將介紹類神經網路的延伸 - 深度學習。</p>\n\n<p style=\"text-align:center\">  \n    <img src=\"http://static.obeobe.com/image/blog-image/Machine-Learning-Techniques-12-19.png\">\n</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"2017-05-22T09:38:06.000Z","created_by":1,"updated_at":"2017-06-01T07:24:58.000Z","updated_by":1,"published_at":"2017-06-01T07:24:52.000Z","published_by":1}],"users":[{"id":1,"uuid":"d2d01eae-cc5f-4a04-85c4-a5e8fb3bb16f","name":"fukuball","slug":"fukuball","password":"$2a$10$iGwk3ud4BHYwt2qH/2/ICun8.6xoYzxRVZ9QSF3t3OoQyt6.OYhVG","email":"fukuball@gmail.com","image":"http://static.obeobe.com/2014/Jul/avatar4-1405834709797.jpg","cover":"http://static.obeobe.com/2014/Jul/tumblr_myp99avE2U1st5lhmo1_1280_2-1405834736469.jpg","bio":"我是林志傑，網路上常用的名字是 Fukuball，前 iNDIEVOX 技術長。我使用 PHP 及 Python，最近對機器學習感到興趣，所以空閒時會將 Python 有關機器學習的 Github Project 翻譯成 PHP 版本。 / 我也是一個快樂的吉他手～","website":"http://www.fukuball.com","location":"Taipei, Taiwan","accessibility":null,"status":"active","language":"en_US","meta_title":null,"meta_description":null,"last_login":null,"created_at":"2014-01-28T06:43:44.000Z","created_by":1,"updated_at":"2017-06-27T07:39:16.000Z","updated_by":1}],"roles":[{"id":1,"uuid":"d4b86fa5-1100-4049-ab7d-5666c53e4a93","name":"Administrator","description":"Administrators","created_at":"2014-07-20T03:41:46.000Z","created_by":1,"updated_at":"2014-07-20T03:41:46.000Z","updated_by":1},{"id":2,"uuid":"46707d68-a442-4880-820a-ce1d90fcdfbd","name":"Editor","description":"Editors","created_at":"2014-07-20T03:41:46.000Z","created_by":1,"updated_at":"2014-07-20T03:41:46.000Z","updated_by":1},{"id":3,"uuid":"f70a9f47-2fb0-4df5-8e6b-ae1c2b307fd6","name":"Author","description":"Authors","created_at":"2014-07-20T03:41:46.000Z","created_by":1,"updated_at":"2014-07-20T03:41:46.000Z","updated_by":1}],"roles_users":[{"id":1,"role_id":1,"user_id":1}],"permissions":[{"id":1,"uuid":"366da989-2c3e-45eb-b28f-81aeb21afb07","name":"Edit posts","object_type":"post","action_type":"edit","object_id":null,"created_at":"2014-07-20T03:41:46.000Z","created_by":1,"updated_at":"2014-07-20T03:41:46.000Z","updated_by":1},{"id":2,"uuid":"ed4a178f-f318-4114-9abe-4807e548400b","name":"Remove posts","object_type":"post","action_type":"remove","object_id":null,"created_at":"2014-07-20T03:41:46.000Z","created_by":1,"updated_at":"2014-07-20T03:41:46.000Z","updated_by":1},{"id":3,"uuid":"d5443acc-1799-4b88-b8f7-74d997bc99da","name":"Create posts","object_type":"post","action_type":"create","object_id":null,"created_at":"2014-07-20T03:41:46.000Z","created_by":1,"updated_at":"2014-07-20T03:41:46.000Z","updated_by":1}],"permissions_users":[],"permissions_roles":[{"id":1,"role_id":1,"permission_id":1},{"id":2,"role_id":1,"permission_id":2},{"id":3,"role_id":1,"permission_id":3}],"settings":[{"id":1,"uuid":"631e01de-214f-470d-a89a-251022c9705e","key":"databaseVersion","value":"002","type":"core","created_at":"2014-07-20T03:41:46.000Z","created_by":1,"updated_at":"2014-07-20T03:41:46.000Z","updated_by":1},{"id":2,"uuid":"205cd0a3-b488-4f9b-9557-2cc61a0da7cd","key":"dbHash","value":"3da18936-d69d-488f-a54d-f9065c588e28","type":"core","created_at":"2014-07-20T03:41:46.000Z","created_by":1,"updated_at":"2014-07-20T03:41:46.000Z","updated_by":1},{"id":3,"uuid":"a7a393e8-ff4f-445a-a860-5bb4114a47cf","key":"postsPerPage","value":"6","type":"blog","created_at":"2014-07-20T03:41:46.000Z","created_by":1,"updated_at":"2014-07-20T05:48:15.000Z","updated_by":1},{"id":4,"uuid":"f9aa5ebd-602d-49de-8a05-920babde19e6","key":"forceI18n","value":"true","type":"blog","created_at":"2014-07-20T03:41:46.000Z","created_by":1,"updated_at":"2014-07-20T05:48:16.000Z","updated_by":1},{"id":5,"uuid":"7ce6b36a-1f74-43f8-8a3e-7ba9723a71da","key":"permalinks","value":"/:slug/","type":"blog","created_at":"2014-07-20T03:41:46.000Z","created_by":1,"updated_at":"2014-07-20T05:48:15.000Z","updated_by":1},{"id":6,"uuid":"5e270a7a-3c42-4e5b-9635-7db62bd1edcc","key":"activeTheme","value":"sticko","type":"theme","created_at":"2014-07-20T03:41:46.000Z","created_by":1,"updated_at":"2014-07-20T05:48:15.000Z","updated_by":1},{"id":7,"uuid":"2bad396e-6dc1-413d-bd89-771c2896d9a6","key":"nextUpdateCheck","value":"1498635557","type":"core","created_at":"2014-07-20T03:41:46.000Z","created_by":1,"updated_at":"2017-06-27T07:39:17.000Z","updated_by":1},{"id":8,"uuid":"1f0a5883-f1b4-4050-a2d4-85bed1630327","key":"displayUpdateNotification","value":"0.11.10","type":"core","created_at":"2014-07-20T03:41:46.000Z","created_by":1,"updated_at":"2017-06-27T07:39:17.000Z","updated_by":1},{"id":9,"uuid":"df4e3f3f-6ace-4a57-8c57-de86f2e10bf7","key":"title","value":"Fukuball","type":"blog","created_at":"2014-07-20T03:41:46.000Z","created_by":1,"updated_at":"2014-07-20T05:48:16.000Z","updated_by":1},{"id":10,"uuid":"87a801cd-12ea-4665-ab95-ad98b74c5049","key":"description","value":"This is not a drive by","type":"blog","created_at":"2014-07-20T03:41:46.000Z","created_by":1,"updated_at":"2014-07-20T05:48:16.000Z","updated_by":1},{"id":11,"uuid":"1f296e16-bf81-4b7d-a8af-8f79fa65ea22","key":"email","value":"fukuball@gmail.com","type":"blog","created_at":"2014-07-20T03:41:46.000Z","created_by":1,"updated_at":"2014-07-20T05:48:16.000Z","updated_by":1},{"id":12,"uuid":"c2a81f87-1ae0-452b-b953-be464298e1cc","key":"logo","value":"http://static.obeobe.com/2014/Jul/avatar4-1405834615486.jpg","type":"blog","created_at":"2014-07-20T03:41:46.000Z","created_by":1,"updated_at":"2014-07-20T05:48:16.000Z","updated_by":1},{"id":13,"uuid":"ed7985ef-6f78-4587-b833-e1504fe246d3","key":"cover","value":"http://static.obeobe.com/2014/Jul/tumblr_myp99avE2U1st5lhmo1_1280_2-1405835270156.jpg","type":"blog","created_at":"2014-07-20T03:41:46.000Z","created_by":1,"updated_at":"2014-07-20T05:48:16.000Z","updated_by":1},{"id":14,"uuid":"5e09b27a-1934-4c20-8ff1-9c031e2863b9","key":"defaultLang","value":"en_US","type":"blog","created_at":"2014-07-20T03:41:46.000Z","created_by":1,"updated_at":"2014-07-20T05:48:16.000Z","updated_by":1},{"id":15,"uuid":"9ba9f831-09db-4b2d-8d60-fc3ac12b6da7","key":"activeApps","value":"[]","type":"app","created_at":"2014-07-20T03:41:46.000Z","created_by":1,"updated_at":"2014-07-20T05:48:16.000Z","updated_by":1},{"id":16,"uuid":"c20de6e4-b9fa-4d10-9ab3-bb0c2e761f36","key":"installedApps","value":"[]","type":"app","created_at":"2014-07-20T03:41:46.000Z","created_by":1,"updated_at":"2017-06-26T18:54:08.000Z","updated_by":1},{"id":17,"uuid":"663234f2-0f9e-4960-8d8c-0333fc24d570","key":"activePlugins","value":"[]","type":"core","created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":18,"uuid":"c24749e7-3ef8-4828-bfb2-1aeefbdf2a1e","key":"installedPlugins","value":"[]","type":"core","created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1}],"tags":[{"id":1,"uuid":"aabef06b-dc72-4009-9001-27569d589805","name":"Getting Started","slug":"getting-started","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T03:41:46.000Z","created_by":1,"updated_at":"2014-07-20T03:41:46.000Z","updated_by":1},{"id":2,"uuid":"72c7b462-75d6-4505-b85e-d9782ecbf71e","name":"about","slug":"about","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":3,"uuid":"091e5b96-97f9-4a33-b45e-37280fa1f428","name":"iNDIEVOX","slug":"indievox","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":4,"uuid":"a282ee13-6ec8-41f3-9426-90237a52e3bf","name":"fukuball","slug":"fukuball","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":5,"uuid":"6e507be3-3bed-4bd8-9a50-173272f11718","name":"小海嚴選","slug":"xiao-hai-yan-xuan","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":6,"uuid":"f37ce46b-ea95-46f5-9891-5995cc87f6df","name":"小海嚴選正妹分頁","slug":"xiao-hai-yan-xuan-zheng-mei-fen-ye","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":7,"uuid":"e42c5ab4-cfd7-4206-9498-d44c28c0dcae","name":"Chrome Extension","slug":"chrome-extension","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":8,"uuid":"2479bc66-50e9-4c15-be1e-f65140d80525","name":"Ghost","slug":"ghost-post","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":9,"uuid":"31096bdf-4b5e-4586-8aaa-5874f4087019","name":"blog","slug":"blog","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":10,"uuid":"38fa24db-d258-40f0-80ae-e422131e6e18","name":"Her","slug":"her","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":11,"uuid":"7f2945c5-4fb2-4211-a56d-063a368ebbfd","name":"電影","slug":"dian-ying","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":12,"uuid":"466ee332-472d-4868-b9d4-401cf81bbc7e","name":"雲端情人","slug":"yun-duan-qing-ren","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":13,"uuid":"8ff6ff06-146a-4c25-9aaa-5aff81a7162f","name":"theme","slug":"theme","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":14,"uuid":"4d3463b7-a49d-40fc-a24f-f831096d81fb","name":"cover image","slug":"cover-image","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":15,"uuid":"b5a356c8-f7bf-4ed5-8105-9820f1ce6fa7","name":"sticko","slug":"sticko","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":16,"uuid":"ec75c32a-040b-492f-bb0f-c50ebffe1470","name":"damianmuti","slug":"damianmuti","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":17,"uuid":"4d418d1b-6ed4-4572-9d09-35e93662e9d8","name":"unsplash","slug":"unsplash","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":18,"uuid":"a2537217-693f-4fcd-8047-a5b6126aceff","name":"Law Abiding Citizen","slug":"law-abiding-citizen","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":19,"uuid":"e9e4327d-2f10-4544-91f9-ab656d34a76a","name":"重案對決","slug":"zhong-an-dui-jue","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":20,"uuid":"a1fd1995-e880-4dbd-ad3a-84108d01e6ef","name":"Anne Hathaway","slug":"anne-hathaway","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":21,"uuid":"2026db75-7ab4-4aa6-969d-8b7027dad5ec","name":"Jim Sturgess","slug":"jim-sturgess","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":22,"uuid":"31c7016f-3888-4a53-87c0-e4615c23689c","name":"One Day","slug":"one-day","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":23,"uuid":"2dd21dfc-a3b2-4ad7-9c13-26ecfbf9f1b2","name":"真愛挑日子","slug":"zhen-ai-tiao-ri-zi","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":24,"uuid":"49f614e2-b25e-46e0-a873-a7fd29e59229","name":"音樂","slug":"yin-le","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":25,"uuid":"fea0de16-19e8-49ed-8283-22fe2b7ef059","name":"music","slug":"music","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":26,"uuid":"1659350d-7c58-4113-9a5b-23a976d2e5a5","name":"聆聽筆記","slug":"ling-ting-bi-ji","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":27,"uuid":"0484be3e-ebf1-45ee-ae99-8e72f5558868","name":"Swim Deep","slug":"swim-deep","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":28,"uuid":"e1b982a6-fb6e-4a59-b175-a5eea2c34d9f","name":"She Changes the Weather","slug":"she-changes-the-weather","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":29,"uuid":"b8c2ebaf-6b01-427a-878e-19569cf449d7","name":"PHP","slug":"php","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":30,"uuid":"32a6f983-83d0-4c7e-872f-852cbbc59a71","name":"CMD","slug":"cmd","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":31,"uuid":"53d638e4-3e8e-46f9-a3da-d051a651d752","name":"shell","slug":"shell","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":32,"uuid":"73545251-210e-4a79-8345-625ccc95aa63","name":"PHP process list","slug":"php-process-list","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":33,"uuid":"be831084-44bb-4b68-802b-adf123c3e199","name":"AWS","slug":"aws","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":34,"uuid":"3968f326-a5df-4944-8202-01dc4d4a34fd","name":"AWS S3","slug":"aws-s3","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":35,"uuid":"f7ab311b-b22d-4a23-9b95-79c37e15b998","name":"AWS EC2","slug":"aws-ec2","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":36,"uuid":"9fee67b8-6e32-474b-b715-3cb89810ef44","name":"crontab","slug":"crontab","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":37,"uuid":"ac4fbbb1-f48a-435d-b98d-10eda18de39b","name":"Apache2","slug":"apache2","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":38,"uuid":"6a5c5042-25a6-4aeb-8bbc-f195049158f4","name":"mod_rewrite","slug":"mod_rewrite","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":39,"uuid":"4fe88494-8098-4a97-a2c0-ac42281ca797","name":"ubuntu","slug":"ubuntu","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":40,"uuid":"471eac96-023a-44eb-8f31-687fff2580f4","name":"OSX","slug":"osx","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":41,"uuid":"22d7e6eb-5990-4dfb-b51d-c2871546d76f","name":"virtualbox","slug":"virtualbox","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":42,"uuid":"180b6e39-8e1a-460c-9e86-293f31e04e83","name":"vagrant","slug":"vagrant","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":43,"uuid":"7c5cb644-1bd3-4e70-a885-bd269f50baeb","name":"Settings","slug":"settings","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":44,"uuid":"c0698b67-f8fa-4de0-9a0f-c755d321a364","name":"文字編輯器","slug":"wen-zi-bian-ji-qi","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":45,"uuid":"046a1cf8-a1ac-4624-9903-da121ef84478","name":"Sublime Text 2","slug":"sublime-text-2","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":46,"uuid":"d2dc2ba9-3760-4dc1-8596-c4783305f423","name":"Useful Shortcuts","slug":"useful-shortcuts","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":47,"uuid":"bbb063fc-d833-4781-be77-133dac39ab20","name":"Git","slug":"git","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":48,"uuid":"8622660b-3972-4c33-8961-dfe2a1657ea6","name":"Github","slug":"github","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":49,"uuid":"9dda0cf1-2308-4ddf-96bc-e7f969d409ad","name":"Heroku","slug":"heroku","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":50,"uuid":"aaa2d4df-dda8-4cef-a1a8-1bbbd29d3319","name":"PaaS","slug":"paas","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":51,"uuid":"3c37899a-8d5c-486e-8dd6-9023d63d6762","name":"Laravel","slug":"laravel","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":52,"uuid":"5ee395fb-65d4-4f7c-b7c0-060220eef943","name":"電影一幕","slug":"dian-ying-mu","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":53,"uuid":"f9413e1f-73f4-4881-8bec-44421686a983","name":"人工進化","slug":"ren-gong-jin-hua","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":54,"uuid":"5df7d1f0-5266-40b3-bbc6-6de1afbe60a6","name":"送行者：禮儀師的樂章","slug":"song-xing-zhe-li-yi-shi-de-le-zhang","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":55,"uuid":"32c1dd4b-1f89-4275-927d-be2569558134","name":"iTunes11, HTML5, JavaScript, CSS3, Color Quantization, Color Thief, Heuristic Algorithm","slug":"itunes11-html5-javascript-css3-color-quantization-color-thief-heuristic-algorithm","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":56,"uuid":"980c2e78-2022-4782-9037-9450138e5b48","name":"CSS3","slug":"css3","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":57,"uuid":"25860c6a-1732-4713-94fc-bb4ffd7aa45b","name":"Animation","slug":"animation","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-07-20T05:19:40.000Z","created_by":1,"updated_at":"2014-07-20T05:19:40.000Z","updated_by":1},{"id":58,"uuid":"92a59a9f-bb43-4abd-87ef-7f3260217441","name":"Composer","slug":"composer","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-08-05T09:35:35.000Z","created_by":1,"updated_at":"2014-08-05T09:35:35.000Z","updated_by":1},{"id":59,"uuid":"0caf0fe3-c6d7-47e3-a79e-f7690e4fdf9b","name":"HHVM","slug":"hhvm","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-08-05T09:42:04.000Z","created_by":1,"updated_at":"2014-08-05T09:42:04.000Z","updated_by":1},{"id":60,"uuid":"a88c796b-c406-4b9d-8c66-0d1f0db555a8","name":"Nginx","slug":"nginx","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-08-05T09:42:04.000Z","created_by":1,"updated_at":"2014-08-05T09:42:04.000Z","updated_by":1},{"id":61,"uuid":"c3812599-d274-49b3-9571-7595f9eb39bd","name":"python","slug":"python","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-08-06T13:49:39.000Z","created_by":1,"updated_at":"2014-08-06T13:49:39.000Z","updated_by":1},{"id":62,"uuid":"d9ad6bf4-de67-4cb8-83ed-aa302ece707b","name":"jieba","slug":"jieba","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-08-06T13:49:39.000Z","created_by":1,"updated_at":"2014-08-06T13:49:39.000Z","updated_by":1},{"id":63,"uuid":"de8c2e13-c6a4-4788-8bf8-3529eae45e5f","name":"machine learning","slug":"machine-learning","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-08-06T13:49:39.000Z","created_by":1,"updated_at":"2014-08-06T13:49:39.000Z","updated_by":1},{"id":64,"uuid":"889b8e90-0eec-4da4-9109-fd4f68dfe97f","name":"自然語言處理","slug":"zi-ran-yu-yan-chu-li","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-08-06T13:49:39.000Z","created_by":1,"updated_at":"2014-08-06T13:49:39.000Z","updated_by":1},{"id":65,"uuid":"bbf32802-57ec-4475-b2ad-27ae496d189d","name":"中文斷詞","slug":"zhong-wen-duan-ci","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-08-06T13:49:40.000Z","created_by":1,"updated_at":"2014-08-06T13:49:40.000Z","updated_by":1},{"id":66,"uuid":"575b6704-1a8f-4070-a36c-dc8d522c39b6","name":"cmd","slug":"cmd-2","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2014-11-25T11:14:32.000Z","created_by":1,"updated_at":"2014-11-25T11:14:32.000Z","updated_by":1},{"id":67,"uuid":"7d3e9d19-5c57-496c-ba58-267876f8b71a","name":"機器學習","slug":"ji-qi-xue-xi","description":null,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":"2015-09-11T09:05:43.000Z","created_by":1,"updated_at":"2015-09-11T09:05:43.000Z","updated_by":1}],"posts_tags":[{"id":2,"post_id":2,"tag_id":2},{"id":3,"post_id":2,"tag_id":3},{"id":4,"post_id":2,"tag_id":4},{"id":5,"post_id":3,"tag_id":5},{"id":6,"post_id":3,"tag_id":6},{"id":7,"post_id":3,"tag_id":7},{"id":8,"post_id":4,"tag_id":8},{"id":9,"post_id":4,"tag_id":9},{"id":10,"post_id":5,"tag_id":10},{"id":11,"post_id":5,"tag_id":11},{"id":12,"post_id":5,"tag_id":12},{"id":13,"post_id":6,"tag_id":9},{"id":14,"post_id":6,"tag_id":13},{"id":15,"post_id":6,"tag_id":14},{"id":16,"post_id":6,"tag_id":15},{"id":17,"post_id":6,"tag_id":16},{"id":18,"post_id":6,"tag_id":17},{"id":19,"post_id":7,"tag_id":11},{"id":20,"post_id":7,"tag_id":18},{"id":21,"post_id":7,"tag_id":19},{"id":22,"post_id":8,"tag_id":11},{"id":23,"post_id":8,"tag_id":20},{"id":24,"post_id":8,"tag_id":21},{"id":25,"post_id":8,"tag_id":22},{"id":26,"post_id":8,"tag_id":23},{"id":27,"post_id":9,"tag_id":24},{"id":28,"post_id":9,"tag_id":25},{"id":29,"post_id":9,"tag_id":26},{"id":30,"post_id":9,"tag_id":27},{"id":31,"post_id":9,"tag_id":28},{"id":32,"post_id":11,"tag_id":29},{"id":33,"post_id":11,"tag_id":30},{"id":34,"post_id":11,"tag_id":31},{"id":35,"post_id":11,"tag_id":32},{"id":36,"post_id":12,"tag_id":33},{"id":37,"post_id":12,"tag_id":34},{"id":38,"post_id":12,"tag_id":35},{"id":39,"post_id":13,"tag_id":34},{"id":40,"post_id":13,"tag_id":35},{"id":41,"post_id":14,"tag_id":31},{"id":42,"post_id":14,"tag_id":33},{"id":43,"post_id":14,"tag_id":34},{"id":44,"post_id":14,"tag_id":35},{"id":45,"post_id":14,"tag_id":36},{"id":46,"post_id":15,"tag_id":33},{"id":47,"post_id":15,"tag_id":35},{"id":48,"post_id":15,"tag_id":37},{"id":49,"post_id":15,"tag_id":38},{"id":50,"post_id":16,"tag_id":39},{"id":51,"post_id":16,"tag_id":40},{"id":52,"post_id":16,"tag_id":41},{"id":53,"post_id":16,"tag_id":42},{"id":54,"post_id":18,"tag_id":11},{"id":55,"post_id":18,"tag_id":52},{"id":56,"post_id":18,"tag_id":53},{"id":57,"post_id":19,"tag_id":35},{"id":58,"post_id":20,"tag_id":43},{"id":59,"post_id":20,"tag_id":44},{"id":60,"post_id":21,"tag_id":45},{"id":61,"post_id":21,"tag_id":46},{"id":62,"post_id":22,"tag_id":47},{"id":63,"post_id":22,"tag_id":48},{"id":64,"post_id":23,"tag_id":47},{"id":65,"post_id":23,"tag_id":48},{"id":66,"post_id":24,"tag_id":49},{"id":67,"post_id":24,"tag_id":50},{"id":68,"post_id":25,"tag_id":49},{"id":69,"post_id":25,"tag_id":51},{"id":70,"post_id":26,"tag_id":49},{"id":71,"post_id":27,"tag_id":54},{"id":72,"post_id":29,"tag_id":55},{"id":73,"post_id":30,"tag_id":56},{"id":74,"post_id":30,"tag_id":57},{"id":75,"post_id":31,"tag_id":1},{"id":78,"post_id":33,"tag_id":29},{"id":79,"post_id":33,"tag_id":47},{"id":80,"post_id":33,"tag_id":51},{"id":81,"post_id":33,"tag_id":60},{"id":82,"post_id":33,"tag_id":59},{"id":88,"post_id":34,"tag_id":61},{"id":89,"post_id":34,"tag_id":62},{"id":90,"post_id":34,"tag_id":63},{"id":91,"post_id":34,"tag_id":64},{"id":92,"post_id":34,"tag_id":65},{"id":94,"post_id":35,"tag_id":51},{"id":95,"post_id":36,"tag_id":51},{"id":97,"post_id":37,"tag_id":51},{"id":98,"post_id":38,"tag_id":51},{"id":100,"post_id":39,"tag_id":51},{"id":102,"post_id":40,"tag_id":51},{"id":103,"post_id":41,"tag_id":51},{"id":104,"post_id":53,"tag_id":30},{"id":105,"post_id":53,"tag_id":66},{"id":106,"post_id":54,"tag_id":30},{"id":107,"post_id":54,"tag_id":66},{"id":108,"post_id":42,"tag_id":51},{"id":109,"post_id":58,"tag_id":51},{"id":110,"post_id":59,"tag_id":56},{"id":111,"post_id":62,"tag_id":63},{"id":112,"post_id":61,"tag_id":63},{"id":114,"post_id":62,"tag_id":67},{"id":115,"post_id":63,"tag_id":63},{"id":116,"post_id":63,"tag_id":67},{"id":117,"post_id":65,"tag_id":63},{"id":118,"post_id":66,"tag_id":63},{"id":119,"post_id":67,"tag_id":63},{"id":120,"post_id":68,"tag_id":63},{"id":121,"post_id":69,"tag_id":63},{"id":122,"post_id":70,"tag_id":63},{"id":123,"post_id":72,"tag_id":63},{"id":124,"post_id":74,"tag_id":63},{"id":125,"post_id":76,"tag_id":63},{"id":126,"post_id":77,"tag_id":63},{"id":127,"post_id":78,"tag_id":63},{"id":128,"post_id":79,"tag_id":63},{"id":129,"post_id":80,"tag_id":63},{"id":130,"post_id":81,"tag_id":63},{"id":131,"post_id":82,"tag_id":63},{"id":132,"post_id":83,"tag_id":63},{"id":133,"post_id":84,"tag_id":63},{"id":134,"post_id":86,"tag_id":63},{"id":135,"post_id":87,"tag_id":63},{"id":136,"post_id":88,"tag_id":63},{"id":137,"post_id":89,"tag_id":63},{"id":138,"post_id":64,"tag_id":63},{"id":139,"post_id":90,"tag_id":63},{"id":140,"post_id":91,"tag_id":63},{"id":141,"post_id":92,"tag_id":63}]}}